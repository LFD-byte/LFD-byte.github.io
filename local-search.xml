<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>线性代数学习日记（一）</title>
    <link href="/2023/08/26/xian-xing-dai-shu-xue-xi-ri-ji-yi/"/>
    <url>/2023/08/26/xian-xing-dai-shu-xue-xi-ri-ji-yi/</url>
    
    <content type="html"><![CDATA[<h2 id="matrices">Matrices</h2><p>线性代数的核心问题是解决等式的一系列问题，这些等式是线性的，即未知数仅被数字乘，不会出现两个未知数<span class="math inline">\(x\)</span> 乘 <spanclass="math inline">\(y\)</span> 的情况。</p><p>矩阵与向量相乘的两种角度</p><h3 id="列角度">列角度</h3><p>矩阵 A 与向量 x 相乘是矩阵 A 的各列以 x 的方式组合 <spanclass="math display">\[Ax =\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   -1 &amp; 1 &amp; 0 \\   0 &amp; -1 &amp; 1\end{array}\right]\left[\begin{array}{}   x_1 \\   x_2 \\   x_3\end{array}\right] =\left[\begin{array}{}   x_1 \\   x_2 - x_1 \\   x_3 - x_2\end{array}\right] =\left[\begin{array}{}   b_1 \\   b_2 \\   b_3\end{array}\right] =b\]</span> 这个 A 称作 diffenence matrix，因为 b 包括了输入向量 x的所有差值。</p><h3 id="行角度">行角度</h3><p>矩阵 A 与向量 x 相乘也可以看作是行的点积 <spanclass="math display">\[Ax =\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   -1 &amp; 1 &amp; 0 \\   0 &amp; -1 &amp; 1\end{array}\right]\left[\begin{array}{}   x_1 \\   x_2 \\   x_3\end{array}\right] =\left[\begin{array}{}   (1, 0, 0) \cdot (x_1, x_2, x_3) \\   (-1, 1, 0) \cdot (x_1, x_2, x_3) \\   (0, -1, 1) \cdot (x_1, x_2, x_3)\end{array}\right]\]</span></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数学习日记（二）</title>
    <link href="/2023/08/23/xian-xing-dai-shu-xue-xi-ri-ji-er/"/>
    <url>/2023/08/23/xian-xing-dai-shu-xue-xi-ri-ji-er/</url>
    
    <content type="html"><![CDATA[<h2 id="elimination-using-matrices">Elimination Using Matrices</h2><p>通过与<strong>消元矩阵</strong> <spanclass="math inline">\(E\)</span> 相乘从第 2 行中减去第 1 行的 2 倍。<span class="math display">\[\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   \bold{-2} &amp; \bold{1} &amp; 0 \\   0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{}   \bold{2} \\   \bold{8} \\   10\end{array}\right]\left[\begin{array}{}   2 \\   \bold{4} \\   10\end{array}\right] \quad \quad\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   \bold{-2} &amp; \bold{1} &amp; 0 \\   0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{}   \bold{b_1} \\   \bold{b_2} \\   b_3\end{array}\right]\left[\begin{array}{}   b_1 \\   \bold{b_2-2b_1} \\   b_3\end{array}\right]\]</span> 通过与<strong>置换矩阵</strong>相乘交换矩阵中的行 <spanclass="math display">\[\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   0 &amp; 0 &amp; 1 \\   0 &amp; 2 &amp; 0\end{array}\right]\left[\begin{array}{}   2 &amp; 4 &amp; 1 \\   \bold{0} &amp; \bold{0} &amp; \bold{3} \\   0 &amp; 6 &amp; 5\end{array}\right] =\left[\begin{array}{}   2 &amp; 4 &amp; 1 \\   0 &amp; 6 &amp; 5 \\   \bold{0} &amp; \bold{0} &amp; \bold{3}\end{array}\right]\]</span> 除 A 矩阵外包含 b作为额外列的矩阵称作<strong>增广矩阵</strong> <spanclass="math display">\[\left[\begin{array}{}   A &amp; b\end{array}\right] =\left[\begin{array}{}   2 &amp; 4 &amp; -2 &amp; \bold{2}\\   4 &amp; 9 &amp; -3 &amp; \bold{8}\\   -2 &amp; -3 &amp; 7 &amp; \bold{10}\end{array}\right]\]</span> 在增广矩阵上进行消元操作 <span class="math display">\[\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   -2 &amp; 1 &amp; 0 \\   0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{}   2 &amp; 4 &amp; -2 &amp; \bold{2}\\   4 &amp; 9 &amp; -3 &amp; \bold{8}\\   -2 &amp; -3 &amp; 7 &amp; \bold{10}\end{array}\right] =\left[\begin{array}{}   2 &amp; 4 &amp; -2 &amp; \bold{2}\\   0 &amp; 1 &amp; 1 &amp; \bold{4}\\   -2 &amp; -3 &amp; 7 &amp; \bold{10}\end{array}\right]\]</span></p><h2 id="rules-for-matrix-operations">Rules for Matrix Operations</h2><h3 id="矩阵加法">矩阵加法</h3><p>两个大小相同矩阵相加 <span class="math inline">\(A + B\)</span>为对应位置元素相加： <span class="math display">\[\left[\begin{matrix}   1 &amp; 2 \\   3 &amp; 4 \\   0 &amp; 0  \end{matrix}  \right] +  \left[  \begin{matrix}  2 &amp; 2 \\  4 &amp; 4 \\  9 &amp; 9  \end{matrix}  \right] =  \left[  \begin{matrix}  3 &amp; 4 \\  7 &amp; 8 \\  9 &amp; 9  \end{matrix}  \right]\]</span> 常数和矩阵相乘 <span class="math inline">\(cA\)</span>为矩阵每个元素乘常数： <span class="math display">\[2\left[\begin{matrix}   1 &amp; 2 \\   3 &amp; 4 \\   0 &amp; 0  \end{matrix}  \right] =  \left[\begin{matrix}   2 &amp; 4 \\   6 &amp; 8 \\   0 &amp; 0  \end{matrix}  \right]\]</span></p><h3 id="矩阵乘法">矩阵乘法</h3><h4 id="点积视角">点积视角</h4><p><span class="math inline">\(m \times n\)</span> 的矩阵 A 和 <spanclass="math inline">\(n \times p\)</span> 的矩阵 B 相乘后维度为 <spanclass="math inline">\(m \times p\)</span> <span class="math display">\[(m \times n)(n \times p) = (m \times p) \\left[\begin{matrix}   m \ rows \\   n \ columns  \end{matrix}\right]\left[\begin{matrix}   n \ rows \\   p \ columns  \end{matrix}\right] =\left[\begin{matrix}   m \ rows \\   p \ columns  \end{matrix}\right]\]</span> 矩阵 AB 第 i 行 第 j 列元素为 A 的第 i 行和 B 的第 j列的点积。 <span class="math display">\[\left[\begin{matrix}   * \\   a_{i1} &amp; a_{i2} &amp; \cdots &amp; a_{i5} \\   * \\   *  \end{matrix}\right]\left[\begin{matrix}   * &amp; * &amp; b_{1j} &amp; * &amp; * &amp; *\\    &amp;  &amp; b_{2j} &amp;  &amp;  &amp; \\    &amp;  &amp; \vdots &amp;  &amp;  &amp; \\    &amp;  &amp; b_{5j} &amp;  &amp;  &amp;  \end{matrix}\right] =\left[\begin{matrix}     &amp;   &amp; * &amp;   &amp;   &amp;  \\   * &amp; * &amp; AB_{ij} &amp; * &amp; * &amp; *\\    &amp;  &amp; * &amp;  &amp;  &amp; \\    &amp;  &amp; * &amp;  &amp;  &amp;  \end{matrix}\right]\]</span>一行乘一列得到一个元素称作内积，也叫点积，一列乘一行得到一个矩阵称作外积。</p><h4 id="列的组合">列的组合</h4><p>矩阵 AB 的每一列是 A 的列的组合 <span class="math display">\[A[b_1, \cdots, b_p] = [Ab_1, \cdots, Ab_p]\]</span></p><h4 id="行的组合">行的组合</h4><p>矩阵 AB 的每一行是 B 的行的组合 <span class="math display">\[[row \ i \ of \ A] \\left[\begin{matrix}   1 &amp; 2 &amp; 3 \\   4 &amp; 5 &amp; 6\\   7 &amp; 8 &amp; 9  \end{matrix}\right] =[row \ i \ of \ AB]\]</span></p><h4 id="列乘行">列乘行</h4><p>矩阵 A 的列 i 与矩阵 B 的行 j 相乘之后相加 <spanclass="math display">\[\left[\begin{matrix}   col\ 1 &amp; col\ 2 &amp; col\ 3 \\   \cdot &amp; \cdot &amp; \cdot \\   \cdot &amp; \cdot &amp; \cdot  \end{matrix}\right]\left[\begin{matrix}   row\ 1 &amp; \cdot &amp; \cdot \\   row\ 2 &amp; \cdot &amp; \cdot \\   row\ 3 &amp; \cdot &amp; \cdot  \end{matrix}\right] =(col \ 1)(row \ 1) + (col \ 2)(row \ 2) + (col \ 3)(row \ 3)\]</span></p><h3 id="矩阵乘法规则">矩阵乘法规则</h3><p>矩阵相乘遵循的规则： <span class="math display">\[AB \neq BA \\A(B+C) = AB + AC \\(A+B)C = AC + BC \\(AB)C=A(BC) \\A_p = AAA\cdots A(p \ factors) \quad (A^p)(A^q) = A^{p+q} \quad (A^p)^q= A^{pq}\]</span></p><h3 id="块矩阵及其乘法">块矩阵及其乘法</h3><p><span class="math inline">\(4 \times 6\)</span> 的矩阵可以切割为6个<span class="math inline">\(2 \times 2\)</span> 的矩阵。 <spanclass="math display">\[\left[\begin{array}{cc|cc|cc}   1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\   0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\   \hline   1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\   0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\end{array}\right] =\left[\begin{array}{}   I &amp; I &amp; I \\   I &amp; I &amp; I \\\end{array}\right]\]</span> 如果 A 的块和 B 的块能够相乘，那么 AB 成立。AB 为 A的列的切片乘 B 的行的切片。 <span class="math display">\[\left[\begin{array}{}   A_{11} &amp; A_{12} \\   A_{21} &amp; A_{22} \\\end{array}\right]\left[\begin{array}{}   B_{11} \\   B_{21} \\\end{array}\right] =\left[\begin{array}{}   A_{11}B_{11} &amp; A_{12}B_{21} \\   A_{21}B_{11} &amp; A_{22}B_{21} \\\end{array}\right]\]</span></p>]]></content>
    
    
    <categories>
      
      <category>数学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>算法笔记——树篇</title>
    <link href="/2023/08/12/suan-fa-bi-ji-shu-pian/"/>
    <url>/2023/08/12/suan-fa-bi-ji-shu-pian/</url>
    
    <content type="html"><![CDATA[<h2 id="二叉树的存储结构与基本操作">二叉树的存储结构与基本操作</h2><p>结点的存储结构</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">node</span>&#123;<br><span class="hljs-keyword">typename</span> data; <span class="hljs-comment">//数据域</span><br>    node* lchild; <span class="hljs-comment">//指向左子树根结点的指针</span><br>    node* rchild; <span class="hljs-comment">//指向右子树根结点的指针</span><br>&#125;<br><br>node* root = <span class="hljs-literal">NULL</span>; <span class="hljs-comment">//根结点的初始化</span><br></code></pre></td></tr></table></figure><p>新建结点</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">node* <span class="hljs-title">newNode</span><span class="hljs-params">(<span class="hljs-type">int</span> v)</span></span>&#123;<br>    node* Node = <span class="hljs-keyword">new</span> node;<br>    Node-&gt;data = v;<br>    Node-&gt;lchild = Node-&gt;rchild = <span class="hljs-literal">NULL</span>;<br>    <span class="hljs-keyword">return</span> Node; <span class="hljs-comment">//返回新建结点的地址</span><br>&#125;<br></code></pre></td></tr></table></figure><p>二叉树结点的查找和修改</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">search</span><span class="hljs-params">(node* root, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> newdata)</span></span>&#123;<br><span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>)&#123;<br>        <span class="hljs-keyword">return</span>; <span class="hljs-comment">//空树，死胡同（递归边界）</span><br>    &#125;<br>    <span class="hljs-keyword">if</span>(root-&gt;data == x)&#123; <span class="hljs-comment">//找到数据域为 x 的结点，把它修改成 newdata</span><br>        root-&gt;data = newdata;<br>    &#125;<br>    <span class="hljs-built_in">search</span>(root-&gt;lchild, x, newdata); <span class="hljs-comment">//往左子树搜索 x (递归式)</span><br>    <span class="hljs-built_in">search</span>(root-&gt;rchild, x, newdata); <span class="hljs-comment">//往右子树搜索 x (递归式)</span><br>&#125;<br></code></pre></td></tr></table></figure><p>二叉树结点的插入</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//insert 函数将在二叉树中插入一个数据域为 x 的新结点</span><br><span class="hljs-comment">//注意根结点指针 root 要使用引用，否则插入不会成功</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">insert</span><span class="hljs-params">(node* &amp;root, <span class="hljs-type">int</span> x)</span></span>&#123;<br><span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>)&#123;<br>        root = <span class="hljs-built_in">newNode</span>(x); <span class="hljs-comment">//空树，说明查找失败，也即插入位置（递归边界）</span><br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(由二叉树的性质，x 应插在左子树)&#123;<br>        <span class="hljs-built_in">insert</span>(root-&gt;lchild, x); <span class="hljs-comment">//往左子树搜索(递归式)</span><br>    &#125; <span class="hljs-keyword">else</span>&#123;<br>        <span class="hljs-built_in">insert</span>(root-&gt;rchild, x); <span class="hljs-comment">//往右子树搜索(递归式)</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>二叉树的创建</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//二叉树的建立</span><br><span class="hljs-function">node* <span class="hljs-title">Create</span><span class="hljs-params">(<span class="hljs-type">int</span> data[], <span class="hljs-type">int</span> n)</span></span>&#123;<br>    node* root = <span class="hljs-literal">NULL</span>; <span class="hljs-comment">//新建空根结点 root</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-built_in">insert</span>(root, data[i]); <span class="hljs-comment">//将 data[0]~data[n-1]插入二叉树中</span><br>    &#125;<br>    <span class="hljs-keyword">return</span> root; <span class="hljs-comment">//返回根结点</span><br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>算法笔记——搜索篇</title>
    <link href="/2023/08/12/suan-fa-bi-ji-sou-suo-pian/"/>
    <url>/2023/08/12/suan-fa-bi-ji-sou-suo-pian/</url>
    
    <content type="html"><![CDATA[<h2 id="深度优先搜索dfs">深度优先搜索（DFS）</h2><p><strong>背包问题</strong></p><p>有 n 件物品，每件物品的重量为 w[i], 价值为c[i]。现在需要选出若干件物品放入一个容量为 V的背包中，使得在选入背包的物品重量和不超过容量 V的前提下，让背包中物品的价值之和最大，求最大价值。（1&lt;= n &lt;=20）</p><p>输入数据：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-number">5</span> <span class="hljs-number">8</span> <span class="hljs-comment">//5 件物品，背包容量为 8</span><br><span class="hljs-number">3</span> <span class="hljs-number">5</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-comment">//重量分别为 3 5 1 2 2</span><br><span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">3</span> <span class="hljs-comment">//价值分别为 4 5 2 1 3</span><br></code></pre></td></tr></table></figure><p>输出数据：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>实现代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstido&gt;</span></span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> maxn = <span class="hljs-number">30</span>;<br><span class="hljs-type">int</span> n, V, maxValue = <span class="hljs-number">0</span>; <span class="hljs-comment">//物品件数 n，背包容量 V，最大价值 maxValue</span><br><span class="hljs-type">int</span> w[maxn], c[maxn]; <span class="hljs-comment">//w[i] 为每件物品的重量，c[i] 为每件物品的价值</span><br><span class="hljs-comment">// DFS, index 为当前处理的物品编号</span><br><span class="hljs-comment">// sumW 和 sumC 分别为当前总重量和当前总价值</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">DFS</span><span class="hljs-params">(<span class="hljs-type">int</span> index, <span class="hljs-type">int</span> sumW, <span class="hljs-type">int</span> sumC)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(index == n)&#123; <span class="hljs-comment">//已经完成对 n 件物品的选择（死胡同）</span><br>        <span class="hljs-keyword">if</span>(sumW &lt;= V &amp;&amp; sumC &gt; maxValue)&#123;<br>            maxValue = sumC; <span class="hljs-comment">//不超过背包容量时更新最大价值 maxValue</span><br>        &#125;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-comment">//岔道口</span><br>    <span class="hljs-built_in">DFS</span>(index + <span class="hljs-number">1</span>, sumW, sumC); <span class="hljs-comment">//不选第 index 件物品</span><br>    <span class="hljs-built_in">DFS</span>(index + <span class="hljs-number">1</span>, sumW + w[index], sumC + c[index]); <span class="hljs-comment">//选第 index 件物品</span><br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d%d&quot;</span>, &amp;n, &amp;V);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d&quot;</span>, &amp;w[i]); <span class="hljs-comment">//每件物品的重量</span><br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d&quot;</span>, &amp;c[i]); <span class="hljs-comment">//每件物品的价值</span><br>    &#125;<br>    <span class="hljs-built_in">DFS</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>); <span class="hljs-comment">//初始时为第 0 件物品、当前总重量和总价值均为 0</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d\n&quot;</span>, maxValue);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="广度优先搜索bfs">广度优先搜索（BFS）</h2><p>给出一个 m * n 的矩阵，矩阵中的元素为 0 或 1。称位置 (x, y)与其上下左右四个位置 (x, y + 1)、(x, y - 1)、(x + 1, y)、(x - 1, y)是相邻的。如果矩阵中有若干个 1 是相邻的（不必两两相邻），那么称这些 1构成了一个 “块”。求给定的矩阵中 “块” 的个数。</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">0 </span><span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span><br><span class="hljs-symbol">0 </span><span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">0 </span><span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">0 </span><span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">1 </span><span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">1 </span><span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>上面的 6 * 7 的矩阵中，“块”的个数为 4。</p><p>实现代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstido&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> maxn = <span class="hljs-number">100</span>;<br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">node</span>&#123;<br>    <span class="hljs-type">int</span> x, y;<br>&#125; Node;<br><br><span class="hljs-type">int</span> n, m;<br><span class="hljs-type">int</span> matrix[maxn][maxn];<br><span class="hljs-type">bool</span> inq[maxn][maxn] = &#123;<span class="hljs-literal">false</span>&#125;;<br><span class="hljs-type">int</span> X[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>&#125;;<br><span class="hljs-type">int</span> Y[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">judge</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(x &gt;= n || x &lt; <span class="hljs-number">0</span> || y &gt;= m || y &lt; <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    <span class="hljs-comment">//</span><br>    <span class="hljs-keyword">if</span>(matrix[x][y] == <span class="hljs-number">0</span> || inq[x][y] == <span class="hljs-literal">true</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    <span class="hljs-comment">//</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>&#125;<br><span class="hljs-comment">//</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">BFS</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span></span>&#123;<br>    queue&lt;node&gt; Q;<br>    Node.x = x, Node.y = y;<br>    Q.<span class="hljs-built_in">push</span>(Node); <span class="hljs-comment">//将结点 Node 入队</span><br>    inq[x][y] = <span class="hljs-literal">true</span>; <span class="hljs-comment">//设置(x, y) 已入过队</span><br>    <span class="hljs-keyword">while</span>(!Q.<span class="hljs-built_in">empty</span>())&#123;<br>        node top = Q.<span class="hljs-built_in">front</span>(); <span class="hljs-comment">//取出队首元素</span><br>        Q.<span class="hljs-built_in">pop</span>(); <span class="hljs-comment">//队首元素出队</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++)&#123; <span class="hljs-comment">//循环 4 次，得到 4 个相邻位置</span><br>            <span class="hljs-type">int</span> newX = top.x + X[i];<br>            <span class="hljs-type">int</span> newY = top.y + Y[i];<br>            <span class="hljs-keyword">if</span>(<span class="hljs-built_in">judge</span>(newX, newY))&#123; <span class="hljs-comment">//如果新位置 (newX, newY) 需要访问</span><br>                <span class="hljs-comment">//设置 Node 的坐标为 (newX, newY)</span><br>                Node.x = newX, Node.y = newY;<br>                Q.<span class="hljs-built_in">push</span>(Node); <span class="hljs-comment">//将结点 Node 加入队列</span><br>                inq[newX][newY] = <span class="hljs-literal">true</span>; <span class="hljs-comment">//设置位置 (newX, newY) 已入过队</span><br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d%d&quot;</span>, &amp;n, &amp;m);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> x = <span class="hljs-number">0</span>; x &lt; n; x++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">0</span>; y &lt; m; y++)&#123;<br>            <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d&quot;</span>, &amp;matrix[x][y]); <span class="hljs-comment">//读入 01 矩阵</span><br>        &#125;<br>    &#125;<br>    <span class="hljs-type">int</span> ans = <span class="hljs-number">0</span>; <span class="hljs-comment">//存放块数</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> x = <span class="hljs-number">0</span>; x &lt; n; x++)&#123; <span class="hljs-comment">//枚举每一个位置</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">0</span>; y &lt; m; y++)&#123;<br>            <span class="hljs-keyword">if</span>(matrix[x][y] == <span class="hljs-number">1</span> &amp;&amp; inq[x][y] == <span class="hljs-literal">false</span>)&#123;<br>                ans++; <span class="hljs-comment">//块数加 1</span><br>                <span class="hljs-built_in">BFS</span>(x, y); <span class="hljs-comment">//访问整个块，将该块所有 &quot;1&quot; 的 inq 都标记为 true</span><br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d\n&quot;</span>, ans);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gene Ontology (Go) overview</title>
    <link href="/2023/08/09/gene-ontology-go-overview/"/>
    <url>/2023/08/09/gene-ontology-go-overview/</url>
    
    <content type="html"><![CDATA[<p>本体（Ontology）是给定领域内知识体系的形式表示。本体通常由一组类（或术语或概念）组成，这些类之间具有运作的关系。基因本体论（GeneOntology GO）从三个方面描述了我们对生物领域的知识：</p><table><thead><tr class="header"><th>术语</th><th>含义</th></tr></thead><tbody><tr class="odd"><td>分子功能（Molecular Function）</td><td>基因产物进行的分子水平活动。分子功能术语描述在分子水平上发生的活动，例如“催化”或“运输”。GO分子功能术语代表活动而不是执行动作的实体（分子或复合物），并且不指定动作发生的地点、时间或背景。分子功能通常对应于单个基因产物（即蛋白质或RNA）可以执行的活动，但某些活动是由多个基因产物组成的分子复合物执行的。广义功能术语的例子有催化活性和转运蛋白活性；较窄的功能术语的例子是腺苷酸环化酶活性或Toll样受体结合。为了避免基因产物名称与其分子功能之间的混淆，GO分子功能通常附加“活性”一词（蛋白激酶将具有GO 分子功能蛋白激酶活性）。</td></tr><tr class="even"><td>细胞成分（Cellular Component）</td><td>基因产物发挥功能的相对于细胞结构的位置，可以是细胞区室（例如线粒体），也可以是它们所属的稳定大分子复合物（例如核糖体）。与GO 的其他方面不同，细胞成分类别不是指过程，而是指细胞解剖结构。</td></tr><tr class="odd"><td>生物过程（Biological Process）</td><td>更广的过程，或由多种分子活动完成的“生物程序”。广泛的生物过程术语的例子有DNA修复或信号转导。更具体的术语的例子是嘧啶核碱基生物合成过程或葡萄糖跨膜转运。请注意，生物过程并不等同于pathway。目前，GO 并未尝试表示完整描述 pathway 所需的动态或依赖性。</td></tr></tbody></table><p>在GO注释的一个例子中，人类“细胞色素c”可以通过<strong>分子功能</strong>氧化还原酶活性、<strong>生物过程</strong>氧化磷酸化和<strong>细胞成分</strong>线粒体膜间隙来描述。</p><p>GO词汇表设计为与物种无关，包括适用于原核生物和真核生物以及单细胞和多细胞生物的术语。</p><p>GO 类由定义、标签、唯一标识符和其他几个元素组成。</p><p>GO的结构可以用图来描述，其中每个GO项是一个节点，项之间的关系是节点之间的边。GO的层次结构松散，“子”术语比“父”术语更专业，但与严格的层次结构不同，一个术语可能有多个父术语。例如，生物过程术语己糖生物合成过程有两个父体：己糖代谢过程和单糖生物合成过程。这反映了生物合成过程是代谢过程的亚型并且己糖是单糖的亚型这一事实。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedhexose-biosynthetic-process.png"alt="hexose-biosynthetic-process" /><figcaption aria-hidden="true">hexose-biosynthetic-process</figcaption></figure><p>如上图所示，三个 GO域（细胞成分、生物过程和分子功能）各自由一个单独的根本体术语表示。域中的所有术语都可以将其起源追溯到根术语，尽管可能存在通过不同数量的中间术语到达本体根的许多不同路径。三个根节点互不相关，没有共同的父节点，因此GO是三个本体。一些基于图形的软件可能需要单个根节点；在这些情况下，可以添加“假”术语作为三个现有根节点的父节点。</p><p>这三个 GO本体是不相交的，这意味着来自不同本体的术语之间没有关系。然而，其他关系（例如部分关系和规范关系）确实在GO本体之间运行。例如，分子功能术语“细胞周期蛋白依赖性蛋白激酶活性”是生物过程“细胞周期”的一部分。</p><p>GO旨在代表生物学知识的当前状态，因此随着生物学知识的积累而不断修订和扩展。每周都会进行更改（大多数相对较小）。本体的修订由在生物学和计算知识表示方面拥有丰富经验的本体编辑团队管理。这些更新是GOC 本体团队和请求更新的科学家之间合作完成的。大多数请求来自进行 GO注释的科学家（这些请求通常只影响几个术语），以及来自特定生物学领域的领域专家（这些专家通常会修改包含许多术语和关系的本体论的整个“分支”）。</p><p><strong>参考文献</strong></p><p>[1] <ahref="https://geneontology.github.io/docs/ontology-documentation/">GeneOntology overview</a></p>]]></content>
    
    
    <categories>
      
      <category>生物信息</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生物信息</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>优化器指南</title>
    <link href="/2023/08/08/you-hua-qi-zhi-nan/"/>
    <url>/2023/08/08/you-hua-qi-zhi-nan/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（三）余篇</title>
    <link href="/2023/08/05/song-ci-san-yu-pian/"/>
    <url>/2023/08/05/song-ci-san-yu-pian/</url>
    
    <content type="html"><![CDATA[<center>木兰花</center><center>钱惟演</center><center>城上风光莺语乱，城下烟波春拍岸。绿杨芳草几时休，泪眼愁肠先已断。</center><center>情怀渐觉成衰晚，鸾镜朱颜惊暗换。昔年多病厌芳尊，今日芳尊惟恐浅。</center><p><br></p><center>木兰花 春景</center><center>宋祁</center><center>东城渐觉风光好，縠皱波纹迎客掉。绿杨烟外晓云轻，红杏枝头春意闹。</center><center>浮生长恨欢娱少，肯爱千金轻一笑？为君持酒劝斜阳，且向花间留晚照。</center><p><br></p><center>踏莎行</center><center>寇准</center><center>春色将阑，莺声渐老，红英落尽青梅小。画堂人静雨蒙蒙，屏山半掩余香袅。</center><center>密约沉沉，离情杳杳，菱花尘满慵将照。倚楼无语欲销魂，长空暗淡逢芳草。</center><p><br></p>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（五）图神经网络（GNN）的理论动机</title>
    <link href="/2023/08/05/tu-shen-jing-wang-luo-xue-xi-ri-ji-wu-tu-shen-jing-wang-luo-gnn-de-li-lun-dong-ji/"/>
    <url>/2023/08/05/tu-shen-jing-wang-luo-xue-xi-ri-ji-wu-tu-shen-jing-wang-luo-gnn-de-li-lun-dong-ji/</url>
    
    <content type="html"><![CDATA[<h2 id="gnn-与图信号">GNN 与图信号</h2><h2 id="gnn-与概率图模型">GNN 与概率图模型</h2><h2 id="gnn-与图同构">GNN 与图同构</h2>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning with Differential Privacy 论文阅读</title>
    <link href="/2023/07/25/lun-wen-yue-du-deep-learning-with-differential-privacy/"/>
    <url>/2023/07/25/lun-wen-yue-du-deep-learning-with-differential-privacy/</url>
    
    <content type="html"><![CDATA[<figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230725160345731.png"alt="image-20230725160345731" /><figcaption aria-hidden="true">image-20230725160345731</figcaption></figure><p>其中，对于高斯机制，<span class="math inline">\(\sigma\)</span>如下式： <span class="math display">\[\sigma^2=\frac{2s^2\log(1.25/\delta)}{\epsilon^2}\]</span></p><p>通过实验作者观察到，模型精度对训练参数（例如批量大小和噪声水平）比神经网络的结构更敏感。非凸学习本质上不太稳定，聚合时需要更大的batch size，更大的 batch size带来了更大的隐私消耗，合理的权衡是使每个时期的批次数量与所需的时期数量具有相同的数量级。从相对较大的学习率开始，然后在几个时期内将其线性衰减到较小的值，并在之后保持恒定，有一定的好处。</p>]]></content>
    
    
    <categories>
      
      <category>差分隐私</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>差分隐私</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 深度单细胞分析</title>
    <link href="/2023/07/11/shen-du-dan-xi-bao-fen-xi/"/>
    <url>/2023/07/11/shen-du-dan-xi-bao-fen-xi/</url>
    
    <content type="html"><![CDATA[<h2 id="section">2023</h2><table><thead><tr class="header"><th>Paper</th><th style="text-align: left;">Function</th><th>Underlying deep models</th><th>Accesses</th></tr></thead><tbody><tr class="odd"><td>ScCCL: Single-Cell Data Clustering Based on Self-SupervisedContrastive Learning</td><td style="text-align: left;">Clustering</td><td></td><td>https://github.com/LuckyxiaoLin/ScCCL.git</td></tr><tr class="even"><td>Deep Multi-Constraint Soft Clustering Analysis for Single-CellRNA-Seq Data via Zero-Inflated Autoencoder Embedding</td><td style="text-align: left;"></td><td></td><td>https://github.com/leaf233/scMCKC</td></tr><tr class="odd"><td>CosTaL: an accurate and scalable graph-based clustering algorithmfor high-dimensional single-cell data analysis</td><td style="text-align: left;"></td><td></td><td>https://github.com/li000678/CosTaL</td></tr><tr class="even"><td>Deep generative modeling and clustering of single cell Hi-Cdata</td><td style="text-align: left;"></td><td></td><td>https://github.com/zhoujt1994/scHiCluster</td></tr><tr class="odd"><td>IsoCell: An Approach to Enhance Single Cell Clustering byIntegrating Isoform-Level Expression Through Orthogonal Projection</td><td style="text-align: left;"></td><td></td><td>https://github.com/genemine/IsoCell</td></tr><tr class="even"><td>Clustering Single-Cell RNA Sequence Data Using Information Maximizedand Noise-Invariant Representations</td><td style="text-align: left;"></td><td></td><td>https://github.com/arnabkmondal/sc-INDC</td></tr><tr class="odd"><td>SSNMDI: a novel joint learning model of semi-supervised non-negativematrix factorization and data imputation for clustering of single-cellRNA-seq data</td><td style="text-align: left;"></td><td></td><td>https://github.com/yushanqiu/SSNMDI</td></tr><tr class="even"><td>scENT for Revealing Gene Clusters From Single-Cell RNA-Seq Data</td><td style="text-align: left;"></td><td></td><td>None</td></tr><tr class="odd"><td>Denoising adaptive deep clustering with self-attention mechanism onsingle-cell sequencing data</td><td style="text-align: left;"></td><td></td><td>https://github.com/LRX2022/scDASFK</td></tr><tr class="even"><td>scDCCA: deep contrastive clustering for single-cell RNA-seq databased on auto-encoder network</td><td style="text-align: left;"></td><td></td><td>https://github.com/WJ319/scDCCA</td></tr><tr class="odd"><td>scBKAP: A Clustering Model for Single-Cell RNA-Seq Data Based onBisecting K-Means</td><td style="text-align: left;"></td><td></td><td>https://github.com/YuBinLab-QUST/scBKAP/</td></tr><tr class="even"><td>scBGEDA: deep single-cell clustering analysis via a dual denoisingautoencoder with bipartite graph ensemble clustering</td><td style="text-align: left;"></td><td></td><td>https://github.com/wangyh082/scBGEDA</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>生物信息</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>生物， 单细胞</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（四）Fluid 主题</title>
    <link href="/2023/07/11/github-pages-ge-ren-bo-ke-da-jian-si-fluid-zhu-ti/"/>
    <url>/2023/07/11/github-pages-ge-ren-bo-ke-da-jian-si-fluid-zhu-ti/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率论（一） 贝叶斯公式</title>
    <link href="/2023/06/16/gai-lu-lun-yi-bei-xie-si-ding-li/"/>
    <url>/2023/06/16/gai-lu-lun-yi-bei-xie-si-ding-li/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>概率</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WandB 学习日记（一）Tutorials</title>
    <link href="/2023/06/12/wandb-xue-xi-ri-ji-yi-doc/"/>
    <url>/2023/06/12/wandb-xue-xi-ri-ji-yi-doc/</url>
    
    <content type="html"><![CDATA[<h2 id="track-experiments">Track experiments</h2><p>快速实验是机器学习的基础。在本教程中，我们使用 W&amp;B来跟踪和可视化实验，以便我们可以快速迭代和理解我们的结果。</p><h3 id="a-shared-dashboard-for-your-experiments">A shared dashboard foryour experiments</h3><p>只需几行代码，您就可以获得丰富的、交互式的、可共享的仪表板，您可以在这里看到<ahref="https://wandb.ai/wandb/wandb_example?_gl=1*1ycseye*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MDE0OC41MC4wLjA.">自己的dashboard</a>。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedPell4Oo.png"alt="dashboard" /><figcaption aria-hidden="true">dashboard</figcaption></figure><h3 id="data-privacy">Data &amp; Privacy</h3><p>我们非常重视安全性，我们的云托管 dashboard使用行业标准最佳加密实践。如果您正在使用无法离开企业集群的数据集，我们可以提供<ahref="https://docs.wandb.com/self-hosted">本地安装</a>。</p><p>下载所有数据并将其导出到其他工具也很容易——例如在 Jupyter笔记本中进行自定义分析。下面是关于我们 <ahref="https://docs.wandb.com/library/api">API</a> 的更多信息。</p><h3 id="install-wandb-library-and-login">Install <code>wandb</code>library and login</h3><p>首先安装库并登录到您的免费帐户。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">!pip install wandb -qU<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Log in to your W&amp;B account</span><br><span class="hljs-keyword">import</span> wandb<br>wandb.login()<br></code></pre></td></tr></table></figure><h3 id="run-an-experiment">Run an experiment</h3><p>1️⃣. 开始新的运行并传入超参数进行跟踪</p><p>2️⃣. 训练或评估的日志指标</p><p>3️⃣. 在 dashboard 中可视化结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><br><span class="hljs-comment"># Launch 5 simulated experiments</span><br>total_runs = <span class="hljs-number">5</span><br><span class="hljs-keyword">for</span> run <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_runs):<br>  <span class="hljs-comment"># 🐝 1️⃣ Start a new run to track this script</span><br>  wandb.init(<br>      <span class="hljs-comment"># Set the project where this run will be logged</span><br>      project=<span class="hljs-string">&quot;basic-intro&quot;</span>, <br>      <span class="hljs-comment"># We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)</span><br>      name=<span class="hljs-string">f&quot;experiment_<span class="hljs-subst">&#123;run&#125;</span>&quot;</span>, <br>      <span class="hljs-comment"># Track hyperparameters and run metadata</span><br>      config=&#123;<br>      <span class="hljs-string">&quot;learning_rate&quot;</span>: <span class="hljs-number">0.02</span>,<br>      <span class="hljs-string">&quot;architecture&quot;</span>: <span class="hljs-string">&quot;CNN&quot;</span>,<br>      <span class="hljs-string">&quot;dataset&quot;</span>: <span class="hljs-string">&quot;CIFAR-100&quot;</span>,<br>      <span class="hljs-string">&quot;epochs&quot;</span>: <span class="hljs-number">10</span>,<br>      &#125;)<br>  <br>  <span class="hljs-comment"># This simple block simulates a training loop logging metrics</span><br>  epochs = <span class="hljs-number">10</span><br>  offset = random.random() / <span class="hljs-number">5</span><br>  <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, epochs):<br>      acc = <span class="hljs-number">1</span> - <span class="hljs-number">2</span> ** -epoch - random.random() / epoch - offset<br>      loss = <span class="hljs-number">2</span> ** -epoch + random.random() / epoch + offset<br>      <br>      <span class="hljs-comment"># 🐝 2️⃣ Log metrics from your script to W&amp;B</span><br>      wandb.log(&#123;<span class="hljs-string">&quot;acc&quot;</span>: acc, <span class="hljs-string">&quot;loss&quot;</span>: loss&#125;)<br>      <br>  <span class="hljs-comment"># Mark the run as finished</span><br>  wandb.finish()<br></code></pre></td></tr></table></figure><p>3️⃣ 当您运行此代码时，您可以通过单击上面的任何 👆 wandb链接找到您的交互式 dashboard。</p><h3 id="simple-pytorch-neural-network">Simple Pytorch NeuralNetwork</h3><p>运行此模型以训练一个简单的 MNIST分类器，然后单击项目页面链接以实时查看您的结果流到 W&amp;B 项目。</p><p>wandb 中的任何运行都会自动记录 <ahref="https://docs.wandb.ai/ref/app/pages/run-page#charts-tab">metrics</a>,<a href="https://docs.wandb.ai/ref/app/pages/run-page#system-tab">systeminformation</a>, <ahref="https://docs.wandb.ai/ref/app/pages/run-page#overview-tab">hyperparameters</a>,<a href="https://docs.wandb.ai/ref/app/pages/run-page#logs-tab">terminaloutput</a> ，您将看到一个包含模型输入和输出的交互式表格。</p><h4 id="set-up-dataloader">Set up Dataloader</h4><p>要运行此示例，我们需要安装 PyTorch。如果您使用的是 GoogleColab，则它已经预装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">!pip install torch torchvision<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> wandb<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch, torchvision<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T<br><br>device = <span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_dataloader</span>(<span class="hljs-params">is_train, batch_size, <span class="hljs-built_in">slice</span>=<span class="hljs-number">5</span></span>):<br>    <span class="hljs-string">&quot;Get a training dataloader&quot;</span><br>    full_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">&quot;.&quot;</span>, train=is_train, transform=T.ToTensor(), download=<span class="hljs-literal">True</span>)<br>    sub_dataset = torch.utils.data.Subset(full_dataset, indices=<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(full_dataset), <span class="hljs-built_in">slice</span>))<br>    loader = torch.utils.data.DataLoader(dataset=sub_dataset, <br>                                         batch_size=batch_size, <br>                                         shuffle=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>, <br>                                         pin_memory=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> loader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>(<span class="hljs-params">dropout</span>):<br>    <span class="hljs-string">&quot;A simple model&quot;</span><br>    model = nn.Sequential(nn.Flatten(),<br>                         nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">256</span>),<br>                         nn.BatchNorm1d(<span class="hljs-number">256</span>),<br>                         nn.ReLU(),<br>                         nn.Dropout(dropout),<br>                         nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">10</span>)).to(device)<br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">validate_model</span>(<span class="hljs-params">model, valid_dl, loss_func, log_images=<span class="hljs-literal">False</span>, batch_idx=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">&quot;Compute performance of the model on the validation dataset and log a wandb.Table&quot;</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    val_loss = <span class="hljs-number">0.</span><br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        correct = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_dl):<br>            images, labels = images.to(device), labels.to(device)<br><br>            <span class="hljs-comment"># Forward pass ➡</span><br>            outputs = model(images)<br>            val_loss += loss_func(outputs, labels)*labels.size(<span class="hljs-number">0</span>)<br><br>            <span class="hljs-comment"># Compute accuracy and accumulate</span><br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>            <span class="hljs-comment"># Log one batch of images to the dashboard, always same batch_idx.</span><br>            <span class="hljs-keyword">if</span> i==batch_idx <span class="hljs-keyword">and</span> log_images:<br>                log_image_table(images, predicted, labels, outputs.softmax(dim=<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> val_loss / <span class="hljs-built_in">len</span>(valid_dl.dataset), correct / <span class="hljs-built_in">len</span>(valid_dl.dataset)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_image_table</span>(<span class="hljs-params">images, predicted, labels, probs</span>):<br>    <span class="hljs-string">&quot;Log a wandb.Table with (img, pred, target, scores)&quot;</span><br>    <span class="hljs-comment"># 🐝 Create a wandb Table to log images, labels and predictions to</span><br>    table = wandb.Table(columns=[<span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;pred&quot;</span>, <span class="hljs-string">&quot;target&quot;</span>]+[<span class="hljs-string">f&quot;score_<span class="hljs-subst">&#123;i&#125;</span>&quot;</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)])<br>    <span class="hljs-keyword">for</span> img, pred, targ, prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(images.to(<span class="hljs-string">&quot;cpu&quot;</span>), predicted.to(<span class="hljs-string">&quot;cpu&quot;</span>), labels.to(<span class="hljs-string">&quot;cpu&quot;</span>), probs.to(<span class="hljs-string">&quot;cpu&quot;</span>)):<br>        table.add_data(wandb.Image(img[<span class="hljs-number">0</span>].numpy()*<span class="hljs-number">255</span>), pred, targ, *prob.numpy())<br>    wandb.log(&#123;<span class="hljs-string">&quot;predictions_table&quot;</span>:table&#125;, commit=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><h4 id="train-your-model">Train Your Model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Launch 5 experiments, trying different dropout rates</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-comment"># 🐝 initialise a wandb run</span><br>    wandb.init(<br>        project=<span class="hljs-string">&quot;pytorch-intro&quot;</span>,<br>        config=&#123;<br>            <span class="hljs-string">&quot;epochs&quot;</span>: <span class="hljs-number">10</span>,<br>            <span class="hljs-string">&quot;batch_size&quot;</span>: <span class="hljs-number">128</span>,<br>            <span class="hljs-string">&quot;lr&quot;</span>: <span class="hljs-number">1e-3</span>,<br>            <span class="hljs-string">&quot;dropout&quot;</span>: random.uniform(<span class="hljs-number">0.01</span>, <span class="hljs-number">0.80</span>),<br>            &#125;)<br>    <br>    <span class="hljs-comment"># Copy your config </span><br>    config = wandb.config<br><br>    <span class="hljs-comment"># Get the data</span><br>    train_dl = get_dataloader(is_train=<span class="hljs-literal">True</span>, batch_size=config.batch_size)<br>    valid_dl = get_dataloader(is_train=<span class="hljs-literal">False</span>, batch_size=<span class="hljs-number">2</span>*config.batch_size)<br>    n_steps_per_epoch = math.ceil(<span class="hljs-built_in">len</span>(train_dl.dataset) / config.batch_size)<br>    <br>    <span class="hljs-comment"># A simple MLP model</span><br>    model = get_model(config.dropout)<br><br>    <span class="hljs-comment"># Make the loss and optimizer</span><br>    loss_func = nn.CrossEntropyLoss()<br>    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)<br><br>   <span class="hljs-comment"># Training</span><br>    example_ct = <span class="hljs-number">0</span><br>    step_ct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>        model.train()<br>        <span class="hljs-keyword">for</span> step, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dl):<br>            images, labels = images.to(device), labels.to(device)<br><br>            outputs = model(images)<br>            train_loss = loss_func(outputs, labels)<br>            optimizer.zero_grad()<br>            train_loss.backward()<br>            optimizer.step()<br>            <br>            example_ct += <span class="hljs-built_in">len</span>(images)<br>            metrics = &#123;<span class="hljs-string">&quot;train/train_loss&quot;</span>: train_loss, <br>                       <span class="hljs-string">&quot;train/epoch&quot;</span>: (step + <span class="hljs-number">1</span> + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, <br>                       <span class="hljs-string">&quot;train/example_ct&quot;</span>: example_ct&#125;<br>            <br>            <span class="hljs-keyword">if</span> step + <span class="hljs-number">1</span> &lt; n_steps_per_epoch:<br>                <span class="hljs-comment"># 🐝 Log train metrics to wandb </span><br>                wandb.log(metrics)<br>                <br>            step_ct += <span class="hljs-number">1</span><br><br>        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-<span class="hljs-number">1</span>)))<br><br>        <span class="hljs-comment"># 🐝 Log train and validation metrics to wandb</span><br>        val_metrics = &#123;<span class="hljs-string">&quot;val/val_loss&quot;</span>: val_loss, <br>                       <span class="hljs-string">&quot;val/val_accuracy&quot;</span>: accuracy&#125;<br>        wandb.log(&#123;**metrics, **val_metrics&#125;)<br>        <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Train Loss: <span class="hljs-subst">&#123;train_loss:<span class="hljs-number">.3</span>f&#125;</span>, Valid Loss: <span class="hljs-subst">&#123;val_loss:3f&#125;</span>, Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><br>    <span class="hljs-comment"># If you had a test set, this is how you could log it as a Summary metric</span><br>    wandb.summary[<span class="hljs-string">&#x27;test_accuracy&#x27;</span>] = <span class="hljs-number">0.8</span><br><br>    <span class="hljs-comment"># 🐝 Close your wandb run </span><br>    wandb.finish()<br></code></pre></td></tr></table></figure><p>您现在已经使用 wandb 训练了您的第一个模型！ 👆 单击上面的 wandb链接查看您的指标</p><h3 id="try-wb-alerts">Try W&amp;B Alerts</h3><p><strong><a href="https://docs.wandb.ai/guides/track/alert">W&amp;BAlerts</a></strong> 允许您将从 Python 代码触发的警报发送到您的 Slack或电子邮件。第一次发送 Slack 或电子邮件警报时，需要执行 2个步骤，这些警报由您的代码触发：</p><p>1) 在你的 W&amp;B <a href="https://wandb.ai/settings">UserSettings</a> 开启警报</p><p>2) 添加 <code>wandb.alert()</code> 到你的代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">wandb.alert(<br>    title=<span class="hljs-string">&quot;Low accuracy&quot;</span>, <br>    text=<span class="hljs-string">f&quot;Accuracy is below the acceptable threshold&quot;</span><br>)<br></code></pre></td></tr></table></figure><p>请参阅下面的最小示例以了解如何使用 wandb.alert，您可以在此处找到 <ahref="https://docs.wandb.ai/guides/track/alert">W&amp;BAlerts</a>的完整文档</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Start a wandb run</span><br>wandb.init(project=<span class="hljs-string">&quot;pytorch-intro&quot;</span>)<br><br><span class="hljs-comment"># Simulating a model training loop</span><br>acc_threshold = <span class="hljs-number">0.3</span><br><span class="hljs-keyword">for</span> training_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<br><br>    <span class="hljs-comment"># Generate a random number for accuracy</span><br>    accuracy = <span class="hljs-built_in">round</span>(random.random() + random.random(), <span class="hljs-number">3</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Accuracy is: <span class="hljs-subst">&#123;accuracy&#125;</span>, <span class="hljs-subst">&#123;acc_threshold&#125;</span>&#x27;</span>)<br>    <br>    <span class="hljs-comment"># 🐝 Log accuracy to wandb</span><br>    wandb.log(&#123;<span class="hljs-string">&quot;Accuracy&quot;</span>: accuracy&#125;)<br><br>    <span class="hljs-comment"># 🔔 If the accuracy is below the threshold, fire a W&amp;B Alert and stop the run</span><br>    <span class="hljs-keyword">if</span> accuracy &lt;= acc_threshold:<br>        <span class="hljs-comment"># 🐝 Send the wandb Alert</span><br>        wandb.alert(<br>            title=<span class="hljs-string">&#x27;Low Accuracy&#x27;</span>,<br>            text=<span class="hljs-string">f&#x27;Accuracy <span class="hljs-subst">&#123;accuracy&#125;</span> at step <span class="hljs-subst">&#123;training_step&#125;</span> is below the acceptable theshold, <span class="hljs-subst">&#123;acc_threshold&#125;</span>&#x27;</span>,<br>        )<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Alert triggered&#x27;</span>)<br>        <span class="hljs-keyword">break</span><br><br><span class="hljs-comment"># Mark the run as finished (useful in Jupyter notebooks)</span><br>wandb.finish()<br></code></pre></td></tr></table></figure><h2 id="visualize-predictions">Visualize predictions</h2><p>这包括如何在训练过程中使用 PyTorch 对 MNIST数据进行跟踪、可视化和比较模型预测。</p><p>你将学到如何：</p><ol type="1"><li>在模型训练或评估期间将指标、图像、文本等记录到<code>wandb.Table()</code></li><li>查看、排序、筛选、分组、加入、交互式查询和探索这些表</li><li>比较模型预测或结果：动态地跨越特定图像、超参数/模型版本或时间步长。</li></ol><h3 id="examples">Examples</h3><h4 id="compare-predicted-scores-for-specific-images">Compare predictedscores for specific images</h4><p><ahref="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*6z1980*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#compare-predictions-after-1-vs-5-epochs">实例：比较1 和 5 个训练周期后的预测</a></p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedNMme6Qj.png"alt="1 epoch vs 5 epochs of training" /><figcaption aria-hidden="true">1 epoch vs 5 epochs oftraining</figcaption></figure><p>直方图比较了两个模型之间的每类分数。每个直方图中顶部的绿色条代表模型“CNN-2,1 epoch”（id 0），它只训练了 1 个 epoch。底部的紫色条代表模型“CNN-2, 5epochs” (id 1)，它训练了 5 个epochs。图像被过滤到模型不一致的情况。例如，在第一行中，“4”在 1个时期后在所有可能的数字中获得高分，但在 5个时期后，它在正确标签上得分最高，而在其余部分得分非常低。</p><h4 id="focus-on-top-errors-over-time">Focus on top errors overtime</h4><p><ahref="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*1nxbzl7*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#top-errors-over-time">实例→</a></p><p>查看完整测试数据的不正确预测（过滤 "guess" != "truth"的行）。请注意，在 1 个训练时期后有 229 个错误猜测，但在 5 个时期后只有98 个。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined7g8nodn.png"alt="side by side, 1 vs 5 epochs of training" /><figcaption aria-hidden="true">side by side, 1 vs 5 epochs oftraining</figcaption></figure><h4 id="compare-model-performance-and-find-patterns">Compare modelperformance and find patterns</h4><p><ahref="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*8r828v*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#false-positives-grouped-by-guess">查看实例中的完整详细信息→</a></p><p>过滤出正确答案，然后按猜测分组，以查看错误分类图像的示例和真实标签的基本分布——并排显示两个模型。具有2X layer sizes和学习率的模型变体在左侧，基线在右侧。请注意，对于每个猜测的类，基线都会犯更多的错误。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedi5PP9AE.png"alt="grouped errors for baseline vs double variant" /><figcaption aria-hidden="true">grouped errors for baseline vs doublevariant</figcaption></figure><h3 id="sign-up-or-login">Sign up or login</h3><p><a href="https://wandb.ai/login">Sign up or login</a> W&amp;B以在浏览器中查看您的实验并与之互动。</p><p>在此示例中，我们使用 Google Colab作为方便的托管环境，但您可以从任何地方运行自己的训练脚本，并使用 W&amp;B的实验跟踪工具可视化指标。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">!pip install wandb -qqq<br></code></pre></td></tr></table></figure><p>登录您的帐户</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> wandb<br>wandb.login()<br><br>WANDB_PROJECT = <span class="hljs-string">&quot;mnist-viz&quot;</span><br></code></pre></td></tr></table></figure><h3 id="setup">0. Setup</h3><p>安装依赖项，下载 MNIST，并使用 PyTorch 创建训练和测试数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T <br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br>device = <span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br><br><span class="hljs-comment"># create train and test dataloaders</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_dataloader</span>(<span class="hljs-params">is_train, batch_size, <span class="hljs-built_in">slice</span>=<span class="hljs-number">5</span></span>):<br>    <span class="hljs-string">&quot;Get a training dataloader&quot;</span><br>    ds = torchvision.datasets.MNIST(root=<span class="hljs-string">&quot;.&quot;</span>, train=is_train, transform=T.ToTensor(), download=<span class="hljs-literal">True</span>)<br>    loader = torch.utils.data.DataLoader(dataset=ds, <br>                                         batch_size=batch_size, <br>                                         shuffle=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>, <br>                                         pin_memory=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> loader<br></code></pre></td></tr></table></figure><h3 id="define-the-model-and-training-schedule">1. Define the model andtraining schedule</h3><ul><li>设置要运行的纪元数，其中每个纪元包含一个训练步骤和一个验证（测试）步骤。（可选）配置每个测试步骤要记录的数据量。这里要可视化的批次数和每批次的图像数设置得较低，以简化演示。</li><li>定义一个简单的卷积神经网络（遵循 <ahref="https://github.com/yunjey/pytorch-tutorial">pytorch-tutorial</a>代码）。</li><li>使用 PyTorch 加载训练集和测试集</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Number of epochs to run</span><br><span class="hljs-comment"># Each epoch includes a training step and a test step, so this sets</span><br><span class="hljs-comment"># the number of tables of test predictions to log</span><br>EPOCHS = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># Number of batches to log from the test data for each test step</span><br><span class="hljs-comment"># (default set low to simplify demo)</span><br>NUM_BATCHES_TO_LOG = <span class="hljs-number">10</span> <span class="hljs-comment">#79</span><br><br><span class="hljs-comment"># Number of images to log per test batch</span><br><span class="hljs-comment"># (default set low to simplify demo)</span><br>NUM_IMAGES_PER_BATCH = <span class="hljs-number">32</span> <span class="hljs-comment">#128</span><br><br><span class="hljs-comment"># training configuration and hyperparameters</span><br>NUM_CLASSES = <span class="hljs-number">10</span><br>BATCH_SIZE = <span class="hljs-number">32</span><br>LEARNING_RATE = <span class="hljs-number">0.001</span><br>L1_SIZE = <span class="hljs-number">32</span><br>L2_SIZE = <span class="hljs-number">64</span><br><span class="hljs-comment"># changing this may require changing the shape of adjacent layers</span><br>CONV_KERNEL_SIZE = <span class="hljs-number">5</span><br><br><span class="hljs-comment"># define a two-layer convolutional neural network</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br>        self.layer1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, L1_SIZE, CONV_KERNEL_SIZE, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(L1_SIZE),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.layer2 = nn.Sequential(<br>            nn.Conv2d(L1_SIZE, L2_SIZE, CONV_KERNEL_SIZE, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(L2_SIZE),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.fc = nn.Linear(<span class="hljs-number">7</span>*<span class="hljs-number">7</span>*L2_SIZE, NUM_CLASSES)<br>        self.softmax = nn.Softmax(NUM_CLASSES)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># uncomment to see the shape of a given layer:</span><br>        <span class="hljs-comment">#print(&quot;x: &quot;, x.size())</span><br>        out = self.layer1(x)<br>        out = self.layer2(out)<br>        out = out.reshape(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.fc(out)<br>        <span class="hljs-keyword">return</span> out<br><br>train_loader = get_dataloader(is_train=<span class="hljs-literal">True</span>, batch_size=BATCH_SIZE)<br>test_loader = get_dataloader(is_train=<span class="hljs-literal">False</span>, batch_size=<span class="hljs-number">2</span>*BATCH_SIZE)<br><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="run-training-and-log-test-predictions">2. Run training and logtest predictions</h3><p>对于每个时期，运行一个训练步骤和一个测试步骤。对于每个测试步骤，创建一个wandb.Table()来存储测试预测。这些可以在您的浏览器中进行可视化、动态查询和并排比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ✨ W&amp;B: Initialize a new run to track this model&#x27;s training</span><br>wandb.init(project=<span class="hljs-string">&quot;table-quickstart&quot;</span>)<br><br><span class="hljs-comment"># ✨ W&amp;B: Log hyperparameters using config</span><br>cfg = wandb.config<br>cfg.update(&#123;<span class="hljs-string">&quot;epochs&quot;</span> : EPOCHS, <span class="hljs-string">&quot;batch_size&quot;</span>: BATCH_SIZE, <span class="hljs-string">&quot;lr&quot;</span> : LEARNING_RATE,<br>            <span class="hljs-string">&quot;l1_size&quot;</span> : L1_SIZE, <span class="hljs-string">&quot;l2_size&quot;</span>: L2_SIZE,<br>            <span class="hljs-string">&quot;conv_kernel&quot;</span> : CONV_KERNEL_SIZE,<br>            <span class="hljs-string">&quot;img_count&quot;</span> : <span class="hljs-built_in">min</span>(<span class="hljs-number">10000</span>, NUM_IMAGES_PER_BATCH*NUM_BATCHES_TO_LOG)&#125;)<br><br><span class="hljs-comment"># define model, loss, and optimizer</span><br>model = ConvNet(NUM_CLASSES).to(device)<br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)<br><br><span class="hljs-comment"># convenience funtion to log predictions for a batch of test images</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_test_predictions</span>(<span class="hljs-params">images, labels, outputs, predicted, test_table, log_counter</span>):<br>  <span class="hljs-comment"># obtain confidence scores for all classes</span><br>  scores = F.softmax(outputs.data, dim=<span class="hljs-number">1</span>)<br>  log_scores = scores.cpu().numpy()<br>  log_images = images.cpu().numpy()<br>  log_labels = labels.cpu().numpy()<br>  log_preds = predicted.cpu().numpy()<br>  <span class="hljs-comment"># adding ids based on the order of the images</span><br>  _<span class="hljs-built_in">id</span> = <span class="hljs-number">0</span><br>  <span class="hljs-keyword">for</span> i, l, p, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(log_images, log_labels, log_preds, log_scores):<br>    <span class="hljs-comment"># add required info to data table:</span><br>    <span class="hljs-comment"># id, image pixels, model&#x27;s guess, true label, scores for all classes</span><br>    img_id = <span class="hljs-built_in">str</span>(_<span class="hljs-built_in">id</span>) + <span class="hljs-string">&quot;_&quot;</span> + <span class="hljs-built_in">str</span>(log_counter)<br>    test_table.add_data(img_id, wandb.Image(i), p, l, *s)<br>    _<span class="hljs-built_in">id</span> += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> _<span class="hljs-built_in">id</span> == NUM_IMAGES_PER_BATCH:<br>      <span class="hljs-keyword">break</span><br><br><span class="hljs-comment"># train the model</span><br>total_step = <span class="hljs-built_in">len</span>(train_loader)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):<br>    <span class="hljs-comment"># training step</span><br>    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        images = images.to(device)<br>        labels = labels.to(device)<br>        <span class="hljs-comment"># forward pass</span><br>        outputs = model(images)<br>        loss = criterion(outputs, labels)<br>        <span class="hljs-comment"># backward and optimize</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>  <br>        <span class="hljs-comment"># ✨ W&amp;B: Log loss over training steps, visualized in the UI live</span><br>        wandb.log(&#123;<span class="hljs-string">&quot;loss&quot;</span> : loss&#125;)<br>        <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;&#x27;</span><br>                .<span class="hljs-built_in">format</span>(epoch+<span class="hljs-number">1</span>, EPOCHS, i+<span class="hljs-number">1</span>, total_step, loss.item()))<br>            <br><br>    <span class="hljs-comment"># ✨ W&amp;B: Create a Table to store predictions for each test step</span><br>    columns=[<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;guess&quot;</span>, <span class="hljs-string">&quot;truth&quot;</span>]<br>    <span class="hljs-keyword">for</span> digit <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>      columns.append(<span class="hljs-string">&quot;score_&quot;</span> + <span class="hljs-built_in">str</span>(digit))<br>    test_table = wandb.Table(columns=columns)<br><br>    <span class="hljs-comment"># test the model</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    log_counter = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        correct = <span class="hljs-number">0</span><br>        total = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:<br>            images = images.to(device)<br>            labels = labels.to(device)<br>            outputs = model(images)<br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> log_counter &lt; NUM_BATCHES_TO_LOG:<br>              log_test_predictions(images, labels, outputs, predicted, test_table, log_counter)<br>              log_counter += <span class="hljs-number">1</span><br>            total += labels.size(<span class="hljs-number">0</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>        acc = <span class="hljs-number">100</span> * correct / total<br>        <span class="hljs-comment"># ✨ W&amp;B: Log accuracy across training epochs, to visualize in the UI</span><br>        wandb.log(&#123;<span class="hljs-string">&quot;epoch&quot;</span> : epoch, <span class="hljs-string">&quot;acc&quot;</span> : acc&#125;)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test Accuracy of the model on the 10000 test images: &#123;&#125; %&#x27;</span>.<span class="hljs-built_in">format</span>(acc))<br><br>    <span class="hljs-comment"># ✨ W&amp;B: Log predictions table to wandb</span><br>    wandb.log(&#123;<span class="hljs-string">&quot;test_predictions&quot;</span> : test_table&#125;)<br><br><span class="hljs-comment"># ✨ W&amp;B: Mark the run as complete (useful for multi-cell notebook)</span><br>wandb.finish()<br></code></pre></td></tr></table></figure><h2 id="tune-hyperparameters">Tune hyperparameters</h2><p><ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W&amp;B.ipynb">在此处试用Colab Notebook →</a></p><p>在高维超参数空间中搜索以找到性能最高的模型可能会很快变得笨拙。超参数扫描提供了一种有组织且高效的方式来进行模型大逃杀并选择最准确的模型。他们通过自动搜索超参数值的组合（例如learning rate, batch size, number of hidden layers, optimizertype）来找到最佳值来实现这一点。</p><p>在本教程中，我们将了解如何使用 Weights &amp; Biases 通过 3个简单步骤运行复杂的超参数扫描。</p><h3 id="follow-along-with-a-video-tutorial">Follow along with a <ahref="http://wandb.me/sweeps-video">video tutorial</a>!</h3><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedWVKkMWw.png"alt="tune hyperparameters" /><figcaption aria-hidden="true">tune hyperparameters</figcaption></figure><h3 id="setup-1">Setup</h3><p>首先安装实验跟踪库并设置您的免费 W&amp;B 帐户：</p><ol type="1"><li>使用 <code>pip install</code> 安装</li><li><code>import</code> Python 所需依赖</li><li><code>.login()</code> 这样您就可以将指标记录到您的项目中</li></ol><p>如果您以前从未使用过 Weights &amp;Biases，登录电话会给您一个注册帐户的链接。 W&amp;B可免费用于个人和学术项目！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">!pip install wandb -Uq<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> wandb<br><br>wandb.login()<br></code></pre></td></tr></table></figure><h3 id="step-1.-define-the-sweep">Step 1️⃣. Define the Sweep</h3><p>从根本上说，Sweep将尝试一堆超参数值的策略与评估它们的代码结合在一起。您只需要以<ahref="https://docs.wandb.com/sweeps/configuration">配置</a>的形式定义您的策略。</p><p>当您像这样在笔记本中设置 Sweep时，该配置对象是一个嵌套字典。当您通过命令行运行 Sweep时，配置对象是一个 <ahref="https://docs.wandb.com/sweeps/quickstart#2-sweep-config">YAMLfile</a>。</p><p>让我们一起了解 Sweep配置的定义。我们会慢慢来，这样我们就有机会解释每个组件。在典型的 Sweep管道中，此步骤将在单个分配中完成。</p><h4 id="pick-a-method">Pick a <code>method</code></h4><p>我们需要定义的第一件事是选择新参数值的 <code>method</code>。</p><p>我们提供以下搜索 <code>methods</code>：</p><ul><li><strong><code>grid</code> Search </strong>–迭代超参数值的每个组合。非常有效，但计算量大。</li><li><strong><code>random</code> Search</strong> – 根据提供的<code>distribution</code> 随机选择每个新组合。出乎意料的有效！</li><li><strong><code>bayesian</code> Search</strong> –创建一个度量分数作为超参数函数的概率模型，并选择具有提高度量的高概率的参数。适用于少量连续参数但扩展性差。</li></ul><p><code>random</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">sweep_config = &#123;<br>    <span class="hljs-string">&#x27;method&#x27;</span>: <span class="hljs-string">&#x27;random&#x27;</span><br>    &#125;<br></code></pre></td></tr></table></figure><p>对于 <code>bayesian</code> Sweeps，您还需要告诉我们一些关于您的metric的信息。我们需要知道它的名称，以便我们可以在模型输出中找到它，我们需要知道您的目标是最小化它（例如，如果它是squared error）还是最大化它（例如，如果它是 accuracy）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">metric = &#123;<br>    <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;loss&#x27;</span>,<br>    <span class="hljs-string">&#x27;goal&#x27;</span>: <span class="hljs-string">&#x27;minimize&#x27;</span>   <br>    &#125;<br><br>sweep_config[<span class="hljs-string">&#x27;metric&#x27;</span>] = metric<br></code></pre></td></tr></table></figure><p>如果您没有运行 <code>bayesian</code>Sweep，则不必这样做，但无论如何将其包含在您的 <code>sweep_config</code>中并不是一个坏主意，以防您以后改变主意。记录这样的事情也是很好的再现性实践，以防万一您或其他人在6 个月或 6 年后回到您的 Sweep 并且不知道 <code>val_G_batch</code>应该是高还是低。</p><h4 id="name-the-hyperparameters">Name thehyper<code>parameters</code></h4><p>一旦您选择了一种 <code>method</code>来尝试超参数的新值，您需要定义这些 <code>parameters</code>是什么</p><p>大多数时候，这一步很简单：您只需为 <code>parameter</code>命名并指定参数的合法 <code>values</code> 列表。</p><p>例如，当我们为我们的网络选择 <code>optimizer</code>时，只有有限数量的选项。在这里，我们坚持使用两个最受欢迎的选择，<code>adam</code>和<code>sgd</code>。即使对于具有潜在无限选项的超参数，通常也只尝试几个选择<code>values</code> 才有意义，就像我们在此处对隐藏层<code>layer_size</code> 和 <code>dropout</code> 所做的那样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">parameters_dict = &#123;<br>    <span class="hljs-string">&#x27;optimizer&#x27;</span>: &#123;<br>        <span class="hljs-string">&#x27;values&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>, <span class="hljs-string">&#x27;sgd&#x27;</span>]<br>        &#125;,<br>    <span class="hljs-string">&#x27;fc_layer_size&#x27;</span>: &#123;<br>        <span class="hljs-string">&#x27;values&#x27;</span>: [<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>]<br>        &#125;,<br>    <span class="hljs-string">&#x27;dropout&#x27;</span>: &#123;<br>          <span class="hljs-string">&#x27;values&#x27;</span>: [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>]<br>        &#125;,<br>    &#125;<br><br>sweep_config[<span class="hljs-string">&#x27;parameters&#x27;</span>] = parameters_dict<br></code></pre></td></tr></table></figure><p>通常情况下，有些超参数我们不想在此 Sweep 中改变，但我们仍希望在我们的<code>sweep_config</code> 中设置它们。</p><p>在那种情况下，我们直接设置 <code>value</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">parameters_dict.update(&#123;<br>    <span class="hljs-string">&#x27;epochs&#x27;</span>: &#123;<br>        <span class="hljs-string">&#x27;value&#x27;</span>: <span class="hljs-number">1</span>&#125;<br>    &#125;)<br></code></pre></td></tr></table></figure><p>对于 <code>grid</code> 搜索，这就是您所需要的。</p><p>对于 <code>random</code> 搜索，参数的所有 <code>values</code>在给定运行中被选择的可能性相同。</p><p>如果这样做不行，您可以改为指定命名 <code>distribution</code>及其参数，例如 <code>normal</code> 分布的均值 <code>mu</code> 和标准差<code>sigma</code>。</p><p>在<ahref="https://docs.wandb.com/sweeps/configuration#distributions">此处</a>查看有关如何设置随机变量分布的更多信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">parameters_dict.update(&#123;<br>    <span class="hljs-string">&#x27;learning_rate&#x27;</span>: &#123;<br>        <span class="hljs-comment"># a flat distribution between 0 and 0.1</span><br>        <span class="hljs-string">&#x27;distribution&#x27;</span>: <span class="hljs-string">&#x27;uniform&#x27;</span>,<br>        <span class="hljs-string">&#x27;min&#x27;</span>: <span class="hljs-number">0</span>,<br>        <span class="hljs-string">&#x27;max&#x27;</span>: <span class="hljs-number">0.1</span><br>      &#125;,<br>    <span class="hljs-string">&#x27;batch_size&#x27;</span>: &#123;<br>        <span class="hljs-comment"># integers between 32 and 256</span><br>        <span class="hljs-comment"># with evenly-distributed logarithms </span><br>        <span class="hljs-string">&#x27;distribution&#x27;</span>: <span class="hljs-string">&#x27;q_log_uniform_values&#x27;</span>,<br>        <span class="hljs-string">&#x27;q&#x27;</span>: <span class="hljs-number">8</span>,<br>        <span class="hljs-string">&#x27;min&#x27;</span>: <span class="hljs-number">32</span>,<br>        <span class="hljs-string">&#x27;max&#x27;</span>: <span class="hljs-number">256</span>,<br>      &#125;<br>    &#125;)<br></code></pre></td></tr></table></figure><p>当我们完成后，<code>sweep_config</code>是一个嵌套的字典，它准确地指定了我们有兴趣尝试哪些<code>parameters</code> 以及我们将使用什么 <code>method</code>来尝试它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pprint<br><br>pprint.pprint(sweep_config)<br></code></pre></td></tr></table></figure><p>但这不是所有的配置选项！</p><p>例如，我们还提供了使用 <ahref="https://arxiv.org/pdf/1603.06560.pdf">HyperBand</a> 调度算法<code>early_terminate</code> 运行的选项。在<ahref="https://docs.wandb.com/sweeps/configuration#stopping-criteria">这里</a>查看更多。</p><p>您可以在<ahref="https://docs.wandb.com/library/sweeps/configuration">此处</a>找到所有配置选项的列表，并在<ahref="https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion">此处</a>找到大量YAML 格式的示例。</p><h3 id="step-2.-initialize-the-sweep">Step 2️⃣. Initialize the Sweep</h3><p>一旦您定义了搜索策略，就该设置一些东西来实现它了。</p><p>负责我们 Sweep 的 clockwork taskmaster 被称为 <em>SweepController</em>。每次运行完成时，它将发出一组新的指令来描述要执行的新运行。这些指令由实际执行运行的<em>agents</em> 获取。</p><p>在典型的 Sweep 中，Controller位于我们的机器上，而完成运行的代理位于您的机器上，如下图所示。这种分工使得只需添加更多机器来运行代理就可以非常容易地扩展Sweeps！</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedzlbw3vQ.png"alt="sweeps-diagram" /><figcaption aria-hidden="true">sweeps-diagram</figcaption></figure><p>我们可以通过使用适当的 <code>sweep_config</code> 和<code>project</code> 名称调用 <code>wandb.sweep</code> 来结束 SweepController。</p><p>此函数返回一个 <code>sweep_id</code>，我们稍后将使用它来将 agents分配给此 Controller。</p><p>旁注：在命令行上，此功能被替换为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wandb sweep config.yaml<br></code></pre></td></tr></table></figure><p><ahref="https://docs.wandb.com/sweeps/quickstart">了解更多关于在命令行中使用Sweeps ➡</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sweep_id = wandb.sweep(sweep_config, project=<span class="hljs-string">&quot;pytorch-sweeps-demo&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="step-3.-run-the-sweep-agent">Step 3️⃣. Run the Sweep agent</h3><h4 id="define-your-training-procedure">Define Your TrainingProcedure</h4><p>在我们实际执行 sweep 之前，我们需要定义使用这些值的训练过程。</p><p>在下面的函数中，我们在 PyTorch中定义了一个简单的全连接神经网络，并添加了以下 <code>wandb</code>工具来记录模型指标、可视化性能和输出并跟踪我们的实验：</p><ul><li><ahref="https://docs.wandb.com/library/init"><strong><code>wandb.init()</code></strong></a>– 初始化新的 W&amp;B 运行。每次运行都是训练功能的一次执行。</li><li><ahref="https://docs.wandb.com/library/config"><strong><code>wandb.config</code></strong></a>– 将所有超参数保存在配置对象中，以便记录它们。在<ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb">此处</a>阅读有关如何使用<code>wandb.config</code> 的更多信息。</li><li><ahref="https://docs.wandb.com/library/log"><strong><code>wandb.log()</code></strong></a>– 将模型行为记录到 W&amp;B。在这里，我们只记录性能；有关可以使用<code>wandb.log</code> 记录的所有其他富媒体，请参阅此 <ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_(Almost)_Anything_with_W%26B_Media.ipynb">Colab</a>。</li></ul><p>有关使用 PyTorch 检测 W&amp;B 的更多详细信息，请参阅此 <ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb">Colab</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-comment"># Initialize a new wandb run</span><br>    <span class="hljs-keyword">with</span> wandb.init(config=config):<br>        <span class="hljs-comment"># If called by wandb.agent, as below,</span><br>        <span class="hljs-comment"># this config will be set by Sweep Controller</span><br>        config = wandb.config<br><br>        loader = build_dataset(config.batch_size)<br>        network = build_network(config.fc_layer_size, config.dropout)<br>        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)<br><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>            avg_loss = train_epoch(network, loader, optimizer)<br>            wandb.log(&#123;<span class="hljs-string">&quot;loss&quot;</span>: avg_loss, <span class="hljs-string">&quot;epoch&quot;</span>: epoch&#125;)           <br></code></pre></td></tr></table></figure><p>这个单元格定义了我们训练过程的四个部分：<code>build_dataset</code>,<code>build_network</code>, <code>build_optimizer</code>和<code>train_epoch</code>.</p><p>所有这些都是基本 PyTorch 管道的标准部分，它们的实现不受使用 W&amp;B的影响，因此我们不会对它们发表评论。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_dataset</span>(<span class="hljs-params">batch_size</span>):<br>   <br>    transform = transforms.Compose(<br>        [transforms.ToTensor(),<br>         transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))])<br>    <span class="hljs-comment"># download MNIST training dataset</span><br>    dataset = datasets.MNIST(<span class="hljs-string">&quot;.&quot;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>,<br>                             transform=transform)<br>    sub_dataset = torch.utils.data.Subset(<br>        dataset, indices=<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">5</span>))<br>    loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size)<br><br>    <span class="hljs-keyword">return</span> loader<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_network</span>(<span class="hljs-params">fc_layer_size, dropout</span>):<br>    network = nn.Sequential(  <span class="hljs-comment"># fully-connected, single hidden layer</span><br>        nn.Flatten(),<br>        nn.Linear(<span class="hljs-number">784</span>, fc_layer_size), nn.ReLU(),<br>        nn.Dropout(dropout),<br>        nn.Linear(fc_layer_size, <span class="hljs-number">10</span>),<br>        nn.LogSoftmax(dim=<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">return</span> network.to(device)<br>        <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_optimizer</span>(<span class="hljs-params">network, optimizer, learning_rate</span>):<br>    <span class="hljs-keyword">if</span> optimizer == <span class="hljs-string">&quot;sgd&quot;</span>:<br>        optimizer = optim.SGD(network.parameters(),<br>                              lr=learning_rate, momentum=<span class="hljs-number">0.9</span>)<br>    <span class="hljs-keyword">elif</span> optimizer == <span class="hljs-string">&quot;adam&quot;</span>:<br>        optimizer = optim.Adam(network.parameters(),<br>                               lr=learning_rate)<br>    <span class="hljs-keyword">return</span> optimizer<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch</span>(<span class="hljs-params">network, loader, optimizer</span>):<br>    cumu_loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> _, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>        data, target = data.to(device), target.to(device)<br>        optimizer.zero_grad()<br><br>        <span class="hljs-comment"># ➡ Forward pass</span><br>        loss = F.nll_loss(network(data), target)<br>        cumu_loss += loss.item()<br><br>        <span class="hljs-comment"># ⬅ Backward pass + weight update</span><br>        loss.backward()<br>        optimizer.step()<br><br>        wandb.log(&#123;<span class="hljs-string">&quot;batch loss&quot;</span>: loss.item()&#125;)<br><br>    <span class="hljs-keyword">return</span> cumu_loss / <span class="hljs-built_in">len</span>(loader)<br></code></pre></td></tr></table></figure><p>现在，我们准备开始 sweeping 了！</p><p>Sweep Controllers，就像我们通过运行 <code>wandb.sweep</code>制作的控制器一样，坐等有人要求他们提供 <code>config</code>来试用。</p><p>某人是 <code>agent</code>，他们是用 <code>wandb.agent</code>创建的。要开始，agent 只需要知道</p><ol type="1"><li>它是 (<code>sweep_id</code>) 的一部分</li><li>它应该运行哪个函数（这里是 <code>train</code>）</li><li>（可选）有多少配置要求控制器（<code>count</code>）</li></ol><p>仅供参考，您可以在不同的计算资源上启动具有相同 <code>sweep_id</code>的多个 <code>agent</code>，Controller 将确保它们根据<code>sweep_config</code>中制定的策略协同工作。这使得在尽可能多的节点上扩展 Sweeps变得轻而易举！</p><p>旁注：在命令行上，此功能被替换为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wandb agent sweep_id<br></code></pre></td></tr></table></figure><p><ahref="https://docs.wandb.com/sweeps/quickstart">了解更多关于在命令行中使用Sweeps ➡</a></p><p>下面的单元格将启动一个运行 <code>train</code> 5 次的<code>agent</code>，使用 Sweep Controller返回的随机生成的超参数值。执行时间不到 5 分钟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">wandb.agent(sweep_id, train, count=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><h3 id="visualize-sweep-results">Visualize Sweep Results</h3><h4 id="parallel-coordinates-plot">Parallel Coordinates Plot</h4><p>此图将超参数值映射到模型指标。它对于磨练导致最佳模型性能的超参数组合很有用。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined5e190366778ad831455f9af2_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695138341_image.png"alt="hyperparameters map to metrics" /><figcaption aria-hidden="true">hyperparameters map tometrics</figcaption></figure><h4 id="hyperparameter-importance-plot">Hyperparameter ImportancePlot</h4><p>超参数重要性图表明哪些超参数是指标的最佳预测因子。我们报告特征重要性（来自随机森林模型）和相关性（隐式线性模型）。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined5e190367778ad820b35f9af5_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695757573_image.png"alt="parameter importance" /><figcaption aria-hidden="true">parameter importance</figcaption></figure><p>这些可视化可以通过磨练最重要的参数（和值范围）来帮助您节省运行昂贵的超参数优化的时间和资源，因此值得进一步探索。</p><h3 id="get-your-hands-dirty-with-sweeps">Get your hands dirty withsweeps</h3><p>我们创建了一个简单的训练脚本和一些 <ahref="https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion">sweepconfigs</a> 风格供您使用。我们强烈建议您尝试一下。</p><p>该存储库还提供了一些示例，可帮助您尝试更高级的 sweep 功能，例如 <ahref="https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/us0ifmrf?workspace=user-lavanyashukla&amp;_gl=1*1h57q6p*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjYyMjIyOS43LjAuMTY4NjYyMjIyOS42MC4wLjA.">BayesianHyperband</a> 和 <ahref="https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/xbs2wm5e?workspace=user-lavanyashukla&amp;_gl=1*5hk37j*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjYyMjIyOS43LjAuMTY4NjYyMjIyOS42MC4wLjA.">Hyperopt</a>。</p><h2 id="track-models-and-datasets">Track models and datasets</h2><p><ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W&amp;B_Artifacts.ipynb">在此处试用Colab Notebook →</a></p><p>在此笔记本中，我们将向您展示如何使用 W&amp;B Artifacts 跟踪您的 ML实验管道。</p><h4 id="follow-along-with-a-video-tutorial-1">Follow along with a <ahref="http://tiny.cc/wb-artifacts-video">video tutorial</a>!</h4><h5 id="what-are-artifacts-and-why-should-i-care">What are Artifacts andWhy Should I Care?</h5><p>“artifact”，如希腊双耳瓶🏺，是一个生产的对象——一个过程的输出。在 ML中，最重要的工件是 <em>datasets</em> 和 <em>models</em>。</p><p>而且，就像 <ahref="https://indianajones.fandom.com/wiki/Cross_of_Coronado">Cross ofCoronado</a>一样，这些重要的文物属于博物馆！也就是说，应该对它们进行分类和组织，以便您、您的团队和整个ML 社区可以向它们学习。毕竟，那些不跟踪训练的人注定要重蹈覆辙。</p><p>使用我们的 Artifacts API，您可以将 <code>Artifacts</code> 记录为W&amp;B <code>Runs</code> 的输出，或使用 <code>Artifacts</code> 作为<code>Runs</code>的输入，如此图所示，其中训练运行接受数据集并生成模型。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedsimple%20artifact%20diagram%202.png"alt="simple artifact diagram" /><figcaption aria-hidden="true">simple artifact diagram</figcaption></figure><p>由于一次运行可以使用另一次的输出作为输入，因此 Artifacts 和 Runs一起形成了一个有向图——实际上是一个二分 <ahref="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a>！ --带有 <code>Artifact</code>s 和 <code>Run</code>s 的节点，以及将<code>Run</code>s 连接到它们消耗或生产的 <code>Artifact</code>s的箭头。</p><h3 id="install-and-import">0️⃣ Install and Import</h3><p>Artifacts 是我们 Python 库的一部分，从 <code>0.9.2</code>版开始。</p><p>与 ML Python 堆栈的大多数部分一样，它可以通过 <code>pip</code>获得。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Compatible with wandb version 0.9.2+</span><br>!pip install wandb -qqq<br>!apt install tree<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> wandb<br></code></pre></td></tr></table></figure><h3 id="log-a-dataset">1️⃣ Log a Dataset</h3><p>首先，让我们定义一些 Artifacts。</p><p>此示例基于此 PyTorch <ahref="https://github.com/pytorch/examples/tree/master/mnist/">"BasicMNIST Example"</a>，但可以在 <ahref="http://wandb.me/artifacts-colab">TensorFlow</a>、任何其他框架或纯Python 中轻松完成。</p><p>我们从 <code>Dataset</code>s 开始：</p><ul><li>一个 <code>train</code>ing set，用于选择参数，</li><li>一个 <code>validation</code> set，用于选择超参数，</li><li>一个 <code>test</code>ing set，用于评估最终模型</li></ul><p>下面的第一个单元格定义了这三个数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random <br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset<br><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># Ensure deterministic behavior</span><br>torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>random.seed(<span class="hljs-number">0</span>)<br>torch.manual_seed(<span class="hljs-number">0</span>)<br>torch.cuda.manual_seed_all(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># Device configuration</span><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-comment"># Data parameters</span><br>num_classes = <span class="hljs-number">10</span><br>input_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br><br><span class="hljs-comment"># drop slow mirror from list of MNIST mirrors</span><br>torchvision.datasets.MNIST.mirrors = [mirror <span class="hljs-keyword">for</span> mirror <span class="hljs-keyword">in</span> torchvision.datasets.MNIST.mirrors<br>                                      <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> mirror.startswith(<span class="hljs-string">&quot;http://yann.lecun.com&quot;</span>)]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load</span>(<span class="hljs-params">train_size=<span class="hljs-number">50_000</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    # Load the data</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># the data, split between train and test sets</span><br>    train = torchvision.datasets.MNIST(<span class="hljs-string">&quot;./&quot;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>)<br>    test = torchvision.datasets.MNIST(<span class="hljs-string">&quot;./&quot;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>)<br>    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)<br><br>    <span class="hljs-comment"># split off a validation set for hyperparameter tuning</span><br>    x_train, x_val = x_train[:train_size], x_train[train_size:]<br>    y_train, y_val = y_train[:train_size], y_train[train_size:]<br><br>    training_set = TensorDataset(x_train, y_train)<br>    validation_set = TensorDataset(x_val, y_val)<br>    test_set = TensorDataset(x_test, y_test)<br><br>    datasets = [training_set, validation_set, test_set]<br><br>    <span class="hljs-keyword">return</span> datasets<br></code></pre></td></tr></table></figure><p>这建立了一个模式，我们将在这个例子中看到重复：将数据记录为工件的代码包裹在生成该数据的代码周围。在这种情况下，用于<code>load</code>ing 数据的代码与用于 <code>load_and_log</code>ging数据的代码分开。</p><p>这是很好的做法！</p><p>为了将这些数据集记录为工件，我们只需要</p><ol type="1"><li>使用 <code>wandb.init</code> 创建 <code>Run</code>，(L4)</li><li>为数据集 (L10) 创建一个 <code>Artifact</code>，以及</li><li>保存并记录相关 <code>file</code>s（L20、L23）。</li></ol><p>查看下面代码单元的示例，然后展开后面的部分以了解更多详细信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_and_log</span>():<br><br>    <span class="hljs-comment"># 🚀 start a run, with a type to label it and a project it can call home</span><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;load-data&quot;</span>) <span class="hljs-keyword">as</span> run:<br>        <br>        datasets = load()  <span class="hljs-comment"># separate code for loading the datasets</span><br>        names = [<span class="hljs-string">&quot;training&quot;</span>, <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>]<br><br>        <span class="hljs-comment"># 🏺 create our Artifact</span><br>        raw_data = wandb.Artifact(<br>            <span class="hljs-string">&quot;mnist-raw&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;dataset&quot;</span>,<br>            description=<span class="hljs-string">&quot;Raw MNIST dataset, split into train/val/test&quot;</span>,<br>            metadata=&#123;<span class="hljs-string">&quot;source&quot;</span>: <span class="hljs-string">&quot;torchvision.datasets.MNIST&quot;</span>,<br>                      <span class="hljs-string">&quot;sizes&quot;</span>: [<span class="hljs-built_in">len</span>(dataset) <span class="hljs-keyword">for</span> dataset <span class="hljs-keyword">in</span> datasets]&#125;)<br><br>        <span class="hljs-keyword">for</span> name, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(names, datasets):<br>            <span class="hljs-comment"># 🐣 Store a new file in the artifact, and write something into its contents.</span><br>            <span class="hljs-keyword">with</span> raw_data.new_file(name + <span class="hljs-string">&quot;.pt&quot;</span>, mode=<span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> file:<br>                x, y = data.tensors<br>                torch.save((x, y), file)<br><br>        <span class="hljs-comment"># ✍️ Save the artifact to W&amp;B.</span><br>        run.log_artifact(raw_data)<br><br>load_and_log()<br></code></pre></td></tr></table></figure><h4 id="wandb.init">🚀 <code>wandb.init</code></h4><p>当我们制作将要产生 <code>Artifact</code>s 的<code>Run</code>时，我们需要说明它属于哪个 <code>project</code>。</p><p>根据您的工作流程，项目可能大到<code>car-that-drives-itself</code>，也可能小到<code>iterative-architecture-experiment-117</code>。</p><p>Depending on your workflow, a project might be as big as<code>car-that-drives-itself</code> or as small as<code>iterative-architecture-experiment-117</code>.</p><blockquote><p>👍规则：如果可以，请将所有共享 <code>Artifact</code>s 的<code>Run</code>s 保留在一个项目中。这使事情变得简单，但不要担心<code>Artifact</code>s 可以跨项目移植！</p></blockquote><p>为了帮助跟踪您可能运行的所有不同类型的作业，在进行 <code>Run</code>s时提供 <code>job_type</code> 很有用。这可以使您的 Artifacts图表保持整洁。</p><blockquote><p>👍规则：<code>job_type</code>应该是描述性的，并且对应于你的管道的单个步骤。在这里，我们将<code>load</code>ing 数据与 <code>preprocess</code>ing 数据分开。</p></blockquote><h4 id="wandb.artifact">🏺 <code>wandb.Artifact</code></h4><p>要将某物记录为 <code>Artifact</code>，我们必须首先创建一个<code>Artifact</code> 对象。</p><p>每个 <code>Artifact</code> 都有一个<code>name</code>——这是第一个参数设置的名称。</p><blockquote><p>👍的规则：<code>name</code>应该是描述性的，但易于记忆和输入——我们喜欢使用连字符分隔的名称，并与代码中的变量名相对应。</p></blockquote><p>它也有一个 <code>type</code>。就像 <code>Run</code>s 的<code>job_types</code> 一样，它用于组织 <code>Run</code>s 和<code>Artifact</code>s 的图表。</p><blockquote><p>👍的规则：<code>type</code> 应该简单：比<code>mnist-data-YYYYMMDD</code> 更像 <code>dataset</code> 或<code>model</code>。</p></blockquote><p>您还可以附加 <code>description</code> 和一些<code>metadata</code>，作为字典。<code>metadata</code> 只需要可序列化为JSON。</p><blockquote><p>👍规则：<code>metadata</code>应尽可能具有描述性。</p></blockquote><h4 id="artifact.new_file-and-run.log_artifact">🐣<code>artifact.new_file</code> and ✍️ <code>run.log_artifact</code></h4><p>一旦我们创建了一个 <code>Artifact</code>对象，我们需要向它添加文件。</p><p>您没看错：带有 s 的 <em>files</em>。<code>Artifact</code>s的结构类似于目录，包含文件和子目录。</p><blockquote><p>👍规则：只要有必要，将 <code>Artifact</code>的内容拆分为多个文件。如果需要扩展，这将有所帮助！</p></blockquote><p>我们使用 <code>new_file</code> 方法同时写入文件并将其附加到<code>Artifact</code>。下面，我们将使用 <code>add_file</code>方法，它将这两个步骤分开</p><p>添加完所有文件后，我们需要将 <code>log_artifact</code> 添加到 <ahref="https://wandb.ai/?_gl=1*r07jdw*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjY1MzgzNC45LjEuMTY4NjY1MzgzOS41NS4wLjA.">wandb.ai</a>。</p><p>您会注意到一些 URL 出现在输出中，包括一个用于运行页面的URL。您可以在此处查看 <code>Run</code> 结果，包括已记录的任何<code>Artifact</code>s。</p><p>我们将在下面看到一些示例，这些示例可以更好地利用“运行”页面的其他组件。</p><h3 id="use-a-logged-dataset-artifact">2️⃣ Use a Logged DatasetArtifact</h3><p>与博物馆中的 artifacts 不同，W&amp;B 中的 <code>Artifact</code>s旨在使用，而不仅仅是存储。</p><p>让我们看看它是什么样的。</p><p>下面的单元格定义了一个管道步骤，该步骤接收原始数据集并使用它来生成<code>preprocess</code>ed 数据集：<code>normalize</code>d和正确整形。</p><p>再次注意，我们从与 <code>wandb</code>接口的代码中分离出了代码的主体，即 <code>preprocess</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess</span>(<span class="hljs-params">dataset, normalize=<span class="hljs-literal">True</span>, expand_dims=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    ## Prepare the data</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, y = dataset.tensors<br><br>    <span class="hljs-keyword">if</span> normalize:<br>        <span class="hljs-comment"># Scale images to the [0, 1] range</span><br>        x = x.<span class="hljs-built_in">type</span>(torch.float32) / <span class="hljs-number">255</span><br><br>    <span class="hljs-keyword">if</span> expand_dims:<br>        <span class="hljs-comment"># Make sure images have shape (1, 28, 28)</span><br>        x = torch.unsqueeze(x, <span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword">return</span> TensorDataset(x, y)<br></code></pre></td></tr></table></figure><p>现在是使用 <code>wandb.Artifact</code> 日志记录这个<code>preprocess</code> 步骤的代码。</p><p>请注意，下面的示例都 <code>use</code>s 了一个新的<code>Artifact</code>，并将其 <code>log</code>s下来，这与上一步相同。<code>Artifact</code>s 既是 <code>Run</code>s的输入又是输出！</p><p>我们使用一个新的<code>job_type</code>，<code>preprocess-data</code>，来明确这是一个不同于之前的job。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_and_log</span>(<span class="hljs-params">steps</span>):<br><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;preprocess-data&quot;</span>) <span class="hljs-keyword">as</span> run:<br><br>        processed_data = wandb.Artifact(<br>            <span class="hljs-string">&quot;mnist-preprocess&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;dataset&quot;</span>,<br>            description=<span class="hljs-string">&quot;Preprocessed MNIST dataset&quot;</span>,<br>            metadata=steps)<br>         <br>        <span class="hljs-comment"># ✔️ declare which artifact we&#x27;ll be using</span><br>        raw_data_artifact = run.use_artifact(<span class="hljs-string">&#x27;mnist-raw:latest&#x27;</span>)<br><br>        <span class="hljs-comment"># 📥 if need be, download the artifact</span><br>        raw_dataset = raw_data_artifact.download()<br>        <br>        <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;training&quot;</span>, <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>]:<br>            raw_split = read(raw_dataset, split)<br>            processed_dataset = preprocess(raw_split, **steps)<br><br>            <span class="hljs-keyword">with</span> processed_data.new_file(split + <span class="hljs-string">&quot;.pt&quot;</span>, mode=<span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> file:<br>                x, y = processed_dataset.tensors<br>                torch.save((x, y), file)<br><br>        run.log_artifact(processed_data)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read</span>(<span class="hljs-params">data_dir, split</span>):<br>    filename = split + <span class="hljs-string">&quot;.pt&quot;</span><br>    x, y = torch.load(os.path.join(data_dir, filename))<br><br>    <span class="hljs-keyword">return</span> TensorDataset(x, y)<br></code></pre></td></tr></table></figure><p>这里要注意的一件事是预处理的 <code>steps</code> 作为<code>metadata</code> 与 <code>preprocessed_data</code> 一起保存。</p><p>如果您想让您的实验可重现，捕获大量元数据是个好主意！</p><p>此外，即使我们的数据集是"<code>large artifact</code>"，<code>download</code>步骤也可以在不到一秒的时间内完成。</p><p>展开下面的 markdown 单元格以了解详细信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">steps = &#123;<span class="hljs-string">&quot;normalize&quot;</span>: <span class="hljs-literal">True</span>,<br>         <span class="hljs-string">&quot;expand_dims&quot;</span>: <span class="hljs-literal">True</span>&#125;<br><br>preprocess_and_log(steps)<br></code></pre></td></tr></table></figure><h4 id="run.use_artifact">✔️ <code>run.use_artifact</code></h4><p>这些步骤比较简单。消费者只需要知道 <code>Artifact</code>的<code>name</code>，再加上 bit more。</p><p>“bit more” 是您想要的 <code>Artifact</code> 的特定版本的<code>alias</code>。</p><p>默认情况下，最后上传的版本被标记为<code>latest</code>。否则，您可以选择带有<code>v0</code>/<code>v1</code>等的旧版本，或者您可以提供自己的别名，例如 <code>best</code> 或<code>jit-script</code>。就像 <a href="https://hub.docker.com/">DockerHub</a> 标签一样，别名与名称用 <code>:</code> 分隔，所以我们想要的<code>Artifact</code> 是 <code>mnist-raw:latest</code>。</p><blockquote><p>👍规则：保持别名简短而甜美。当您想要满足某些属性的<code>Artifact</code> 时，请使用自定义 <code>alias</code>es，如<code>latest</code> 或 <code>best</code></p></blockquote><h4 id="artifact.download">📥 <code>artifact.download</code></h4><p>现在，您可能正在担心 <code>download</code>调用。如果我们再下载一份，内存的负担会不会加倍？</p><p>别担心，朋友。在我们实际下载任何东西之前，我们会检查本地是否有正确的版本。使用和版本控制<code>git</code> 和 <ahref="https://en.wikipedia.org/wiki/Torrent_file">torrenting</a>相同的技术：hashing。</p><p>随着 <code>Artifact</code>s 的创建和记录，工作目录中名为<code>artifacts</code> 的文件夹将开始填充子目录，每个<code>Artifact</code>一个。使用 <code>!tree artifacts</code>检查其内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">!tree artifacts<br></code></pre></td></tr></table></figure><h4 id="the-artifacts-page-on-wandb.ai">🌐 The Artifacts page on <ahref="https://wandb.ai/">wandb.ai</a></h4><p>现在我们已经记录并使用了一个 <code>Artifact</code>，让我们检查一下Run 页面上的 Artifacts 选项卡。</p><p>从<code>wandb</code> 输出导航到运行页面URL，然后从左侧边栏中选择“工件”选项卡（它是带有数据库图标的选项卡，看起来像三个冰球叠在一起）。</p><p>单击 "Input Artifacts" 表或 "Output Artifacts"表中的一行，然后查看选项卡（"Overview", "Metadata"）以查看记录的有关<code>Artifact</code> 的所有内容。</p><p>我们特别喜欢 "Graph View"。默认情况下，它显示一个图表，其中<code>Artifact</code>s 的 <code>type</code>s 和 <code>Run</code> 的<code>job_types</code> 是两种类型的节点，箭头代表消费和生产。</p><h3 id="log-a-model">3️⃣ Log a Model</h3><p>这足以了解 <code>Artifact</code>s 的 API如何工作，但让我们按照这个示例一直到管道的末尾，以便我们可以了解<code>Artifact</code>s 如何改进您的 ML 工作流程。</p><p>这里的第一个单元格在 PyTorch 中构建了一个 DNN<code>model</code>——一个非常简单的 ConvNet。</p><p>我们将从初始化 <code>model</code>开始，而不是训练它。这样，我们可以重复训练，同时保持其他一切不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> floor<br><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_layer_sizes=[<span class="hljs-number">32</span>, <span class="hljs-number">64</span>],</span><br><span class="hljs-params">                  kernel_sizes=[<span class="hljs-number">3</span>],</span><br><span class="hljs-params">                  activation=<span class="hljs-string">&quot;ReLU&quot;</span>,</span><br><span class="hljs-params">                  pool_sizes=[<span class="hljs-number">2</span>],</span><br><span class="hljs-params">                  dropout=<span class="hljs-number">0.5</span>,</span><br><span class="hljs-params">                  num_classes=num_classes,</span><br><span class="hljs-params">                  input_shape=input_shape</span>):<br>      <br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br><br>        self.layer1 = nn.Sequential(<br>              nn.Conv2d(in_channels=input_shape[<span class="hljs-number">0</span>], out_channels=hidden_layer_sizes[<span class="hljs-number">0</span>], kernel_size=kernel_sizes[<span class="hljs-number">0</span>]),<br>              <span class="hljs-built_in">getattr</span>(nn, activation)(),<br>              nn.MaxPool2d(kernel_size=pool_sizes[<span class="hljs-number">0</span>])<br>        )<br>        self.layer2 = nn.Sequential(<br>              nn.Conv2d(in_channels=hidden_layer_sizes[<span class="hljs-number">0</span>], out_channels=hidden_layer_sizes[-<span class="hljs-number">1</span>], kernel_size=kernel_sizes[-<span class="hljs-number">1</span>]),<br>              <span class="hljs-built_in">getattr</span>(nn, activation)(),<br>              nn.MaxPool2d(kernel_size=pool_sizes[-<span class="hljs-number">1</span>])<br>        )<br>        self.layer3 = nn.Sequential(<br>              nn.Flatten(),<br>              nn.Dropout(dropout)<br>        )<br><br>        fc_input_dims = floor((input_shape[<span class="hljs-number">1</span>] - kernel_sizes[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>) / pool_sizes[<span class="hljs-number">0</span>]) <span class="hljs-comment"># layer 1 output size</span><br>        fc_input_dims = floor((fc_input_dims - kernel_sizes[-<span class="hljs-number">1</span>] + <span class="hljs-number">1</span>) / pool_sizes[-<span class="hljs-number">1</span>]) <span class="hljs-comment"># layer 2 output size</span><br>        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-<span class="hljs-number">1</span>] <span class="hljs-comment"># layer 3 output size</span><br><br>        self.fc = nn.Linear(fc_input_dims, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.fc(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>在这里，我们使用 W&amp;B 来跟踪运行，因此使用<code>wandb.config</code> 对象来存储所有超参数。</p><p>该 <code>config</code> 对象的 <code>dict</code>ionary版本是一个非常有用的 <code>metadata</code>，所以一定要包含它！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model_and_log</span>(<span class="hljs-params">config</span>):<br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;initialize&quot;</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        config = wandb.config<br>        <br>        model = ConvNet(**config)<br><br>        model_artifact = wandb.Artifact(<br>            <span class="hljs-string">&quot;convnet&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;model&quot;</span>,<br>            description=<span class="hljs-string">&quot;Simple AlexNet style CNN&quot;</span>,<br>            metadata=<span class="hljs-built_in">dict</span>(config))<br><br>        torch.save(model.state_dict(), <span class="hljs-string">&quot;initialized_model.pth&quot;</span>)<br>        <span class="hljs-comment"># ➕ another way to add a file to an Artifact</span><br>        model_artifact.add_file(<span class="hljs-string">&quot;initialized_model.pth&quot;</span>)<br><br>        wandb.save(<span class="hljs-string">&quot;initialized_model.pth&quot;</span>)<br><br>        run.log_artifact(model_artifact)<br><br>model_config = &#123;<span class="hljs-string">&quot;hidden_layer_sizes&quot;</span>: [<span class="hljs-number">32</span>, <span class="hljs-number">64</span>],<br>                <span class="hljs-string">&quot;kernel_sizes&quot;</span>: [<span class="hljs-number">3</span>],<br>                <span class="hljs-string">&quot;activation&quot;</span>: <span class="hljs-string">&quot;ReLU&quot;</span>,<br>                <span class="hljs-string">&quot;pool_sizes&quot;</span>: [<span class="hljs-number">2</span>],<br>                <span class="hljs-string">&quot;dropout&quot;</span>: <span class="hljs-number">0.5</span>,<br>                <span class="hljs-string">&quot;num_classes&quot;</span>: <span class="hljs-number">10</span>&#125;<br><br>build_model_and_log(model_config)<br></code></pre></td></tr></table></figure><h4 id="artifact.add_file">➕ <code>artifact.add_file</code></h4><p>与在数据集日志记录示例中同时编写 <code>new_file</code> 并将其添加到<code>Artifact</code> 不同，我们还可以一步写入文件（此处为<code>torch.save</code>），然后在另一步中将它们 <code>add</code> 到<code>Artifact</code>。</p><blockquote><p>👍规则：尽可能使用 <code>new_file</code>，以防止重复。</p></blockquote><h3 id="use-a-logged-model-artifact">4️⃣ Use a Logged Model Artifact</h3><p>就像我们可以在 <code>dataset</code> 上调用 <code>use_artifact</code>一样，我们可以在我们的 <code>initialized_model</code>上调用它以在另一个运行中使用它。</p><p>这一次，让我们 <code>train</code> <code>model</code>。</p><p>有关更多详细信息，请查看我们关于 <ahref="http://wandb.me/pytorch-colab">instrumenting W&amp;B withPyTorch</a> 的 Colab。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model, train_loader, valid_loader, config</span>):<br>    optimizer = <span class="hljs-built_in">getattr</span>(torch.optim, config.optimizer)(model.parameters())<br>    model.train()<br>    example_ct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>        <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>            data, target = data.to(device), target.to(device)<br>            optimizer.zero_grad()<br>            output = model(data)<br>            loss = F.cross_entropy(output, target)<br>            loss.backward()<br>            optimizer.step()<br><br>            example_ct += <span class="hljs-built_in">len</span>(data)<br><br>            <span class="hljs-keyword">if</span> batch_idx % config.batch_log_interval == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0%&#125;)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                    epoch, batch_idx * <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(train_loader.dataset),<br>                    batch_idx / <span class="hljs-built_in">len</span>(train_loader), loss.item()))<br>                <br>                train_log(loss, example_ct, epoch)<br><br>        <span class="hljs-comment"># evaluate the model on the validation set at each epoch</span><br>        loss, accuracy = test(model, valid_loader)  <br>        test_log(loss, accuracy, example_ct, epoch)<br><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">model, test_loader</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    test_loss = <span class="hljs-number">0</span><br>    correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            test_loss += F.cross_entropy(output, target, reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)  <span class="hljs-comment"># sum up batch loss</span><br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># get the index of the max log-probability</span><br>            correct += pred.eq(target.view_as(pred)).<span class="hljs-built_in">sum</span>()<br><br>    test_loss /= <span class="hljs-built_in">len</span>(test_loader.dataset)<br><br>    accuracy = <span class="hljs-number">100.</span> * correct / <span class="hljs-built_in">len</span>(test_loader.dataset)<br>    <br>    <span class="hljs-keyword">return</span> test_loss, accuracy<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_log</span>(<span class="hljs-params">loss, example_ct, epoch</span>):<br>    loss = <span class="hljs-built_in">float</span>(loss)<br><br>    <span class="hljs-comment"># where the magic happens</span><br>    wandb.log(&#123;<span class="hljs-string">&quot;epoch&quot;</span>: epoch, <span class="hljs-string">&quot;train/loss&quot;</span>: loss&#125;, step=example_ct)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Loss after &quot;</span> + <span class="hljs-built_in">str</span>(example_ct).zfill(<span class="hljs-number">5</span>) + <span class="hljs-string">f&quot; examples: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br>    <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_log</span>(<span class="hljs-params">loss, accuracy, example_ct, epoch</span>):<br>    loss = <span class="hljs-built_in">float</span>(loss)<br>    accuracy = <span class="hljs-built_in">float</span>(accuracy)<br><br>    <span class="hljs-comment"># where the magic happens</span><br>    wandb.log(&#123;<span class="hljs-string">&quot;epoch&quot;</span>: epoch, <span class="hljs-string">&quot;validation/loss&quot;</span>: loss, <span class="hljs-string">&quot;validation/accuracy&quot;</span>: accuracy&#125;, step=example_ct)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Loss/accuracy after &quot;</span> + <span class="hljs-built_in">str</span>(example_ct).zfill(<span class="hljs-number">5</span>) + <span class="hljs-string">f&quot; examples: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.3</span>f&#125;</span>/<span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>这次我们将运行两个独立的 <code>Artifact</code> 生产<code>Run</code>s。</p><p>一旦第一个完成 <code>train</code>ing<code>model</code>，第二个将通过评估其在 <code>test_dataset</code>上的性能来使用 <code>trained-model</code> <code>Artifact</code>。</p><p>此外，我们将提取网络最混乱的 32个示例——在这些示例中，<code>categorical_crossentropy</code> 最高。</p><p>这是诊断数据集和模型问题的好方法！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">model, test_loader</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    ## Evaluate the trained model</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    loss, accuracy = test(model, test_loader)<br>    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)<br><br>    <span class="hljs-keyword">return</span> loss, accuracy, highest_losses, hardest_examples, true_labels, predictions<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_hardest_k_examples</span>(<span class="hljs-params">model, testing_set, k=<span class="hljs-number">32</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br><br>    loader = DataLoader(testing_set, <span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># get the losses and predictions for each item in the dataset</span><br>    losses = <span class="hljs-literal">None</span><br>    predictions = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            loss = F.cross_entropy(output, target)<br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            <br>            <span class="hljs-keyword">if</span> losses <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                losses = loss.view((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>                predictions = pred<br>            <span class="hljs-keyword">else</span>:<br>                losses = torch.cat((losses, loss.view((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))), <span class="hljs-number">0</span>)<br>                predictions = torch.cat((predictions, pred), <span class="hljs-number">0</span>)<br><br>    argsort_loss = torch.argsort(losses, dim=<span class="hljs-number">0</span>)<br><br>    highest_k_losses = losses[argsort_loss[-k:]]<br>    hardest_k_examples = testing_set[argsort_loss[-k:]][<span class="hljs-number">0</span>]<br>    true_labels = testing_set[argsort_loss[-k:]][<span class="hljs-number">1</span>]<br>    predicted_labels = predictions[argsort_loss[-k:]]<br><br>    <span class="hljs-keyword">return</span> highest_k_losses, hardest_k_examples, true_labels, predicted_labels<br></code></pre></td></tr></table></figure><p>这些日志记录功能不会添加任何新的 <code>Artifact</code>功能，因此我们不会对其进行评论：我们只是在使用、下载和记录<code>Artifact</code>s。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_and_log</span>(<span class="hljs-params">config</span>):<br><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;train&quot;</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        config = wandb.config<br><br>        data = run.use_artifact(<span class="hljs-string">&#x27;mnist-preprocess:latest&#x27;</span>)<br>        data_dir = data.download()<br><br>        training_dataset =  read(data_dir, <span class="hljs-string">&quot;training&quot;</span>)<br>        validation_dataset = read(data_dir, <span class="hljs-string">&quot;validation&quot;</span>)<br><br>        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)<br>        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)<br>        <br>        model_artifact = run.use_artifact(<span class="hljs-string">&quot;convnet:latest&quot;</span>)<br>        model_dir = model_artifact.download()<br>        model_path = os.path.join(model_dir, <span class="hljs-string">&quot;initialized_model.pth&quot;</span>)<br>        model_config = model_artifact.metadata<br>        config.update(model_config)<br><br>        model = ConvNet(**model_config)<br>        model.load_state_dict(torch.load(model_path))<br>        model = model.to(device)<br> <br>        train(model, train_loader, validation_loader, config)<br><br>        model_artifact = wandb.Artifact(<br>            <span class="hljs-string">&quot;trained-model&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;model&quot;</span>,<br>            description=<span class="hljs-string">&quot;Trained NN model&quot;</span>,<br>            metadata=<span class="hljs-built_in">dict</span>(model_config))<br><br>        torch.save(model.state_dict(), <span class="hljs-string">&quot;trained_model.pth&quot;</span>)<br>        model_artifact.add_file(<span class="hljs-string">&quot;trained_model.pth&quot;</span>)<br>        wandb.save(<span class="hljs-string">&quot;trained_model.pth&quot;</span>)<br><br>        run.log_artifact(model_artifact)<br><br>    <span class="hljs-keyword">return</span> model<br><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_and_log</span>(<span class="hljs-params">config=<span class="hljs-literal">None</span></span>):<br>    <br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;report&quot;</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        data = run.use_artifact(<span class="hljs-string">&#x27;mnist-preprocess:latest&#x27;</span>)<br>        data_dir = data.download()<br>        testing_set = read(data_dir, <span class="hljs-string">&quot;test&quot;</span>)<br><br>        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>        model_artifact = run.use_artifact(<span class="hljs-string">&quot;trained-model:latest&quot;</span>)<br>        model_dir = model_artifact.download()<br>        model_path = os.path.join(model_dir, <span class="hljs-string">&quot;trained_model.pth&quot;</span>)<br>        model_config = model_artifact.metadata<br><br>        model = ConvNet(**model_config)<br>        model.load_state_dict(torch.load(model_path))<br>        model.to(device)<br><br>        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)<br><br>        run.summary.update(&#123;<span class="hljs-string">&quot;loss&quot;</span>: loss, <span class="hljs-string">&quot;accuracy&quot;</span>: accuracy&#125;)<br><br>        wandb.log(&#123;<span class="hljs-string">&quot;high-loss-examples&quot;</span>:<br>            [wandb.Image(hard_example, caption=<span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(pred)) + <span class="hljs-string">&quot;,&quot;</span> +  <span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(label)))<br>             <span class="hljs-keyword">for</span> hard_example, pred, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(hardest_examples, preds, true_labels)]&#125;)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">train_config = &#123;<span class="hljs-string">&quot;batch_size&quot;</span>: <span class="hljs-number">128</span>,<br>                <span class="hljs-string">&quot;epochs&quot;</span>: <span class="hljs-number">5</span>,<br>                <span class="hljs-string">&quot;batch_log_interval&quot;</span>: <span class="hljs-number">25</span>,<br>                <span class="hljs-string">&quot;optimizer&quot;</span>: <span class="hljs-string">&quot;Adam&quot;</span>&#125;<br><br>model = train_and_log(train_config)<br>evaluate_and_log()<br></code></pre></td></tr></table></figure><h4 id="the-graph-view">🔁 The Graph View</h4><p>请注意，我们更改了 <code>Artifact</code> 的 <code>type</code>：这些<code>Run</code>s 使用的是模型，而不是数据集。在 Artifacts页面的图形视图中，生产模型的 <code>Run</code>s 将与生成<code>dataset</code>s 的运行分开。</p><p>去看看吧！和以前一样，您需要前往 Run 页面，从左侧栏中选择 "Artifacts"选项卡，选择一个 <code>Artifact</code>，然后单击 "Graph View"选项卡。</p><h4 id="exploded-graphs">💣 Exploded Graphs</h4><p>您可能已经注意到标有“爆炸”的按钮。不要点击它，因为它会在 W&amp;B总部您不起眼的作者办公桌下引爆一枚小炸弹！</p><p>只是在开玩笑。它以更温和的方式“分解”图表：<code>Artifact</code>s 和<code>Run</code>s 在单个实例级别而不是类型级别分离：节点不是<code>dataset</code> 和 <code>load-data</code>，而是<code>dataset:mnist-raw:v1</code> 和<code>load-data:sunny-smoke-1</code>，等等。</p><p>这提供了对您的管道的全面洞察，记录的指标、元数据等都触手可及——您仅受限于您选择与我们一起记录的内容。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>WandB</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（四）Typora + PicGo+ 腾讯云图床</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-san-typora-teng-xun-yun-tu-chuang/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-san-typora-teng-xun-yun-tu-chuang/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（五）自定义域名及百度谷歌收录</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-wu-zi-ding-yi-yu-ming-ji-bai-du-gu-ge-shou-lu/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-wu-zi-ding-yi-yu-ming-ji-bai-du-gu-ge-shou-lu/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（二）Matery 主题</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-er-matery-zhu-ti/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-er-matery-zhu-ti/</url>
    
    <content type="html"><![CDATA[<h2 id="下载">下载</h2><p>本主题<strong>推荐你使用 Hexo 5.0.0及以上的版本</strong>。如果，你已经有一个自己的 <ahref="https://hexo.io/zh-cn/">Hexo</a> 博客了，建议你将 Hexo升级到最新稳定的版本。</p><p>点击 <ahref="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master">这里</a>下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将<code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的<code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用<code>git clone</code> 命令来下载:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/blinkfox/hexo-theme-matery.git<br></code></pre></td></tr></table></figure><h2 id="配置">配置</h2><h3 id="切换主题">切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的 <code>theme</code>的值：<code>theme: hexo-theme-matery</code></p><h4 id="config.yml-文件的其它修改建议"><code>_config.yml</code>文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主<code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code>的倍数，如：<code>12</code>、<code>18</code>等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为<code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页">新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客<code>source</code> 目录下还没有 <code>categories/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;categories&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/categories/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">categories</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;categories&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;categories&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="新建标签-tags-页">新建标签 tags 页</h3><p><code>tags</code> 页是用来展示所有标签的页面，如果在你的博客<code>source</code> 目录下还没有 <code>tags/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;tags&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/tags/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">tags</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 18:23:38</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;tags&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;tags&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="新建关于我-about-页">新建关于我 about 页</h3><p><code>about</code>页是用来展示<strong>关于我和我的博客</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>about/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;about&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/about/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">about</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;about&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;about&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="新建留言板-contact-页可选的">新建留言板 contact页（可选的）</h3><p><code>contact</code>页是用来展示<strong>留言板</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>contact/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;contact&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/contact/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">contact</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;contact&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;contact&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><blockquote><p><strong>注</strong>：本留言板功能依赖于第三方评论系统，请<strong>激活</strong>你的评论系统才有效果。并且在主题的<code>_config.yml</code> 文件中，第 <code>19</code> 至 <code>21</code>行的“<strong>菜单</strong>”配置，取消关于留言板的注释即可。</p></blockquote><h3 id="新建友情链接-friends-页可选的">新建友情链接 friends页（可选的）</h3><p><code>friends</code>页是用来展示<strong>友情链接</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>friends/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;friends&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/friends/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">friends</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-12-12 21:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;friends&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;friends&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><p>同时，在你的博客 <code>source</code> 目录下新建 <code>_data</code>目录，在 <code>_data</code> 目录中新建 <code>friends.json</code>文件，文件内容如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;avatar&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;http://image.luokangyuan.com/1_qq_27922023.jpg&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;码酱&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;introduction&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我不是大佬，只是在追寻大佬的脚步&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;http://luokangyuan.com/&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;前去学习&quot;</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;avatar&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;http://image.luokangyuan.com/4027734.jpeg&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;闪烁之狐&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;introduction&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;https://blinkfox.github.io/&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;前去学习&quot;</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;avatar&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;http://image.luokangyuan.com/avatar.jpg&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ja_rome&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;introduction&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;平凡的脚步也可以走出伟大的行程&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;https://me.csdn.net/jlh912008548&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;前去学习&quot;</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><h3 id="新建-404-页">新建 404 页</h3><p>如果在你的博客 <code>source</code> 目录下还没有 <code>404.md</code>文件，那么你就需要新建一个。编辑你刚刚新建的页面文件<code>/source/404.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-number">404</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;404&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;404&quot;</span><br><span class="hljs-attr">description:</span> <span class="hljs-string">&quot;Oops～，我崩溃了！找不到你想要的页面 :(&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="菜单导航配置">菜单导航配置</h3><h4id="配置基本菜单导航的名称路径url和图标icon.">配置基本菜单导航的名称、路径url和图标icon.</h4><p>1.菜单导航名称可以是中文也可以是英文(如：<code>Index</code>或<code>主页</code>)2.图标icon 可以在<a href="https://fontawesome.com/icons">FontAwesome</a> 中查找</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-attr">Index:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-home</span><br>  <span class="hljs-attr">Tags:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/tags</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-tags</span><br>  <span class="hljs-attr">Categories:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/categories</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-bookmark</span><br>  <span class="hljs-attr">Archives:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/archives</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-archive</span><br>  <span class="hljs-attr">About:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/about</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-user-circle</span><br>  <span class="hljs-attr">Friends:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/friends</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-address-book</span><br></code></pre></td></tr></table></figure><h4 id="二级菜单配置方法">二级菜单配置方法</h4><p>如果你需要二级菜单则可以在原基本菜单导航的基础上如下操作</p><ol type="1"><li>在需要添加二级菜单的一级菜单下添加<code>children</code>关键字(如:<code>About</code>菜单下添加<code>children</code>)<br /></li><li>在<code>children</code>下创建二级菜单的名称name,路径url和图标icon.<br /></li><li>注意每个二级菜单模块前要加 <code>-</code>.<br /></li><li>注意缩进格式</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-attr">Index:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-home</span><br>  <span class="hljs-attr">Tags:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/tags</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-tags</span><br>  <span class="hljs-attr">Categories:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/categories</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-bookmark</span><br>  <span class="hljs-attr">Archives:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/archives</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-archive</span><br>  <span class="hljs-attr">About:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/about</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-user-circle-o</span><br>  <span class="hljs-attr">Friends:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/friends</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-address-book</span><br>  <span class="hljs-attr">Medias:</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-list</span><br>    <span class="hljs-attr">children:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Music</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/music</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-music</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Movies</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/movies</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-film</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Books</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/books</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-book</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Galleries</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/galleries</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-image</span><br></code></pre></td></tr></table></figure><h3 id="代码高亮">代码高亮</h3><p>从 Hexo5.0 版本开始自带了 <code>prismjs</code>代码语法高亮的支持，本主题对此进行了改造支持。</p><p>如果你的博客中曾经安装过 <code>hexo-prism-plugin</code>的插件，那么你须要执行 <code>npm uninstall hexo-prism-plugin</code>来卸载掉它，否则生成的代码中会有 <code>&amp;#123;</code> 和<code>&amp;#125;</code> 的转义字符。</p><p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中<code>highlight.enable</code> 的值为 <code>false</code>，并将<code>prismjs.enable</code> 的值设置为<code>true</code>，主要配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">highlight:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">line_number:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">auto_detect:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">tab_replace:</span> <span class="hljs-string">&#x27;&#x27;</span><br>  <span class="hljs-attr">wrap:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">hljs:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">prismjs:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">preprocess:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">line_number:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">tab_replace:</span> <span class="hljs-string">&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>主题中默认的 <code>prismjs</code> 主题是<code>Tomorrow Night</code>，如果你想定制自己的主题，可以前往 <ahref="https://prismjs.com/download.html">prismjs 下载页面</a>定制下载自己喜欢的主题 <code>css</code> 文件，然后将此 css主题文件取名为 <code>prism.css</code>，替换掉<code>hexo-theme-matery</code> 主题文件夹中的<code>source/libs/prism/prism.css</code> 文件即可。</p><h3 id="搜索">搜索</h3><p>本主题中还使用到了 <ahref="https://github.com/wzpan/hexo-generator-search">hexo-generator-search</a>的 Hexo 插件来做内容搜索，安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-generator-search --save<br></code></pre></td></tr></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">search:</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">search.xml</span><br>  <span class="hljs-attr">field:</span> <span class="hljs-string">post</span><br></code></pre></td></tr></table></figure><h3 id="中文链接转拼音建议安装">中文链接转拼音（建议安装）</h3><p>如果你的文章名称是中文的，那么 Hexo默认生成的永久链接也会有中文，这样不利于 <code>SEO</code>，且<code>gitment</code> 评论对中文链接也不支持。我们可以用 <ahref="https://github.com/viko16/hexo-permalink-pinyin">hexo-permalink-pinyin</a>Hexo 插件使在生成文章时生成中文拼音的永久链接。</p><p>安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm i hexo-permalink-pinyin --save<br></code></pre></td></tr></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">permalink_pinyin:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">separator:</span> <span class="hljs-string">&#x27;-&#x27;</span> <span class="hljs-comment"># default: &#x27;-&#x27;</span><br></code></pre></td></tr></table></figure><blockquote><p><strong>注</strong>：除了此插件外，<ahref="https://github.com/rozbo/hexo-abbrlink">hexo-abbrlink</a>插件也可以生成非中文的链接。</p></blockquote><h3 id="文章字数统计插件建议安装">文章字数统计插件（建议安装）</h3><p>如果你想要在文章中显示文章字数、阅读时长信息，可以安装 <ahref="https://github.com/willin/hexo-wordcount">hexo-wordcount</a>插件。</p><p>安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm i --save hexo-wordcount<br></code></pre></td></tr></table></figure><p>然后只需在本主题下的 <code>_config.yml</code>文件中，将各个文章字数相关的配置激活即可：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">postInfo:</span><br>  <span class="hljs-attr">date:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">update:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">wordCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 设置文章字数统计为 true.</span><br>  <span class="hljs-attr">totalCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 设置站点文章总字数统计为 true.</span><br>  <span class="hljs-attr">min2read:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 阅读时长.</span><br>  <span class="hljs-attr">readCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 阅读次数.</span><br></code></pre></td></tr></table></figure><h3 id="添加emoji表情支持可选的">添加emoji表情支持（可选的）</h3><p>本主题新增了对<code>emoji</code>表情的支持，使用到了 <ahref="https://npm.taobao.org/package/hexo-filter-github-emojis">hexo-filter-github-emojis</a>的 Hexo 插件来支持<code>emoji</code>表情的生成，把对应的<code>markdown emoji</code>语法（<code>::</code>,例如：<code>:smile:</code>）转变成会跳跃的<code>emoji</code>表情，安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-filter-github-emojis --save<br></code></pre></td></tr></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">githubEmojis:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">className:</span> <span class="hljs-string">github-emoji</span><br>  <span class="hljs-attr">inject:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">styles:</span><br>  <span class="hljs-attr">customEmojis:</span><br></code></pre></td></tr></table></figure><p>执行 <code>hexo clean &amp;&amp; hexo g</code>重新生成博客文件，然后就可以在文章中对应位置看到你用<code>emoji</code>语法写的表情了。</p><h3 id="添加-rss-订阅支持可选的">添加 RSS 订阅支持（可选的）</h3><p>本主题中还使用到了 <ahref="https://github.com/hexojs/hexo-generator-feed">hexo-generator-feed</a>的 Hexo 插件来做 <code>RSS</code>，安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-generator-feed --save<br></code></pre></td></tr></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">feed:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">atom</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">atom.xml</span><br>  <span class="hljs-attr">limit:</span> <span class="hljs-number">20</span><br>  <span class="hljs-attr">hub:</span><br>  <span class="hljs-attr">content:</span><br>  <span class="hljs-attr">content_limit:</span> <span class="hljs-number">140</span><br>  <span class="hljs-attr">content_limit_delim:</span> <span class="hljs-string">&#x27; &#x27;</span><br>  <span class="hljs-attr">order_by:</span> <span class="hljs-string">-date</span><br></code></pre></td></tr></table></figure><p>执行 <code>hexo clean &amp;&amp; hexo g</code>重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到<code>atom.xml</code> 文件，说明你已经安装成功了。</p><h3 id="添加-daovoice-在线聊天功能可选的">添加 <ahref="http://www.daovoice.io/">DaoVoice</a> 在线聊天功能（可选的）</h3><p>前往 <a href="http://www.daovoice.io/">DaoVoice</a> 官网注册并且获取<code>app_id</code>，并将 <code>app_id</code> 填入主题的<code>_config.yml</code> 文件中。</p><h3 id="添加-tidio-在线聊天功能可选的">添加 <ahref="https://www.tidio.com/">Tidio</a> 在线聊天功能（可选的）</h3><p>前往 <a href="https://www.tidio.com/">Tidio</a> 官网注册并且获取<code>Public Key</code>，并将 <code>Public Key</code> 填入主题的<code>_config.yml</code> 文件中。</p><h3 id="修改页脚">修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的<code>/layout/_partial/footer.ejs</code>文件中，包括站点、使用的主题、访问量等。</p><h3 id="添加中文繁简转换">添加中文繁简转换</h3><p>在主题的 <code>_config.yml</code> 文件中，开启 translate 为enable。</p><blockquote><p>开启中文繁简转换如下修改。默认不开启。 实例演示： <ahref="https://blog.17lai.site">繁简转换</a> 底下 footer 栏</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">translate:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><h3 id="修改社交链接">修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持<code>QQ</code>、<code>GitHub</code> 和邮箱等的配置，你可以在主题文件的<code>/layout/_partial/social-link.ejs</code>文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs html">&lt;% if (theme.socialLink.github) &#123; %&gt;<br>    <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;&lt;%= theme.socialLink.github %&gt;&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;tooltipped&quot;</span> <span class="hljs-attr">target</span>=<span class="hljs-string">&quot;_blank&quot;</span> <span class="hljs-attr">data-tooltip</span>=<span class="hljs-string">&quot;访问我的GitHub&quot;</span> <span class="hljs-attr">data-position</span>=<span class="hljs-string">&quot;top&quot;</span> <span class="hljs-attr">data-delay</span>=<span class="hljs-string">&quot;50&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">i</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;fab fa-github&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">i</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>&lt;% &#125; %&gt;<br></code></pre></td></tr></table></figure><p>其中，社交图标（如：<code>fa-github</code>）你可以在 <ahref="https://fontawesome.com/icons">Font Awesome</a>中搜索找到。以下是常用社交图标的标识，供你参考：</p><ul><li>Facebook: <code>fab fa-facebook</code></li><li>Twitter: <code>fab fa-twitter</code></li><li>Google-plus: <code>fab fa-google-plus</code></li><li>Linkedin: <code>fab fa-linkedin</code></li><li>Tumblr: <code>fab fa-tumblr</code></li><li>Medium: <code>fab fa-medium</code></li><li>Slack: <code>fab fa-slack</code></li><li>Sina Weibo: <code>fab fa-weibo</code></li><li>Wechat: <code>fab fa-weixin</code></li><li>QQ: <code>fab fa-qq</code></li><li>Zhihu: <code>fab fa-zhihu</code></li></ul><blockquote><p><strong>注意</strong>: 本主题中使用的 <code>Font Awesome</code>版本为 <code>5.11.0</code>。</p></blockquote><h3 id="修改打赏的二维码图片">修改打赏的二维码图片</h3><p>在主题文件的 <code>source/medias/reward</code>文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p><h3 id="配置音乐播放器可选的">配置音乐播放器（可选的）</h3><p>要支持音乐播放，在主题的 <code>_config.yml</code>配置文件中激活music配置即可：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 是否在首页显示音乐</span><br><span class="hljs-attr">music:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">title:</span>         <span class="hljs-comment"># 非吸底模式有效</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">show:</span> <span class="hljs-string">听听音乐</span><br>  <span class="hljs-attr">server:</span> <span class="hljs-string">netease</span>   <span class="hljs-comment"># require music platform: netease, tencent, kugou, xiami, baidu</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">playlist</span>    <span class="hljs-comment"># require song, playlist, album, search, artist</span><br>  <span class="hljs-attr">id:</span> <span class="hljs-number">503838841</span>     <span class="hljs-comment"># require song id / playlist id / album id / search keyword</span><br>  <span class="hljs-attr">fixed:</span> <span class="hljs-literal">false</span>      <span class="hljs-comment"># 开启吸底模式</span><br>  <span class="hljs-attr">autoplay:</span> <span class="hljs-literal">false</span>   <span class="hljs-comment"># 是否自动播放</span><br>  <span class="hljs-attr">theme:</span> <span class="hljs-string">&#x27;#42b983&#x27;</span><br>  <span class="hljs-attr">loop:</span> <span class="hljs-string">&#x27;all&#x27;</span>       <span class="hljs-comment"># 音频循环播放, 可选值: &#x27;all&#x27;, &#x27;one&#x27;, &#x27;none&#x27;</span><br>  <span class="hljs-attr">order:</span> <span class="hljs-string">&#x27;random&#x27;</span>   <span class="hljs-comment"># 音频循环顺序, 可选值: &#x27;list&#x27;, &#x27;random&#x27;</span><br>  <span class="hljs-attr">preload:</span> <span class="hljs-string">&#x27;auto&#x27;</span>   <span class="hljs-comment"># 预加载，可选值: &#x27;none&#x27;, &#x27;metadata&#x27;, &#x27;auto&#x27;</span><br>  <span class="hljs-attr">volume:</span> <span class="hljs-number">0.7</span>       <span class="hljs-comment"># 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效</span><br>  <span class="hljs-attr">listFolded:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># 列表默认折叠</span><br></code></pre></td></tr></table></figure><blockquote><p><code>server</code>可选<code>netease</code>（网易云音乐），<code>tencent</code>（QQ音乐），<code>kugou</code>（酷狗音乐），<code>xiami</code>（虾米音乐），</p><p><code>baidu</code>（百度音乐）。</p><p><code>type</code>可选<code>song</code>（歌曲），<code>playlist</code>（歌单），<code>album</code>（专辑），<code>search</code>（搜索关键字），<code>artist</code>（歌手）</p><p><code>id</code>获取方法示例:浏览器打开网易云音乐，点击我喜欢的音乐歌单，浏览器地址栏后面会有一串数字，<code>playlist</code>的<code>id</code></p><p>即为这串数字。</p></blockquote><h3 id="添加note">添加note</h3><blockquote><p><ahref="https://blog.17lai.site/posts/cf0f47fd/#tag-note">演示</a></p></blockquote><h4 id="usage">Usage</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% note [class] [no-icon] [summary] %&#125;<br>Any content (support inline tags too).<br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><ul><li><code>[class]</code> : <em>Optional parameter.</em> Supportedvalues: default | primary | success | info | warning | danger.</li><li><code>[no-icon]</code> : <em>Optional parameter.</em> Disable iconin note.</li><li><code>[summary]</code> : <em>Optional parameter.</em> Optionalsummary of the note.</li></ul><p>All parameters are optional.</p><h4 id="example">example</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% note %&#125;<br>#### Header<br>(without define class style)<br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><h3 id="添加button">添加button</h3><blockquote><p><ahref="https://blog.17lai.site/posts/cf0f47fd/#tag-button">演示</a></p></blockquote><h4 id="usage-1">Usage</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% button url, text, icon [class], [title] %&#125;<br></code></pre></td></tr></table></figure><p>or</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% btn url, text, icon [class], [title] %&#125;<br></code></pre></td></tr></table></figure><ul><li><code>url</code> : Absolute or relative path to URL.</li><li><code>text</code> : Button text. Required if no icon specified.</li><li><code>icon</code> : Font Awesome icon name. Required if no textspecified.</li><li><code>[class]</code> : <em>Optional parameter.</em> Font Awesomeclass(es): <code>fa-fw</code> | <code>fa-lg</code> | <code>fa-2x</code>| <code>fa-3x</code> | <code>fa-4x</code> | <code>fa-5x</code></li><li><code>[title]</code> : <em>Optional parameter.</em> Tooltip atmouseover.</li></ul><h4 id="examples">Examples</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% button #, Text %&#125;<br></code></pre></td></tr></table></figure><h2 id="文章-front-matter-介绍">文章 Front-matter 介绍</h2><h3 id="front-matter-选项详解">Front-matter 选项详解</h3><p><code>Front-matter</code>选项中的所有内容均为<strong>非必填</strong>的。但我仍然建议至少填写<code>title</code> 和 <code>date</code> 的值。</p><table><thead><tr class="header"><th>配置选项</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>title</td><td><code>Markdown</code> 的文件标题</td><td>文章标题，强烈建议填写此选项</td></tr><tr class="even"><td>date</td><td>文件创建时的日期时间</td><td>发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr class="odd"><td>author</td><td>根 <code>_config.yml</code> 中的 <code>author</code></td><td>文章作者</td></tr><tr class="even"><td>img</td><td><code>featureImages</code> 中的某个值</td><td>文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如:<code>http://xxx.com/xxx.jpg</code></td></tr><tr class="odd"><td>top</td><td><code>true</code></td><td>推荐文章（文章是否置顶），如果 <code>top</code> 值为<code>true</code>，则会作为首页推荐文章</td></tr><tr class="even"><td>hide</td><td><code>false</code></td><td>隐藏文章，如果<code>hide</code>值为<code>true</code>，则文章不会在首页显示</td></tr><tr class="odd"><td>cover</td><td><code>false</code></td><td><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr class="even"><td>coverImg</td><td>无</td><td><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr class="odd"><td>password</td><td>无</td><td>文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置<code>password</code> 的值，该值必须是用 <code>SHA256</code>加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code>中激活了 <code>verifyPassword</code> 选项</td></tr><tr class="even"><td>toc</td><td><code>true</code></td><td>是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的<code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr class="odd"><td>mathjax</td><td><code>false</code></td><td>是否开启数学公式支持 ，本文章是否开启<code>mathjax</code>，且需要在主题的 <code>_config.yml</code>文件中也需要开启才行</td></tr><tr class="even"><td>summary</td><td>无</td><td>文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr class="odd"><td>categories</td><td>无</td><td>文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr class="even"><td>tags</td><td>无</td><td>文章标签，一篇文章可以多个标签</td></tr><tr class="odd"><td>keywords</td><td>文章标题</td><td>文章关键字，SEO 时需要</td></tr><tr class="even"><td>reprintPolicy</td><td>cc_by</td><td>文章转载规则， 可以是 cc_by, cc_by_nd, cc_by_sa, cc_by_nc,cc_by_nc_nd, cc_by_nc_sa, cc0, noreprint 或 pay 中的一个</td></tr></tbody></table><blockquote><p><strong>注意</strong>: 1. 如果 <code>img</code>属性不填写的话，文章特色图会根据文章标题的 <code>hashcode</code>的值取余，然后选取主题中对应的特色图片，从而达到让所有文章的特色图<strong>各有特色</strong>。2. <code>date</code> 的值尽量保证每篇文章是唯一的，因为本主题中<code>Gitalk</code> 和 <code>Gitment</code> 识别 <code>id</code> 是通过<code>date</code> 的值来作为唯一标识的。 3.如果要对文章设置阅读验证密码的功能，不仅要在 Front-matter 中设置采用了SHA256 加密的 password 的值，还需要在主题的 <code>_config.yml</code>中激活了配置。有些在线的 SHA256 加密的地址，可供你使用：<ahref="http://tool.oschina.net/encrypt?type=2">开源中国在线工具</a>、<ahref="http://encode.chahuo.com/">chahuo</a>、<ahref="http://tool.chinaz.com/tools/hash.aspx">站长工具</a>。 4.您可以在文章md文件的 front-matter 中指定 reprintPolicy来给单个文章配置转载规则</p></blockquote><p>以下为文章的 <code>Front-matter</code> 示例。</p><h3 id="最简示例">最简示例</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">typora-vue-theme主题介绍</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-07 09:25:00</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="最全示例">最全示例</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">typora-vue-theme主题介绍</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-07 09:25:00</span><br><span class="hljs-attr">author:</span> <span class="hljs-string">赵奇</span><br><span class="hljs-attr">img:</span> <span class="hljs-string">/source/images/xxx.jpg</span><br><span class="hljs-attr">top:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">hide:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">cover:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">coverImg:</span> <span class="hljs-string">/images/1.jpg</span><br><span class="hljs-attr">password:</span> <span class="hljs-string">8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92</span><br><span class="hljs-attr">toc:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">mathjax:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">summary:</span> <span class="hljs-string">这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</span><br><span class="hljs-attr">categories:</span> <span class="hljs-string">Markdown</span><br><span class="hljs-attr">tags:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Typora</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Markdown</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h2 id="自定制修改">自定制修改</h2><p>在本主题的 <code>_config.yml</code>中可以修改部分自定义信息，有以下几个部分：</p><ul><li>菜单</li><li>我的梦想</li><li>首页的音乐播放器和视频播放器配置</li><li>是否显示推荐文章名称和按钮配置</li><li><code>favicon</code> 和 <code>Logo</code></li><li>个人信息</li><li>TOC 目录</li><li>文章打赏信息</li><li>复制文章内容时追加版权信息</li><li>MathJax</li><li>文章字数统计、阅读时长</li><li>点击页面的'爱心'效果</li><li>我的项目</li><li>我的技能</li><li>我的相册</li><li><code>Gitalk</code>、<code>Gitment</code>、<code>Valine</code> 和<code>disqus</code> 评论配置</li><li><ahref="http://busuanzi.ibruce.info/">不蒜子统计</a>和谷歌分析（<code>Google Analytics</code>）</li><li>默认特色图的集合。当文章没有设置特色图时，本主题会根据文章标题的<code>hashcode</code> 值取余，来选择展示对应的特色图</li></ul><p><strong>我认为个人博客应该都有自己的风格和特色</strong>。如果本主题中的诸多功能和主题色彩你不满意，可以在主题中自定义修改，很多更自由的功能和细节点的修改难以在主题的<code>_config.yml</code>中完成，需要修改源代码才来完成。以下列出了可能对你有用的地方：</p><h3 id="修改主题颜色">修改主题颜色</h3><p>在主题文件的 <code>/source/css/matery.css</code> 文件中，搜索<code>.bg-color</code> 来修改背景颜色：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-comment">/* 整体背景颜色，包括导航、移动端的导航、页尾、标签页等的背景颜色. */</span><br><span class="hljs-selector-class">.bg-color</span> &#123;<br>    <span class="hljs-attribute">background-image</span>: <span class="hljs-built_in">linear-gradient</span>(to right, <span class="hljs-number">#4cbf30</span> <span class="hljs-number">0%</span>, <span class="hljs-number">#0f9d58</span> <span class="hljs-number">100%</span>);<br>&#125;<br><br><span class="hljs-keyword">@-webkit-keyframes</span> rainbow &#123;<br>   <span class="hljs-comment">/* 动态切换背景颜色. */</span><br>&#125;<br><br><span class="hljs-keyword">@keyframes</span> rainbow &#123;<br>    <span class="hljs-comment">/* 动态切换背景颜色. */</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="修改-banner-图和文章特色图">修改 banner 图和文章特色图</h3><p>你可以直接在 <code>/source/medias/banner</code> 文件夹中更换你喜欢的<code>banner</code> 图片，主题代码中是每天动态切换一张，只需<code>7</code> 张即可。如果你会 <code>JavaScript</code>代码，可以修改成你自己喜欢切换逻辑，如：随机切换等，<code>banner</code>切换的代码位置在 <code>/layout/_partial/bg-cover-content.ejs</code>文件的 <code>&lt;script&gt;&lt;/script&gt;</code> 代码中：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs javascript">$(<span class="hljs-string">&#x27;.bg-cover&#x27;</span>).<span class="hljs-title function_">css</span>(<span class="hljs-string">&#x27;background-image&#x27;</span>, <span class="hljs-string">&#x27;url(/medias/banner/&#x27;</span> + <span class="hljs-keyword">new</span> <span class="hljs-title class_">Date</span>().<span class="hljs-title function_">getDay</span>() + <span class="hljs-string">&#x27;.jpg)&#x27;</span>);<br></code></pre></td></tr></table></figure><p>在 <code>/source/medias/featureimages</code> 文件夹中默认有 24张特色图片，你可以再增加或者减少，并需要在 <code>_config.yml</code>做同步修改。</p><p>参考</p><p>[1] <ahref="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">hexo-theme-matery中文文档</a></p>]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex 多行公式等号对齐</title>
    <link href="/2023/06/01/latex-duo-xing-gong-shi-dui-qi/"/>
    <url>/2023/06/01/latex-duo-xing-gong-shi-dui-qi/</url>
    
    <content type="html"><![CDATA[<p>在 LaTeX中，使用“align”环境可以方便地实现等号对齐。使用“&amp;”符号可以在每行中分隔等号两边的内容，表示对齐位置，‘’\\‘’表示换行<spanclass="math inline">\(\lambda\)</span>：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;aligned&#125;<br>  2x + 3y <span class="hljs-built_in">&amp;</span>= 7 <span class="hljs-keyword">\\</span><br>  5x - 2y <span class="hljs-built_in">&amp;</span>= 1<br><span class="hljs-keyword">\end</span>&#123;aligned&#125;<br></code></pre></td></tr></table></figure><p>该代码将产生以下等式： <span class="math display">\[\begin{align}  2x + 3y &amp;= 7 \\\\  5x - 2y &amp;= 1\end{align}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github kex_exchange_identification Connection closed by remote host</title>
    <link href="/2023/05/31/github-kex-exchange/"/>
    <url>/2023/05/31/github-kex-exchange/</url>
    
    <content type="html"><![CDATA[<h2 id="问题描述">问题描述</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">kex_exchange_identification: Connection closed by remote host<br>Connection closed by 20.205.243.166 port 22<br>fatal: Could not <span class="hljs-built_in">read</span> from remote repository.<br><br>Please make sure you have the correct access rights<br>and the repository exists.<br></code></pre></td></tr></table></figure><h2 id="解决方法">解决方法</h2><p><strong>本地生成.pub文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-keygen -t ed25519 -C <span class="hljs-string">&quot;your_mail@xxx.com&quot;</span><br>ssh-agent bash<br></code></pre></td></tr></table></figure><p>会在<code>C:\Users\admin\.ssh</code>目录生成一个<code>id_ed25519.pub</code>文件，复制里面的内容。</p><p><strong>Github 新建 SSH key</strong></p><p>打开 Github，点击 Setting 如下图所示：</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531145549324.png"alt="Setting" /><figcaption aria-hidden="true">Setting</figcaption></figure><p>在 Setting 页面中找到 SSH and GPG keys 选项，新建 SSH key。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531145936726.png"alt="SSH and GPG keys" /><figcaption aria-hidden="true">SSH and GPG keys</figcaption></figure><p>Title 内容任意，Key 文本框内填入 <code>id_ed25519.pub</code>内的内容。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531150100824.png"alt="SSH keys" /><figcaption aria-hidden="true">SSH keys</figcaption></figure><p>重新尝试上传。</p><p>如果仍然有上述错误提示可进行以下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">git init<br>git add .<br>git commit -m &quot;init&quot;<br>git branch -M main<br>git remote add origin git@github.com:xxx/xxx.git<br>git push -f -u origin main<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Github</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Github</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（一）基础部署</title>
    <link href="/2023/05/31/github-pages-ge-ren-bo-ke-da-jian-yi-ji-chu-bu-shu/"/>
    <url>/2023/05/31/github-pages-ge-ren-bo-ke-da-jian-yi-ji-chu-bu-shu/</url>
    
    <content type="html"><![CDATA[<h2 id="本地创建环境">本地创建环境</h2><p>本人环境：</p><p><code>windows 10</code></p><h3 id="安装-node.js">安装 node.js</h3><p>建议使用 nvm （node version manager（node版本管理工具））安装node.js，</p><p><strong>下载地址</strong>：https://github.com/coreybutler/nvm-windows/releases</p><p><strong>安装</strong></p><p>（1）双击解压后的文件<code>nvm-setup.exe</code>； （2）选择 nvm安装路径（填坑警告：路径不能有空格！！！） （3）选择 node.js 路径；（4）确认安装； （5）检测：打开 cmd，输入<code>nvm</code>，显示当前 nvm版本以及 nvm 命令，成功！</p><p><strong>使用 nvm</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 1.nvm list 命令 - 显示版本列表</span><br>nvm list // 显示已安装的版本（同 nvm list installed）<br>nvm list installed // 显示已安装的版本<br>nvm list available // 显示所有可以下载的版本<br><br><span class="hljs-comment"># 2.nvm install 命令 - 安装指定版本nodejs</span><br>nvm install 16.15.1// 安装16.15.1版本node<br>nvm install latest // 安装最新版本node<br><br><span class="hljs-comment"># 3.nvm use 命令 - 使用指定版本node</span><br>nvm use 16.15.1 // 使用16.15.1版本node<br><br><span class="hljs-comment"># 4.nvm uninstall 命令 - 卸载指定版本 node</span><br>nvm uninstall 16.15.1 // 卸载16.15.1版本node<br></code></pre></td></tr></table></figure><p>填坑警告：nvm install 的时候，出现无权安装，需<code>以管理员身份</code>运行 cmd。！！！</p><p>本人目前安装 node.js 版本为16.15.1.</p><p><strong>设置nodejsprefix（全局）和cache（缓存）路径（非必须操作）</strong></p><p>在<code>nodejs</code>安装目录下新建两个文件夹，用于存放全局包和缓存，如下： 我的 node.js安装目录：<code>E:\Program\nvm</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm config <span class="hljs-built_in">set</span> prefix <span class="hljs-string">&quot;E:\Program\nvm\node_gobal&quot;</span> <br>npm config <span class="hljs-built_in">set</span> cache <span class="hljs-string">&quot;E:\Program\nvm\node_cache&quot;</span><br></code></pre></td></tr></table></figure><h3 id="安装-hexo">安装 Hexo</h3><p>命令行输入以下命令安装 Hexo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install -g hexo-cli<br></code></pre></td></tr></table></figure><p>输入以下命令验证是否安装成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo -v<br></code></pre></td></tr></table></figure><h2 id="本地部署">本地部署</h2><p>选择一个准备放置博客网站的目录，然后使用以下命令来初始化一个项目：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init LFD-byte.github.io<br><span class="hljs-built_in">cd</span> LFD-byte.github.io<br>npm install<br></code></pre></td></tr></table></figure><p>该命令将会在当前目录下，生成一个名为 <code>LFD-byte.github.io</code>的新目录，当然，你可以把这个名字换成任何你想要的名字，并将<code>hexo</code> 的初始化文件写入其中。</p><p>新建完成后，<code>LFD-byte.github.io</code> 文件夹的目录如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">.<br>├── _config.yml <br>├── package.json <br>├── node_modules <br>├── scaffolds <br>├── <span class="hljs-built_in">source</span> <br>| ├── _drafts <br>| └── _posts <br>└── themes<br></code></pre></td></tr></table></figure><p><code>_config.yml</code>是配置文件，里面有很多可以配置的数据，这里暂时不多介绍，后面的文章里会进行详细说明。</p><p><code>package.json</code> 是应用程序信息，通常不需要关心。</p><p><code>node_modules</code> 用来存放 <code>node</code>相关的模块，通常不需要关心。</p><p><code>scaffolds</code>里面是模版文件，也就是每次新建文章时，都会根据模版文件来创建对应的<code>md</code> 文件，这一点也会在后续的文章里进行详细介绍。</p><p><code>source</code> 是资源文件夹，用来存放用户资源的地方。除<code>_posts</code> 文件夹之外，开头命名为 _ (下划线)的文件 /文件夹和隐藏的文件将会被忽略。</p><p><code>theme</code>是主题文件夹，每个主题的配置都会有些不一样，需要根据具体主题情况来定，后续介绍主题的文章里会有说明。</p><p>在 <code>breeze-blog</code> 目录下使用以下命令来运行我们的博客：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo server<br></code></pre></td></tr></table></figure><p>在默认情况下，服务会使用 <code>4000</code>端口，如果已经被占用，也可以添加 <code>-p</code>参数来换用其它端口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo server -p 8080<br></code></pre></td></tr></table></figure><p>打开 <code>http://localhost:4000</code>即可访问我们生成的网站了。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedntX31VrhMTjFozQ.jpg"alt="Hexo" /><figcaption aria-hidden="true">Hexo</figcaption></figure><p>这样，我们的博客就搭建起来了。</p><h2 id="部署到-github-pages">部署到 Github Pages</h2><p>在你的 Github 账号创建一个新的仓库，仓库命名规范为<code>账号名.github.io</code>，公开仓库。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230604151148301.png"alt="create repository" /><figcaption aria-hidden="true">create repository</figcaption></figure><p>在 Git bash 或 CMD 中<code>LFD-byte.github.io</code>博客目录下执行以下命令连接到 Github：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">git init<br>git add .<br>git commit -m <span class="hljs-string">&quot;first commit&quot;</span><br>git branch -M main<br>git remote add origin git@github.com:xxx/xxx.git<br>git push -u origin main<br></code></pre></td></tr></table></figure><p>在 <code>LFD-byte.github.io</code> 仓库下 Settings 中开启 GithubPages：</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230604152021402.png"alt="open github pages" /><figcaption aria-hidden="true">open github pages</figcaption></figure><p>然后我们修改一下本地的 <code>hexo</code>的配置文件(<code>_config.yml</code>)，我的在<code>LFD-byte.github.io</code>根目录下，找到对应的地方进行修改，指定我们的仓库信息，并修改<code>deploy</code> 信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">deploy:<br><span class="hljs-built_in">type</span>: git<br>repo: git@github.com:xxx/blogxxx<br>branch: main<br></code></pre></td></tr></table></figure><p>把这里的 <code>repo</code> 地址修改为你的仓库地址即可。</p><p>安装 <code>hexo-deployer-git</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> LFD-byte.github.io<br>npm install hexo-deployer-git --save<br></code></pre></td></tr></table></figure><p>进行部署</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean &amp;&amp; hexo generate<br>hexo deploy<br></code></pre></td></tr></table></figure><p>运行完成后，我们的博客文件就顺利部署到 <code>github pages</code>上了，现在我们打开下面网址来查看我们的博客效果： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://用户名.github.io<br></code></pre></td></tr></table></figure></p><p>之后每次我们添加或修改完本地文件后，使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean &amp;&amp; hexo g -d<br></code></pre></td></tr></table></figure><p>即可重新生成项目文件。</p><h2 id="部署到-vercel">部署到 Vercel</h2><p>注册 <a href="https://vercel.com/">Vercel</a>账号，建议谷歌邮箱注册。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedvercel-logo-freelogovectors.net.jpg"alt="vercel" /><figcaption aria-hidden="true">vercel</figcaption></figure><p>使用 GitHub 账户登录 <a href="https://vercel.com/">Vercel</a>，授予Vercel repo 的 read 权限。</p><p>导入 GitHub 账户中的网站 repo，比如此处的 LFD-byte.github.io。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205007431.png"alt="Project import" /><figcaption aria-hidden="true">Project import</figcaption></figure><p>在项目构建中，Framework Preset 选择 Other，其余不做改动。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205116417.png"alt="Build" /><figcaption aria-hidden="true">Build</figcaption></figure><p>稍等片刻，部署成功，此时我们就可以直接通过部署完成后 Vercel提供的域名访问个人网站了，点击 Visit 进行访问。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205341204.png"alt="Production Deployment" /><figcaption aria-hidden="true">Production Deployment</figcaption></figure><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205507057.png"alt="Visit Blog" /><figcaption aria-hidden="true">Visit Blog</figcaption></figure><p>之后每次我们添加或修改完本地文件后，使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean &amp;&amp; hexo g -d<br></code></pre></td></tr></table></figure><p>就可以生成静态文件同步部署到 Vercel 上了。</p><h2 id="注意">注意</h2><p>若 npm 不能下载包，可通过 nvm 更换 node.js 版本重新尝试下载。</p><h2 id="参考">参考</h2><p>[1] <ahref="https://mfrank2016.github.io/breeze-blog/2020/05/02/hexo/hexo-start/">【Hexo】使用Hexo+githubpages+travis ci搭建好看的个人博客（一）</a></p><p>[2] <ahref="https://blog.csdn.net/liangpingguo/article/details/125324362">Windows下使用nvm安装nodejs</a></p><p>[3] <ahref="https://blog.csdn.net/weixin_40026797/article/details/126919662">建站过程中的踩坑记录：自定义域名、百度收录与备案</a></p>]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（四）图神经网络（GNN）</title>
    <link href="/2023/05/31/tu-shen-jing-wang-luo-xue-xi-ri-ji-si-tu-shen-jing-wang-luo-gnn/"/>
    <url>/2023/05/31/tu-shen-jing-wang-luo-xue-xi-ri-ji-si-tu-shen-jing-wang-luo-gnn/</url>
    
    <content type="html"><![CDATA[<p><strong>置换不变性和置换同变性</strong></p><p>任何将邻接矩阵<spanclass="math inline">\(A\)</span>作为输入的函数<spanclass="math inline">\(f\)</span>在理想状态下，都应满足下面两个条件之一：<span class="math display">\[\begin{align}f(PAP^T)&amp;=f(A)\text{(置换不变)} \\f(PAP^T)&amp;=Pf(A)\text{(置换同变)}\end{align}\]</span></p><p>其中<spanclass="math inline">\(P\)</span>是置换矩阵。置换不变是指函数不依赖邻接矩阵中行/列的任意顺序，置换同变表示当置换邻接矩阵时<spanclass="math inline">\(f\)</span>的输出以一致的方式置换。</p><h2 id="神经消息传递">神经消息传递</h2><h3 id="gnn-框架">GNN 框架</h3><p>在 GNN 的每个消息传递迭代期，通过聚合每个节点<spanclass="math inline">\(u \in \mathcal{V}\)</span>的邻域<spanclass="math inline">\(\mathcal{N}(u)\)</span>的信息来更新其隐藏嵌入<spanclass="math inline">\(h_u^{(k)}\)</span>，过程如下式所示： <spanclass="math display">\[\begin{align}  h_u^{(k+1)} &amp; = UPDATE^{(k)}(h_u^{(k)},AGGREGATE^{(k)}(\{h_v^{(k)}, \forall v \in \mathcal{V}(u)\})) \\\\  &amp; = UPDATE^{(k)}(h_u^{(k)}, m_{\mathcal{N}(u)}^{(k)})\end{align}\]</span></p><p>其中 <span class="math inline">\(UPDATA\)</span> 和 <spanclass="math inline">\(AGGREGATE\)</span> 是任意可微函数，<spanclass="math inline">\(m_{\mathcal{N}(u)}\)</span> 是聚合节点 <spanclass="math inline">\(u\)</span>邻域消息的结果，上标表示消息迭代期的索引。</p><p>迭代最后一层的输出定义为每个节点的嵌入： <spanclass="math display">\[z_u = h_u^{(K)}, \forall u \in \mathcal{V}\]</span> 由于 <span class="math inline">\(AGGREGATE\)</span>函数将整个集合作为输入，这种方式定义的 GNN 是置换同变的。</p><p>节点嵌入编码了两种形式的信息。</p><p><strong>图的结构信息</strong></p><p><strong>基于节点特征的信息</strong></p><h3 id="gnn-实例">GNN 实例</h3><p>基本 GNN 的消息传递定义如下式： <span class="math display">\[h_u^{(k)} = \sigma(W_{self}^{(k)}h_u^{(k-1)} + W_{neigh}^{(k)}\sum_{v\in \mathcal{N}(u)} h_v^{(k-1)} + b^{(k)})\]</span> 其中，<span class="math inline">\(W_{self}^{(k)}\)</span> 和<span class="math inline">\(W_{neigh}^{(k)} \in \mathbb{R}^{d^{(k)}\times d^{(k-1)}}\)</span> 是可训练参数矩阵，<spanclass="math inline">\(\sigma\)</span> 表示逐元素的非线性函数。</p><p>通过定义更新和聚合函数等效地定义基本的 GNN： <spanclass="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)} h_v \\\\UPDATE(h_u, m_{\mathcal{N}(u)}) = \sigma (W_{self} h_u +W_{neigh}m_{\mathcal{N}(u)})\]</span> 下式可作为从节点 <span class="math inline">\(u\)</span>的图上邻域聚合消息的简写： <span class="math display">\[m_{\mathcal{N}(u)} = AGGREGATE^{(k)}(\{h_v^{(k)}, \forall v \in\mathcal{N}(u)\})\]</span> <strong>图级别 GNN 定义</strong> <span class="math display">\[H^{(k)} = \sigma(AH^{(k-1)}W_{neigh}^{(k)} + H^{(k-1)}W_{self}^{(k)})\]</span> 其中，<span class="math inline">\(H^{(k)} \in\mathbb{R}^{|\mathcal{V}| \times d}\)</span> 表示 GNN 中第 <spanclass="math inline">\(k\)</span>层的节点表示矩阵（每个节点对应矩阵的一行），<spanclass="math inline">\(A\)</span> 是邻接矩阵。</p><p><strong>自环消息传递</strong></p><p>添加自环并省略显示的更新步骤消息传递可定义如下： <spanclass="math display">\[h_u^{(k)} = AGGREGATE(\{ h_v^{(k-1)}, \forall v \in \mathcal{N}(n)\bigcup \{u\} \})\]</span> 其中，聚合在集合 <span class="math inline">\(\mathcal{N}(u)\bigcup \{u\}\)</span>上进行。这种消息传递方式可以缓解拟合问题，也因为无法区分节点和邻域的信息严重限制了GNN 的表达能力。</p><p>在基本 GNN 模型中，添加自环等效于在 <spanclass="math inline">\(W_{self}\)</span> 和 <spanclass="math inline">\(W_{neigh}\)</span>矩阵之间共享参数，<strong>图级别更新方式</strong>如下所示： <spanclass="math display">\[H^{(t)} = \sigma ((A+I)H^{(t-1)}W^{(t)})\]</span></p><h2 id="广义邻域聚合">广义邻域聚合</h2><h3 id="邻域归一化">邻域归一化</h3><p>最基本的邻域聚合函数仅取邻居嵌入的总和，这种方法有可能不稳定并且对节点度高度敏感。解决该问题方案之一是基于所涉及节点的度来归一化聚合操作。</p><p>均值代替求和，如下式所示： <span class="math display">\[m_{\mathcal{N}(u)} = \frac{\sum_{v \in\mathcal{N}(n)}h_v}{|\mathcal{N}(u)|}\]</span> 对称归一化，如下式所示： <span class="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)}\frac{h_v}{\sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}}\]</span> 图卷积神经网络（GCN）</p><p>采用对称归一化聚合及自环更新方法，GCN 消息传递函数如下式定义： <spanclass="math display">\[h_u^{(k)} = \sigma (W^{(k)} \sum_{v \in \mathcal{N}(u)} \bigcup \{u\}\frac{h_v}{|\mathcal{N}(u)||\mathcal{N}(v)|})\]</span> 是否归一化？</p><p>归一化可能导致信息丢失，在归一化后可能很难使用学习到的嵌入来区分不同度的节点，并且归一化会掩盖各种其他的图结构特征。</p><p>在通常情况下，在节点特征信息远比结构信息有用或由于节点度范围过于广泛导致优化过程可能不稳定的任务中，归一化最有用。</p><h3 id="集合聚合操作">集合聚合操作</h3><p><strong>集合池化</strong></p><p>一种定义聚合函数的原则是基于置换不变神经网络的理论，具有下式的聚合函数是通用的集合函数逼近器：<span class="math display">\[m_{\mathcal{N}(u)} = MLP_{\theta} (\sum_{v \in \mathcal{N}(u)}MLP_{\phi}(h_v))\]</span> 依照惯例用 <span class="math inline">\(MLP_{\theta}\)</span>表示可训练参数为 <span class="math inline">\(\theta\)</span>的任一深度多层感知器。将一组嵌入映射到一个嵌入的任何置换不变函数都可以基于上式的模型逼近到任意精度。</p><p><strong>Janossy 池化</strong></p><p>不使用置换不变的压缩方法（如求和或取均值），而是采用置换敏感的函数并对多种可能的置换取均值。具体操作为：令<span class="math inline">\(\pi_i \in \Pi\)</span> 表示将集合 <spanclass="math inline">\(\{h_v, \forall v \in \mathcal{N}(u)\}\)</span>映射到特定序列 <span class="math inline">\(((h_{v_1}, h_{v_2}, \cdots,h_{v_{| \mathcal{N}(u) |}})_{\pi_i})\)</span> 的置换函数。即 <spanclass="math inline">\(\pi_i\)</span>将无序的邻居嵌入集置于任意排列的序列中。然后通过 Janossy池化实现邻域聚合，如下式所示：</p><p><span class="math display">\[m_{\mathcal{N}(u)} = MLP_{\theta} (\frac{1}{|\Pi|} \sum_{\pi \in \Pi}\rho_{\phi} (h_{v_1}, h_{v_2}, \cdots,h_{v_{|\mathcal{N}(u)|}})_{\pi_i})\]</span></p><p>其中，<span class="math inline">\(\Pi\)</span> 表示一组置换函数，<span class="math inline">\(\rho_{\phi}\)</span>是置换敏感的函数（如应用于序列数据集的神经网络）。在实践中，通常将 <spanclass="math inline">\(\rho_{\phi}\)</span> 定义为 LSTM。</p><p>如果上式中的置换函数集合 <span class="math inline">\(\Pi\)</span>包含所有可能的置换函数，则上式中的聚合函数也是通用集合函数逼近器。但是对所有可能的置换求和很困难，在实践中通常采用如下两种方法进行Janossy 池化：</p><p>1在每次应用聚合函数时，对所有可能的置换采样出一个随机子集，并且对该随机自己进行求和。</p><p>2对邻域中的节点进行规范化排序，例如，根据节点度对节点进行降序排序，并随机断开一些关联关系。</p><h3 id="邻域注意力模型">邻域注意力模型</h3><p>基本思想是为每个邻域中的节点分配注意力权重，该权重用于在聚合步骤中权衡该节点的影响力。第一个引入注意力机制的GNN模型是图注意力网络GAT，该网络使用注意力权重来定义邻域的加权和，如下式所示： <spanclass="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)} \alpha_{u,v}h_v\]</span> 其中，<span class="math inline">\(\alpha_{u,v}\)</span>表示在节点 <span class="math inline">\(u\)</span>处聚合信息时，其邻域中的节点 <span class="math inline">\(v \in\mathcal{N}(u)\)</span> 的注意力权重。GAT 中注意力权重的定义如下式所示：<span class="math display">\[\alpha_{u,v} = \frac{exp([Wh_v \bigoplus Wh_v])}{\sum_{v&#39; \in\mathcal{N}(u)} exp(a^T[Wh_v \bigoplus Wh_{v&#39;}])}\]</span> 其中，<span class="math inline">\(a\)</span>是可训练的注意力向量，<span class="math inline">\(W\)</span>是可训练的矩阵，<span class="math inline">\(\bigoplus\)</span>表示拼接操作。</p><p>注意力机制变体： <span class="math display">\[\alpha_{u,v} = \frac{exp(h_u^TWh_v)}{\sum_{v&#39; \in \mathcal{N}(u)}exp(h_u^TWh_{v&#39;})}\]</span> MLP 注意力层的变体： <span class="math display">\[\alpha_{u,v} = \frac{exp(MLP(h_u,h_v))}{\sum_{v&#39; \in \mathcal{N}(u)}exp(MLP(h_u,h_{v&#39;}))}\]</span> 上式限定 MLP 输出为标量。</p><p>添加多注意力头，使用彼此独立的 <span class="math inline">\(K\)</span>个注意力层计算 <span class="math inline">\(K\)</span> 个不同的注意力权重<span class="math inline">\(\alpha_{u,v,k}\)</span>，然后使用不同的注意力权重聚合的消息会在聚合步骤中进行转换和合并，通常是先进性线性映射，再进行拼接操作，如下式所示：<span class="math display">\[\begin{aligned}m_{\mathcal{N}(u)} &amp;= [a_1 \bigoplus a_2 \bigoplus \cdots \bigoplusa_K] \\\\a_k &amp;= W_i \sum_{v \in \mathcal{N}(u)} \alpha_{u,v,k}h_v\end{aligned}\]</span> 其中，<span class="math inline">\(K\)</span>个注意力头中的每一个注意力权重 <spanclass="math inline">\(\alpha_{u,v,k}\)</span>可以使用上述任何一种注意力机制进行计算。</p><h2 id="广义更新方法">广义更新方法</h2><p><strong>过度平滑和邻域影响</strong></p><p>GNN 的一个常见问题是过度平滑。过度平滑的基本原理是：经过多次 GNN消息传递后，图中所有节点的表示可能变得非常相似。过度平滑导致无法建立更深的GNN 模型以利用图上的长期依赖关系，因为这些深层的 GNN模型往往会生成过度平滑的嵌入。</p><p>可以通过定义每个节点的输入特征 <spanclass="math inline">\(h_u^{(0)}=x_u\)</span>对图上其它节点的最终层输出的嵌入（<spanclass="math inline">\(h_v^{(K)},\forall v \in\mathcal{V}\)</span>）的影响来形式化定义 GNN中的过度平滑问题。对于任意一对节点 <spanclass="math inline">\(u\)</span> 和 <spanclass="math inline">\(v\)</span>，可以通过检查相应的雅可比矩阵的大小来量化 GNN 中节点 <spanclass="math inline">\(u\)</span> 对节点 <spanclass="math inline">\(v\)</span> 的影响，如下式所示： <spanclass="math display">\[I_{K(u,v)} = 1^T(\frac{\partial h_v^{(K)}}{\partial h_u^{(0)}})1\]</span> 其中，<span class="math inline">\(1\)</span> 是元素全为 1的向量。<span class="math inline">\(I_{K(u,v)}\)</span> 是雅可比矩阵<span class="math inline">\(\frac{\partial h_v^{(K)}}{\partialh_u^{(0)}}\)</span> 中的元素之和。用来衡量 GNN 中节点 <spanclass="math inline">\(u\)</span> 的初始嵌入对节点 <spanclass="math inline">\(v\)</span> 的最终嵌入的影响程度。</p><p>定理：对于任何使用自环更新方法和用下式表示聚合函数的 GNN 模型 <spanclass="math display">\[AGGRGATE(\{h_v, \forall v \in \mathcal{N}(u) \bigcup \{u\}\}) =\frac{1}{f_n(|\mathcal{N}(u) \bigcup \{u\}|)} \sum_{v \in \mathcal{N}(u)\bigcup \{u\}} h_v\]</span> 其中，<span class="math inline">\(f:\mathbb{R}^+ \rightarrow\mathbb{R}^+\)</span> 是任意可微的归一化函数。</p><p>可得出下式结论： <span class="math display">\[I_K (u,v) \propto p_{\mathcal{G},K}(u|v)\]</span> 其中，<spanclass="math inline">\(p_{\mathcal{G},K}(u|v)\)</span> 表示从节点 <spanclass="math inline">\(u\)</span> 开始的 <spanclass="math inline">\(K\)</span> 步随机游走过程中访问节点 <spanclass="math inline">\(v\)</span> 的概率。</p><p>当使用 <span class="math inline">\(K\)</span> 层 GCN 型魔性时，节点<span class="math inline">\(u\)</span> 对节点 <spanclass="math inline">\(v\)</span> 的影响从节点 <spanclass="math inline">\(u\)</span> 开始经过 <spanclass="math inline">\(K\)</span> 步随机游走到达节点 <spanclass="math inline">\(v\)</span> 的概率成正比。但是，随着 <spanclass="math inline">\(K \rightarrow\infty\)</span>，每个节点的影响都接近图上随机游走的平稳分布，这意味着本地邻域信息会丢失。</p><p>上述定理直接适用于使用自环更新方法的模型，但是只要任意层 <spanclass="math inline">\(k\)</span> 满足 <span class="math inline">\(\|W_{self}^{(k)} \| &lt; \| W_{neigh}^{(k)}\|\)</span>，其结果也可以渐近地扩展基本 GNN 的更新。因此当使用简单 GNN模型时，构建更深的模型实际上会损害模型性能。随着更多层的加入，模型将丢失更多关于本地邻域结构的信息，并且学习的嵌入会变得过于平滑，接近几乎均匀的分布。</p><h3 id="拼接和跳跃连接">拼接和跳跃连接</h3><p>最简单更新跳跃连接的方式之一是使用拼接操作再消息传递期间保留更多节点级别信息，如下式所示：<span class="math display">\[UPDATE_{concat}(h_u, m_{\mathcal{N}(u)}) =[UPDATE_{base}(h_u,m_{\mathcal{N}(u)}) \bigoplus h_u]\]</span>其中，直接将基本更新函数的输出与节点的上一层表示拼接，鼓励模型再消息传递过程中解耦信息，将来自邻域的信息（<spanclass="math inline">\(m_{\mathcal{N}(u)}\)</span>）与每个节点当前的表示（<spanclass="math inline">\(h_u\)</span>）分开。</p><p>线性插值法跳跃连接，如下式所示： <span class="math display">\[UPDATE_{interpolate}(h_u,m_{\mathcal{N}(u)}) = \alpha_1 \circUPDATE_{base}(h_u,m_{\mathcal{N}(u)}) + \alpha_2 \bigodot h_u\]</span> 其中，<span class="math inline">\(\alpha_1,\alpha_2 \in[0,1]^d\)</span> 是满足 <span class="math inline">\(\alpha_2 = 1 -\alpha_1\)</span> 的门控向量，<span class="math inline">\(\circ\)</span>表示逐元素相乘。</p><p>最终更新的表示是先前表示与基于邻域信息进行更新的表示之间的线性插值。</p><p>在通常情况下，拼接和跳跃连接有助于缓解 GNN中过渡平滑问题，同时可以提高优化数值的稳定性。</p><h3 id="门控更新函数">门控更新函数</h3><p>一种解读 GNN消息传递算法的观点是：聚合函数从邻域接收观察结果，然后将其用于更新每个节点的隐状态。基于这一观点可以根据观察结果直接使用更新RNN 框架的隐状态的方法，最早的 GNN 架构之一定义更新函数如下式所示：<span class="math display">\[h_u^{(k)} = GRU(h_u^{(k-1)},m_{\mathcal{N}(u)}^{(k)})\]</span> 其中，GRU 表示 GRU 单元的更新函数。</p><p>门控更新方法在提高 GNN框架的模型深度（超过10层）和防止过度平滑问题方面非常有效。</p><h3 id="跳跃知识连接">跳跃知识连接</h3><p>提高最终的节点表示质量的一种补充策略是利用消息传递的每一层输出的表示，叫做加入跳跃知识，如下式所示：<span class="math display">\[z_u = f_{JK}(h_u^{(0)} \bigoplus h_u^{(1)} \bigoplus \cdots \bigoplush_u^{(K)})\]</span> 其中，<span class="math inline">\(f_{JK}\)</span>是任意微分函数。</p><h2 id="边特征和多元关系-gnn">边特征和多元关系 GNN</h2><p>下面介绍 GNN 在多元关系图或其它异构图中的应用。</p><h3 id="关系-gnn">关系 GNN</h3><p>关系图卷积网络（RGCN），通过为每种关系类型指定一个单独的变化矩阵来增强聚合函数处理多种关系的能力，如下式所示：<span class="math display">\[m_{\mathcal{N}(u)} = \sum_{\tau \in \mathcal{R}} \sum_{v \in\mathcal{N}_{\tau}(u)} \frac{W_{\tau}h_v}{f_n(\mathcal{N}(u),\mathcal{N}(v))}\]</span> 其中， <span class="math inline">\(f_n\)</span>是一个归一化函数，它的值取决于节点 <spanclass="math inline">\(u\)</span> 的邻域以及被聚合的节点 <spanclass="math inline">\(v\)</span> 的邻域。RGCN中的多元关系聚合类似具有归一化函数的基本GNN，但根据边的类型不同分别聚合信息。</p><p><strong>参数共享</strong></p><p>朴素 RGCN方法的一个缺点是由于每一种关系类型都需要一个可训练的矩阵导致参数量急剧增加，这种参数量的激增可能导致过拟合和训练缓慢的问题。</p><p>通过与基矩阵共享参数的方法来解决此问题，如下式： <spanclass="math display">\[W_{\tau} = \sum_{i=1}^b \alpha_{i,\tau}B_i\]</span> 该方法中，所有关系矩阵都定义为 <spanclass="math inline">\(b\)</span> 个基矩阵（<spanclass="math inline">\(B1,\cdots,B_b\)</span>）的线性组合；唯一的关于关系的参数是每种关系<span class="math inline">\(\tau\)</span> 的 <spanclass="math inline">\(b\)</span> 个组合权重 <spanclass="math inline">\(\alpha_{1,\tau}, \cdots,\alpha_{b,\tau}\)</span>。在这种基本共享方法中，看可以将完整聚合函数表示为下式：<span class="math display">\[m_{\mathcal{N}(u)} = \sum_{\tau \in \mathcal{R}} \sum_{v \in\mathcal{N}_{\tau}(u)} \frac{\alpha_{\tau} \times_{1} B \times_{2}h_v}{f_n(\mathcal{N}(u), \mathcal{N}(v))}\]</span> 其中，<span class="math inline">\(B = (B_1, \cdots,B_b)\)</span> 是一个由基矩阵堆叠构成的张量， <spanclass="math inline">\(\alpha_{\tau} = \alpha_{1,\tau}, \cdots,\alpha_{b,\tau}\)</span> 是一个关于关系 <spanclass="math inline">\(\tau\)</span> 的包含基矩阵组合权重的向量，<spanclass="math inline">\(\times_{i}\)</span> 表示沿着模 <spanclass="math inline">\(i\)</span> 的张量积。另一种理解参数共享 RGCN方法的过程是：学习每个关系的嵌入及所有关系之间共享的张量。</p><h3 id="注意力机制和特征拼接">注意力机制和特征拼接</h3><p>为适应更一般形式的边特征的情况，可以在消息传递过程中基于注意力机制或将这些信息与邻域嵌入拼接来充分利用这些特征。在给定任意基本聚合方法<span class="math inline">\(AGGREGATE_{base}\)</span>的情况下，利用边特征的一种简单策略是如下式定义新的聚合函数： <spanclass="math display">\[m_{\mathcal{N}(u)} = AGGREGATE_{base}(\{h_v \bigoplus e_{(u,\tau,v)},\forall v \in \mathcal{N}(u) \})\]</span> 其中，<span class="math inline">\(e_{(u,\tau,v)}\)</span>表示边 <span class="math inline">\((u,\tau,v)\)</span> 的特征。</p><h2 id="图池化">图池化</h2><p><strong>集合池化方法</strong></p><p>与 AGGREGATE操作类似，图池化任务可以看作是解决集合上的问题。要设计一个池化函数 <spanclass="math inline">\(f_p\)</span> 将一组节点嵌入 <spanclass="math inline">\(\{ z_1, \cdots, z_{|V|} \}\)</span>映射为表示整张图的嵌入 <spanclass="math inline">\(z_{\mathcal{G}}\)</span>。</p><p>第一种常用方法是对节点嵌入求和（或取均值），如下式所示： <spanclass="math display">\[z_{\mathcal{G}} = \frac{\sum_{v \in \mathcal{V}}z_c}{f_n(|\mathcal{V}|)}\]</span> 其中，<span class="math inline">\(f_n\)</span>是归一化函数（如恒等函数）。适用于小规模图。</p><p>第二种常用方法基于集合的方法，结合了 LSTM和注意力机制来池化节点嵌入。这种池化方法迭代 <spanclass="math inline">\(t=1,\cdots,T\)</span>步基于注意力机制的聚合操作，如下式所示： <span class="math display">\[\begin{aligned}q_i &amp;= LSTM(o_{t-1}, q_{t-1}) \\\\e_{v,t} &amp;= f_a(z_v,q_t),\forall v \in \mathcal{V} \\\\a_{v,t} &amp;= \frac{exp(e_{v,i})}{\sum_{u \in \mathcal{V}} e_{u,t}},\forall v \in \mathcal{V} \\\\o_t &amp;= \sum_{v \in \mathcal{V}} a_{v,t} z_v\end{aligned}\]</span></p><p>其中，<span class="math inline">\(q_t\)</span> 表示每次迭代 <spanclass="math inline">\(t\)</span>次注意力机制中的查询向量。查询向量用于使用注意力函数 <spanclass="math inline">\(f_a: \mathbb{R}^d \times \mathbb{R} \rightarrow\mathbb{R}\)</span>（如点积）为每个节点计算注意力分数，然后将该注意力分数进行归一化，最后根据注意力权重计算节点嵌入的加权和，并基于该加权和采用LSTM 更新来更新查询向量。通常情况下，用全零向量初始化 <spanclass="math inline">\(q_0\)</span> 和 <spanclass="math inline">\(o_0\)</span> ，进行了 <spanclass="math inline">\(T\)</span> 次迭代后计算整张图的嵌入如下式所示：<span class="math display">\[z_{\mathcal{G}} = o_1 \bigoplus o_2 \bigoplus \cdots \bigoplus o_T\]</span> <strong>图粗糙化方法</strong></p><p>集合池化方法的局限性在于不能利用图的结构信息。在池化阶段利用图的拓扑信息可以进一步提供增益，实现次目的的一种流形策略是用图聚类或粗糙化作为池化节点表示的一种方法。</p><p>假设有聚类函数如下式所示： <span class="math display">\[f_c \rightarrow \mathcal{G} \times \mathbb{R}^{|V| \times d} \rightarrow\mathbb{R}^{+|V| \times c}\]</span> 聚类函数将图上的所有节点分为 <spanclass="math inline">\(c\)</span> 个簇。假定该函数输出一个分配矩阵 <spanclass="math inline">\(S = f_c(\mathcal{G}, Z)\)</span>，其中，<spanclass="math inline">\(S[u,i] \in \mathbb{R}\)</span> 表示节点 <spanclass="math inline">\(u\)</span> 和簇 <spanclass="math inline">\(i\)</span> 之间的关联强度。</p><p>图粗糙化方法的关键思想是使用聚类分配矩阵来粗糙化图。这里使用分配矩阵<span class="math inline">\(S\)</span>来计算新的粗糙化邻接矩阵和一个新的节点特征集合，如下式所示： <spanclass="math display">\[\begin{aligned}A^{new} &amp;= S^TAS \in \mathbb{R}^{+c \times c} \\\\X^{new} &amp;= S^TX \in \mathbb{R}^{c \times d}\end{aligned}\]</span>这一新的邻接矩阵表示图中的簇之间的关联强度（边），而新的特征矩阵表示聚合分配给每个簇的所有节点嵌入的结果。在该粗糙化的图上运行GNN，并在每次迭代的过程中重复粗糙化过程，图在每一步都会减小，最后在足够粗糙化的图上对节点嵌入执行集合池化可以获得图的最终表示。</p><h2 id="通用消息传递方法">通用消息传递方法</h2><p>GNN消息传递方法可以泛化为在消息传递的每个阶段利用边和图级别的信息。</p><p>更为通用的消息传递方法如下式： <span class="math display">\[\begin{aligned}h_{(u,v)}^{(k)} &amp;= UPDATE_{edge}(h_{(u,v)}^{(k-1)}, h_u^{(k-1)},h_v^{(k-1)}, h_{\mathcal{G}}^{(k-1)}) \\\\m_{\mathcal{N}(u)} &amp;= AGGREGATE_{node}(\{ h_{(u,v)}^{(k-1)}, \forallv \in \mathcal{N}(u) \}) \\\\h_u^{(k)} &amp;= UPDATE_{node}(h_u^{(k-1)}, m_{\mathcal{N}(u)},h_{\mathcal{G}}^{(k-1)}) \\\\h_{\mathcal{G}}^{(k)} &amp;= UPDATE_{graph}(h_{\mathcal{G}}^{(k-1)}, \{h_u^{(k-1)}, \forall u \in \mathcal{V}, \{ h_{(u,v)}^{(k)}, \forall(u,v) \in \varepsilon \} \})\end{aligned}\]</span> 通用消息传递框架中，在消息传递过程中为图上的每条边生成嵌入<spanclass="math inline">\(h_(u,v)^{(k)}\)</span>，并为整张图生成相应的嵌入<spanclass="math inline">\(h_{\mathcal{G}}^{(k)}\)</span>，这使得消息传递模型可以聚合边和图级别的特征。</p><p>在通用消息传递框架中进行消息传递时，首先根据便关联的节点的嵌入来更新边的嵌入。接下来，通过聚合节点关联所有边的嵌入来更新节点嵌入。图嵌入被用于节点和边表示的更新函数中，并且图级别的嵌入本身通过在每次迭代结束时对所有节点和边的嵌入进行聚合来更新。</p><h2 id="损失函数">损失函数</h2><p><strong>用于节点分类的 GNN</strong></p><p>以完全监督方式训练 GNN，使用 softmax分类函数和负对数似然损失来定义损失函数，如下式：</p><p><span class="math display">\[\mathcal{L} = \sum_{u \in \mathcal{V}_{train}} = -log(softmax(z_u,y_u))\]</span></p><p>其中，假设 <span class="math inline">\(y_u \in \mathbb{Z}^c\)</span>是一个独热向量，表示用于训练的节点 <span class="math inline">\(u \in\mathcal{V}_{train}\)</span> 的类。</p><p>在引用网络中，<span class="math inline">\(y_u\)</span>表示论文的主题，<span class="math inline">\(Softmax(z_u,y_u)\)</span>表示通过 softmax 函数计算节点属于类 <spanclass="math inline">\(y_u\)</span> 的概率，如下式：</p><p><span class="math display">\[softmax(z_u,y_u) = \sum_{i=1}^c y_u[i]\frac{e^{z_u^Tw_i}}{\sum_{j=1}^ce^{z_u^Tw_j}}\]</span></p><p>其中，<span class="math inline">\(w_i \in\mathbb{R}^d,i=1,\cdots,c\)</span> 是可训练的参数。</p><p><strong>用于图分类的 GNN</strong></p><p>图分类的损失函数值是通过一组有标记的训练图 <spanclass="math inline">\(\mathcal{T} ={\mathcal{G}_1,\cdots,\mathcal{G}_n}\)</span> 上学习的图嵌入 <spanclass="math inline">\(z_{\mathcal{G}_i}\)</span>计算，通常使用如下式定义的平方误差损失函数：</p><p><span class="math display">\[\mathcal{L} = \sum_{\mathcal{G} \in \mathcal{T}}\|MLP(z_{\mathcal{G}_i}) - y_{\mathcal{G}_i} \|_2^2\]</span></p><p>其中，MLP 是具有单一变量输出的密集连接的神经网络，<spanclass="math inline">\(y_{\mathcal{G}_i} \in \mathbb{R}\)</span> 是训练图<span class="math inline">\(\mathcal{G}_i\)</span> 的标签值。</p><p><strong>用于关系预测的 GNN</strong></p><p>深度图信息最大化（DGI）节点嵌入 <spanclass="math inline">\(z_u\)</span> 和图嵌入 <spanclass="math inline">\(z_{\mathcal{G}}\)</span>之间的互信息，损失函数如下式：</p><p><span class="math display">\[\mathcal{L} = -\sum_{u \in \mathcal{V}_{train}} \mathbb{E}_{\mathcal{G}}log(D(z_u,z_{\mathcal{G}})) +\gamma\mathbb{E}log(1-D(\tilde{z}_u,z_{\mathcal{G}}))\]</span></p><p>其中，<span class="math inline">\(z_u\)</span> 表示 GNN 根据图 <spanclass="math inline">\(\mathcal{G}\)</span> 生成的节点 <spanclass="math inline">\(u\)</span> 的嵌入，而 <spanclass="math inline">\(\tilde{z}_u\)</span> 表示根据 <spanclass="math inline">\(\mathcal{G}\)</span> 的损坏版本 <spanclass="math inline">\(\tilde{\mathcal{G}}\)</span> 生成的节点 <spanclass="math inline">\(u\)</span> 的嵌入。这里用 <spanclass="math inline">\(D\)</span>表示判别函数，它是一个被训练用以预测节点嵌入是基于真实的图 <spanclass="math inline">\(\mathcal{G}\)</span> 还是损坏版本的图 <spanclass="math inline">\(\tilde{\mathcal{G}}\)</span>生成的。在通常情况下，通过一种随机的方式（如打乱特征矩阵中的元素）修改节点特征或邻接矩阵，抑或是同时修改两者来破坏图。上述损失函数背后的思想是：GNN模型必须学会生成可以区分真是图和其损坏版本的节点嵌入。这一优化目标与最大化节点嵌入<span class="math inline">\(z_u\)</span> 和图嵌入 <spanclass="math inline">\(z_{\mathcal{G}}\)</span>之间的互信息密切相关。</p><h2 id="节点采样">节点采样</h2><p>基于节点级别消息传递的角度直接实现 GNN可能存在计算效率低的问题。例如，当多个节点共享邻域时，如果图中所有节点独立执行消息传递操作，最终可能会执行大量冗余计算。</p><p><strong>图级别的实现方法</strong></p><p><strong>子采样和小批量</strong></p><p>为了限制 GNN的内存占用并促进小批量训练方式，可以在消息传递过程中使用节点集的子集。从数学角度，可以认为这是在每个批次中为图中节点的子集运行节点级的GNN 公式。</p><p>挑战在于不能在不丢失信息的情况下简单地在图上的一部分节点中执行消息传递操作，每次删除节点时也会删除其关联的边，这无法保证选择的节点的随机子集会构成连接图，并且为每个小批次选择一个节点的随机自己会对模型性能产生严重的不利影响。</p><p>通过对节点邻域进行子采样的策略来克服此问题：首先为每个批次选择一组目标节点，然后递归采样这些节点的邻域以确保能够保持图的连通性。为了尽可能避免为一个批次采样过多节点的情况，建议使用固定容量对每个节点的邻域进行子采样以提高批量张量操作的效率。</p><h2 id="参数共享和正则化">参数共享和正则化</h2><p><strong>层间参数共享</strong></p><p>在 GNN 的所有聚合和更新函数中使用相同的函数。通常情况下，在6层以上的GNN 中最有效，并且通常与门控更新函数结合使用。</p><p><strong>边丢弃</strong></p><p>训练过程中随机删除（或屏蔽）邻接矩阵的边。直观来看，这将使 GNN不太容易过拟合，并且对邻接矩阵中的噪声更具鲁棒性。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python程序运行时间查看方法</title>
    <link href="/2023/05/30/python-cheng-xu-yun-xing-shi-jian-cha-kan-fang-fa/"/>
    <url>/2023/05/30/python-cheng-xu-yun-xing-shi-jian-cha-kan-fang-fa/</url>
    
    <content type="html"><![CDATA[<p>time包查看程序运行时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><br>start_time = time.time()  <span class="hljs-comment"># 记录程序开始运行时间</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000000</span>):<br>    <span class="hljs-keyword">pass</span><br>end_time = time.time()  <span class="hljs-comment"># 记录程序结束运行时间</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;cost %f s&#x27;</span> % (end_time - start_time))<br></code></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>cost <span class="hljs-number">0.179781</span> s<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（三）节点嵌入</title>
    <link href="/2023/05/29/tu-shen-jing-wang-luo-xue-xi-ri-ji-san-jie-dian-qian-ru/"/>
    <url>/2023/05/29/tu-shen-jing-wang-luo-xue-xi-ri-ji-san-jie-dian-qian-ru/</url>
    
    <content type="html"><![CDATA[<p>节点嵌入目的是将节点编码为包含图位置和局部邻域结构信息的低维向量，其几何关系与原始图或网络中的关系相对应。</p><h2 id="简单加权图节点嵌入">简单加权图节点嵌入</h2><h3 id="基于编码-解码框架">基于编码-解码框架</h3><p>编码器将图中每个节点映射为低维向量或低维嵌入，解码器将利用低维节点嵌入重构原始图中每一个节点的邻域信息。</p><p><strong>编码器</strong></p><p>编码器将节点 <span class="math inline">\(v \in \mathcal{V}\)</span>映射为向量嵌入 <span class="math inline">\(z_v \in \mathbb{R}^d\)</span>，其中，<span class="math inline">\(z_v\)</span> 为节点 <spanclass="math inline">\(v \in \mathcal{V}\)</span>对应的嵌入表示，可表示为下式： <span class="math display">\[ENC: \mathcal{V} \rightarrow \mathbb{R}^d\]</span> 编码器依赖 shallow embedding 方法，即根据节点的 ID进行简单地嵌入查找，如下式： <span class="math display">\[ENC(v) = Z[v]\]</span> 其中，<span class="math inline">\(Z \in\mathbb{R}^{|\mathcal{V}| \times d}\)</span>为包含所有节点嵌入向量的矩阵，<span class="math inline">\(Z[v]\)</span>表示节点 <span class="math inline">\(v\)</span> 对应 <spanclass="math inline">\(Z\)</span> 的某一行向量。</p><p><strong>解码器</strong></p><p>解码器的任务是根据解码器生成的节点嵌入重构某些确定的图形统计信息。</p><p>标准形式为成对的解码器，如下式： <span class="math display">\[DEC:\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}^+\]</span> 成对的解码器可以预测成对出现的节点的关系和相似性。</p><p>在一对节点的嵌入表示 <span class="math inline">\((z_u,z_v)\)</span>中使用成对解码器，能重构节点 <span class="math inline">\(u\)</span> 和<span class="math inline">\(v\)</span>之间的关系。重构的目标在于通过最小化重构损失来优化编码器和解码器，如下式：<span class="math display">\[DEC(ENC(u),ENC(v)) = DEC(z_u,z_v) \approx S[u,v]\]</span> 其中，假定 <span class="math inline">\(S[u,v]\)</span>是基于图的节点间相似性的度量。</p><p><strong>损失函数</strong></p><p>实现重构目标函数的标准操作是在一组训练节点对 <spanclass="math inline">\(\mathcal{D}\)</span> 上最小化经验重构损失 <spanclass="math inline">\(\mathcal{L}\)</span> ，如下式： <spanclass="math display">\[\mathcal{L} = \sum_{(u,v)\in \mathcal{D}}\mathcal{l}(DEC(z_u,z_v),S[u,v])\]</span> 其中，<span class="math inline">\(\mathcal{l}: \mathbb{R}\times \mathbb{R}\)</span> 是衡量解码后的近似值 <spanclass="math inline">\(DEC(z_u,z_v)\)</span> 与真值 <spanclass="math inline">\(S[u,v]\)</span> 之间差异的函数。在训练集 <spanclass="math inline">\(\mathcal{D}\)</span>上，全过程的目标就是通过训练编码器和解码器，有效地重构成对节点的关系信息。</p><h3 id="基于因式分解的方法">基于因式分解的方法</h3><p>节点嵌入表示来解码局部邻居结构与在图的邻接矩阵中重构邻居密切相关，邻接矩阵中重构邻居是使用矩阵分解来学习节点-节点相似性矩阵<span class="math inline">\(S\)</span> 的低维表示，其中， <spanclass="math inline">\(S\)</span>概括了邻接矩阵并且能描述用户定义的节点-节点的相似性概念。</p><p><strong>拉普拉斯特征映射</strong></p><p>使用基于节点嵌入之间的 <span class="math inline">\(L_2\)</span>距离定义解码器，如下式： <span class="math display">\[DEC(z_u,z_v) = ||z_u - z_v||_2^2\]</span>根据节点在图中的相似性对节点对进行加权，以得到最终的损失函数，如下式：<span class="math display">\[\mathcal{L} = \sum_{(u,v) \in \mathcal{D}} DEC(z_u,z_v) \cdot S[u,v]\]</span> <strong>内积法</strong></p><p>基于内积的解码器，如下式： <span class="math display">\[DEC(z_u,z_v) = z_u^Tz_v\]</span>假设两个节点之间的相似度（如这两个节点局部邻域之间重叠的部分）与节点嵌入表示的点积成正比。</p><p>损失函数如下式： <span class="math display">\[\mathcal{L} = \sum_{(u,v) \in \mathcal{D}} ||DEC(Z_u,z_v)-S[u,v]||_2^2\]</span> 将节点嵌入表示 <span class="math inline">\(z_u \in\mathbb{R}^d\)</span> 堆叠为矩阵 <span class="math inline">\(Z \in\mathbb{R}^{|\mathcal{V}| \times d}\)</span>，则可以将目标函数写成下式： <span class="math display">\[\mathcal{L} = ||{ZZ^T}_2^2||\]</span></p><h3 id="随机游走嵌入表示">随机游走嵌入表示</h3><p>随机游走嵌入表示是内积法应用于邻域重构的随机度量计算的方法。</p><p><strong>Deepwalk 和 node2vec</strong></p><p>Deepwalk 和 node2vec 使用了 shallow embedding方法和内积解码器，巧妙的定义相似度和邻域重构的概念，通过优化嵌入表示对随机游走的统计信息进行编码。通过学习节点嵌入使得下式成立：<span class="math display">\[DEC(z_u,z_v) \overset{d}{=} \frac{e^{z_u^Tz_v}}{\sum_{v_k \in\mathcal{V}}e^{z_u^Tz_k}} \approx p_{\mathcal{g},T}(v|u)\]</span> 其中，<spanclass="math inline">\(p_{\mathcal{g},T}(v|u)\)</span> 是指从节点 <spanclass="math inline">\(u\)</span> 出发，随机游走 <spanclass="math inline">\(T\)</span> 步后访问节点 <spanclass="math inline">\(v\)</span> 的概率，<span class="math inline">\(T\in {2,\cdots}, 10\)</span>。</p><p>通过上式的解码器使用下式训练随机游走嵌入： <spanclass="math display">\[\mathcal{L} = \sum_{(u,v) \in \mathcal{D}} -log(DEC(z_u,z_v))\]</span>通过从每一个节点开始进行随机游走采样，可以生成随机游走的训练集，并用<span class="math inline">\(\mathcal{D}\)</span> 表示。</p><p>上式损失函数时间复杂度为 <spanclass="math inline">\(O(|\mathcal{D}||\mathcal{V}|)\)</span> 。Deepwalk使用层次的 softmax函数近似表示解码器，主要通过利用二叉树结构来加速计算，node2vec使用噪声对比方法近似表示损失函数，使用下式所示的负样本近似表示归一化引子：<span class="math display">\[\mathcal{L} = \sum_{(u,v)\in \mathcal{D}} -log(\sigma(z_u^T,z_v)) -\gamma \mathbb{E}_{v_n \simP_n(\mathcal{V})}[log(-\sigma(z_u^T,z_{v_n}))]\]</span> 其中，<span class="math inline">\(\sigma\)</span> 表示 log激活函数， <span class="math inline">\(P_n(\mathcal{V})\)</span>表示节点 <span class="math inline">\(\mathcal{V}\)</span> 的分布，<spanclass="math inline">\(\gamma &gt; 0\)</span> 为超参数。<spanclass="math inline">\(P_n(\mathcal{V})\)</span>往往服从均匀分布，且期望值可采用蒙特卡洛算法近似计算。</p><p>Deepwalk 只简单使用均匀的随机游走来定义分布 <spanclass="math inline">\(p_{\mathcal{g},T}(v|u)\)</span>，而 node2vec方法引入了超参数，这些超参数允许随机游走在于图的广度优先搜索或深度优先搜索更相似的游走的概率之间平滑地插值。</p><p><strong>随机游走与矩阵分解方法</strong></p><p>假设定义下式为节点-节点相似度矩阵的值： <span class="math display">\[S_{DW} = log(\frac{vol(\mathcal{V})}{T}(\sum_{t=1}^TP^t)D^{-1})-log(b)\]</span> 其中 <span class="math inline">\(b\)</span> 为常值，<spanclass="math inline">\(P=D^{-1}A\)</span> 。</p><p>还可以将上式内部的部分表示分解成下式所示形式： <spanclass="math display">\[(\sum_{t=1}^TP^T)D^{-1} =D^{-\frac{1}{2}}(U(\sum_{t=1}^T\Lambda^t)U^T)D^{-\frac{1}{2}}\]</span> 其中，<span class="math inline">\(U\LambdaU^T=L_{sym}\)</span> 是对称归一化拉普拉斯算子的本征分解。</p><p>裘捷中等提出通过 DeepWalk 学习到的嵌入 <spanclass="math inline">\(Z\)</span> 需满足下式要求： <spanclass="math display">\[Z^TZ \approx S_{DW}\]</span> <strong>shallow embedding 的局限性</strong></p><ul><li><p>shallow embedding方法不会在编码器的节点之间共享任何参数，因为编码器会直接为每个节点优化唯一的嵌入向量，不进行参数共享会导致算法在统计和计算过程中效率低下。</p></li><li><p>shallow embedding 方法没有利用编码器中的节点特征。</p></li><li><p>shallow embedding方法是转导性（Transductive）的，只能为训练过程中存在的节点生成嵌入，无法为训练之后出现的新节点生成嵌入。</p></li></ul><h2 id="多关系图节点嵌入">多关系图节点嵌入</h2><p>设多关系图为 <span class="math inline">\(\mathcal{G} = (\mathcal{V},\varepsilon)\)</span>，其中边被定义为元组 <spanclass="math inline">\(e=(u,\tau,v)\)</span> 中两个节点间存在的特定关系<span class="math inline">\(\tau \in \mathcal{T}\)</span>。</p><p>可以将嵌入多关系图视作重构任务，即给定两个节点的嵌入表示 <spanclass="math inline">\(z_u\)</span> 和 <spanclass="math inline">\(z_v\)</span>，重构这些节点之间的关系。将解码器定义为可同时输入一对节点嵌入及一个关系类型（<spanclass="math inline">\(DEC: \mathbb{R}^d \times \mathcal{R} \times\mathbb{R}^d \rightarrow \mathbb{R}+\)</span>），将解码器的输出（<spanclass="math inline">\(DEC(z_u,\tau,z_v)\)</span>）定义为边 <spanclass="math inline">\((u,\tau,v)\)</span> 存在于图谱的可能性。</p><p>本节所有方法都是假定从低维向量中直接重构（多关系）邻居。</p><h3 id="损失函数">损失函数</h3><p><strong>负采样的交叉熵函数</strong> <span class="math display">\[\mathcal{L} = \sum_{(u,\tau,v) \in \varepsilon}-log(\sigma(DEC(z_u,\tau,z_v))) - \gamma\mathbb{E}_{v_n \simP_{n,u}(\mathcal{V})}[log(\sigma(-DEC(z_u,\tau,z_{v_n})))])\]</span> 其中，<span class="math inline">\(\sigma\)</span>为对数函数，<span class="math inline">\(P_{n,u}(\mathcal{V})\)</span>为节点集 <span class="math inline">\(\mathcal{V}\)</span>的负采样分布，且超参数 <span class="math inline">\(\gamma &gt;0\)</span> 。</p><p>实际存在于图谱中的、预测为 true 的边的对数似然如下式： <spanclass="math display">\[log(\sigma(DEC(z_u,\tau,z_v)))\]</span> 图中不存在的、预测为 false 的边的期望对数似然如下式： <spanclass="math display">\[\mathbb{E}_{v_n \simP_{n,u}(\mathcal{V})}[log(\sigma(-DEC(z_u,\tau,z_{v_n})))]\]</span> 使用蒙特卡洛近似法可以对期望值进行评估，常见损失函数如下式：<span class="math display">\[\mathcal{L} = \sum_{(u,\tau,v)\in \varepsilon}-log(\sigma(DEC(z_u,\tau,z_v))) - \sum_{v_n \in \mathcal{P}_{n,u}}-[log(\sigma(-DEC(z_u,\tau,z_{v_n})))]\]</span> 其中，<span class="math inline">\(\mathcal{P}_{n,u}\)</span>为从 <span class="math inline">\(P_{n,u}(\mathcal{V})\)</span>采样得到的较小节点集。</p><p><strong>最大间距损失</strong> <span class="math display">\[\mathcal{L} = \sum_{(u,\tau,v)\in \varepsilon} \sum_{v_n \in\mathcal{P}_{n,u}} max(0, -DEC(z_u,\tau,z_v) + DEC(z_u,\tau,z_{v_n}) +\Delta)\]</span>该损失函数采用对比估计策略，将真实节点解码后的得分与负样本进行对比。其中，<spanclass="math inline">\(\Delta\)</span>项为间隔项，如果所有样本的得分差距都很大，那损失将为 0，该损失函数称为hinge loss。</p><h3 id="多关系解码器">多关系解码器</h3><p><strong>RESCAL 方法</strong></p><p>解码器定义为如下式： <span class="math display">\[DEC(u,\tau,v) = z_u^TR_{\tau}z_v\]</span> 其中，<span class="math inline">\(R_{\tau} \in \mathbb{E}^{d\times d}\)</span> 是基于特定关系 <span class="math inline">\(\tau \in\mathcal{R}\)</span> 的可学习矩阵。可借助基本的重构损失函数训练嵌入矩阵<span class="math inline">\(Z\)</span> 和关系矩阵 <spanclass="math inline">\(R_{\tau}, \forall \tau \in\mathcal{R}\)</span>，如下式： <span class="math display">\[\begin{aligned}  \mathcal{L} &amp;= \sum_{u\in \mathcal{V}} \sum_{v\in \mathcal{V}}\sum_{\tau\in \mathcal{R}} ||DEC(u,\tau,v)-\mathcal{A}[u,\tau,v||^2 \\  &amp;= \sum_{u\in \mathcal{V}} \sum_{v\in \mathcal{V}} \sum_{\tau\in\mathcal{R}} ||z_u^T\mathcal{R}_{\tau},z_v - \mathcal{A}[u,\tau,v]||\end{aligned}\]</span> 其中，<span class="math inline">\(\mathcal{A} \in\mathbb{R}^{|\mathcal{V}| \times |\mathcal{R}| \times|\mathcal{V}|}\)</span> 为多关系图的邻接张量。</p><p>该方法进行关系表示时需要较高的计算量和统计成本，对于每种类型的关系，RESCAL模型具有 <span class="math inline">\(O(d^2)\)</span>的参数量，与节点表示相比，关系表示要求具有更大数量级的参数。</p><p><strong>平移解码模型（Translational Decoders）</strong></p><p>TransE 模型将关系作为嵌入空间中的平移向量，定义的解码器如下图所示：<span class="math display">\[DEC(z_u,\tau,z_v) = -|| z_u + r_{\tau} +z_v ||\]</span> 其中，使用 <span class="math inline">\(d\)</span>维嵌入向量来表示每种关系。在嵌入空间中，根据关系嵌入对头节点进行平移，边存在的可能性与头节点和尾节点嵌入间的距离成比例。</p><p>改进模型 1 <span class="math display">\[DEC(z_u,\tau,z_v) = -|| g_{1,\tau}(z_u) + r_{\tau} - g_{2,\tau}(z_v)||\]</span> 其中，<span class="math inline">\(g_{i,\tau}\)</span>为一种基于关系 <span class="math inline">\(\tau\)</span>空间的可训练转换方式。</p><p>改进模型 2 TransH <span class="math display">\[DEC(z_u,\tau,z_v) = -|| (z_u - w_r^Tz_uw_r) + r_{\tau} - (z_u -w_r^Tz_vw_r) ||\]</span> TransH模型可将实体嵌入映射到一个可学习的特定关系超平面上（在执行转换之前，由法线向量<span class="math inline">\(w_r\)</span> 定义）。</p><p><strong>多段点积（Multi-Linear Dot Products）</strong></p><p>多段点积方法通过从简单图中扩展点积解码器来研究多关系解码模型，解码器如下式所示：<span class="math display">\[DEC(z_u, \tau, z_v) = &lt;z_u,r_{\tau},z_v&gt; = \sum_{i=1}^d z_u[i]\times r_{\tau}[i] \times z_v[i]\]</span>该方法直接计算解码器中三个向量表示的点积。该解码器只能编码对称的关系，满足下式：<span class="math display">\[\begin{aligned}DEC(z_u, \tau, z_v) &amp;= &lt;z_u,r_{\tau},z_v&gt; \\&amp;= \sum_{i=1}^d z_u[i] \times r_{\tau}[i] \times z_v[i] \\&amp;= &lt;z_v,r_{\tau},z_u&gt; \\&amp;= DEC(z_v, \tau, z_u)\end{aligned}\]</span> <strong>复解码器</strong></p><p>复解码器能够编码有向且不对称的关系。</p><p>ComplEx 通过引入复值嵌入表示来扩展 DistMult 解码器，定义为下式：<span class="math display">\[\begin{aligned}DEC(z_u,\tau,z_v) &amp;= Re&lt;z_u,r_{\tau},\bar{z}_v&gt; \\&amp;= Re(\sum_{i=1}^d z_u[i] \times r_{\tau}[i] \times z_v[i])\end{aligned}\]</span> 其中，<span class="math inline">\(z_u,z_v,r_{\tau} \in\mathbb{C}^d\)</span> 为复值嵌入，Re表示复值向量的实部。由于采用了尾部嵌入表示的复共轭 <spanclass="math inline">\(\bar{z}_v\)</span>，因此这种解码方法适用于非对称关系。</p><p>RotateE 方法主要将解码过程定义为嵌入表示在复平面上的旋转，如下式：<span class="math display">\[DEC(z_u,\tau,z_v) = || Z_u \circ r_{\tau} - z_v ||\]</span> 其中，<span class="math inline">\(\circ\)</span>为哈达玛乘积。假设上式中所有嵌入表示都是复值，且使 <spanclass="math inline">\(|r_{\tau}[i]|=1,\forall i \in{1,\cdots,d}\)</span>，则关系嵌入的每一维向量能表示成 <spanclass="math inline">\(r_{\tau}[i]=e^{i\theta_{r,i}}\)</span>，其对应关系向量在复平面内的旋转。</p><h2 id="解码器的性能表征">解码器的性能表征</h2><p><strong>对称性与非对称性</strong></p><p>满足下式的关系具有对称性： <span class="math display">\[(u,\tau,v) \in \varepsilon \leftrightarrow (v,\tau,u) \in \varepsilon\]</span> 满足下式的关系具有非对称性： <span class="math display">\[(u,\tau,v) \in \varepsilon \leftrightarrow (v,\tau,u) \notin \varepsilon\]</span> DistMult 仅能建模对称关系。</p><p>TransE 解码器仅能建模非对称关系，如下式所示： <spanclass="math display">\[\begin{aligned}DEC(Z_u, \tau, z_v) &amp;= DEC(z_v,\tau,z_u) \\-|| z_u + r_{\tau} -z_v || &amp;= -|| z_v + r_{\tau} -z_u || \\\Longrightarrow -r_{\tau} &amp;= r_{\tau} \\\Longrightarrow r_{\tau} &amp;= 0\end{aligned}\]</span> <strong>互逆性</strong></p><p>互逆性是指一种关系的存在暗含这另一种相反方向关系的存在，如下式：<span class="math display">\[(u,\tau_1,v) \leftrightarrow (v,\tau_2,u) \in \varepsilon\]</span> DisMult解码器无法对这种关心模式建模，但大多数其它类型解码器都能够表示逆关系。</p><p><strong>组合性</strong></p><p>满足下式的关系具有组合性： <span class="math display">\[(u,\tau_1,v) \in \varepsilon \wedge (v,\tau_2,u) \in \varepsilon\rightarrow (u,\tau_3,v) \in \varepsilon\]</span> 在 TransE 模型中，当 <span class="math inline">\(r_{\tau_3} =r_{\tau_1} + r_{\tau_2}\)</span>，满足上述组合性。在 RESCAL中可以通过定义 <span class="math inline">\(R_{\tau_3} =R_{\tau_1}R_{\tau_2}\)</span> 满足上述组合性。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（二）柳永篇</title>
    <link href="/2023/05/28/song-ci-er-liu-yong-pian/"/>
    <url>/2023/05/28/song-ci-er-liu-yong-pian/</url>
    
    <content type="html"><![CDATA[<center>曲玉管</center><center>陇首云飞，江边日晚，烟波满目凭栏久。一望关河萧索，千里清秋，忍凝眸。</center><center>杳杳神京，盈盈仙子，别来锦字终难偶。断雁无凭，冉冉飞下汀州，思悠悠。</center><center>暗想当初，有多少，幽欢佳会；岂知聚散难期，翻成雨恨云愁。</center><center>阻追游，每登山临水，惹起平生心事，一场消黯，永日无言，却下层楼。</center><p><br></p><center>雨霖铃</center><center>寒蝉凄切，对长亭晚，骤雨初歇。都门帐饮无绪，留恋处，兰舟催发。执手相看泪眼，竟无语凝噎。念去去，千里烟波，暮</center><p>霭沉沉楚天阔。</p><center>多情自古伤离别，更那堪、冷落清秋节！今宵酒醒何处？杨柳岸、晓风残月。此去经年，应是良辰好景虚设。便纵有千种风</center><p>情，更与何人说？</p><p><br></p><center>蝶恋花</center><center>伫倚危楼风细细，望极春愁，黯黯生天际。草色烟光残照里，无言谁会凭栏意？</center><center>拟把疏狂图一醉，对酒当歌，强乐还无味。衣带渐宽终不悔，为伊消得人憔悴。</center><p><br></p><center>采莲令</center><center>月华收，云淡霜天曙。西征客、此时情苦。翠娥执手，送临歧、轧轧开朱户。千娇面、盈盈伫立，无言有泪，断肠争忍回顾？</center><center>一叶兰舟，便恁急桨凌波去。贪行色、岂知离绪，万般方寸，但饮恨、脉脉同谁语？更回首、重城不见，寒江天外，隐隐两三烟树。</center>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（一）苏轼篇</title>
    <link href="/2023/05/28/song-ci-yi-su-shi-pian/"/>
    <url>/2023/05/28/song-ci-yi-su-shi-pian/</url>
    
    <content type="html"><![CDATA[<center>水调歌头</center><p>丙辰中秋，欢饮达旦，大醉，作此篇兼怀子由。</p><center>明月几时有，把酒问青天。不知天上宫阙，今夕是何年。我欲乘风归去，惟恐琼楼玉宇，高处不胜寒。起舞弄清影，何似在</center><p>人间。</p><center>转朱阁，低绮户，照无眠。不应有恨，何事长向别时圆？人有悲欢离合，月有阴晴圆缺。此时古难全。但愿人长久，千里共</center><p>婵娟。</p>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（二）传统机器学习方法</title>
    <link href="/2023/05/28/tu-shen-jing-wang-luo-xue-xi-ri-ji-er-chuan-tong-ji-qi-xue-xi-fang-fa/"/>
    <url>/2023/05/28/tu-shen-jing-wang-luo-xue-xi-ri-ji-er-chuan-tong-ji-qi-xue-xi-fang-fa/</url>
    
    <content type="html"><![CDATA[<p><strong>标准机器学习范式</strong></p><p>首先，基于启发式函数或领域知识提取一些统计特征；然后将其作为标准机器学习分类器（如逻辑回归）的输入。</p><h2 id="图统计特征">图统计特征</h2><h3 id="节点层面的统计特征">节点层面的统计特征</h3><p><strong>节点的度</strong></p><p>节点<spanclass="math inline">\(u\)</span>的度反映与这个节点相连接的边的数目，可由下式表示<span class="math display">\[d_u = \sum_{v \in \mathcal{V}} A[u,v]\]</span> 对于有向图和加权图，度分为入度和出度。对矩阵<spanclass="math inline">\(A\)</span>中的节点<spanclass="math inline">\(u\)</span>的行求和可以得到出度，对节点<spanclass="math inline">\(u\)</span>的列求和可以得到入度。</p><p><strong>节点的中心性</strong></p><p>角度一：特征向量中心性度量不仅考虑邻居节点个数的度，还考虑了邻居节点的重要性。<span class="math display">\[e_u = \frac{1}{\lambda}\sum_{v\in \mathcal{V}}A[u,v]e_v, \forall u \in\mathcal{V}\]</span> 将向量表示<spanclass="math inline">\(e\)</span>代入上述等式取代节点中心性向量可得到邻接矩阵的标准特征向量方程：<span class="math display">\[\lambda e = Ae\]</span>角度二：特征向量中心性衡量了一个节点在路径无限长的情况下在随机游走时被访问的概率。这种理解方式连接了节点重要性、随机游走和谱三个重要概念。<spanclass="math inline">\(\lambda\)</span>是<spanclass="math inline">\(A\)</span>的主要特征向量，可通过幂次迭代法则计算<spanclass="math inline">\(e\)</span>如下所示： <span class="math display">\[e^{(t+1)} = Ae^{(t)}\]</span> 从向量<spanclass="math inline">\(e^{(0)}=(1,1,\cdots,1)^T\)</span>开始，依据幂次迭代法则，第一次迭代可以得到每个节点的度，在第<spanclass="math inline">\(t\)</span>次迭代<span class="math inline">\((t\geq 1)\)</span>时，<spanclass="math inline">\(e^{(t)}\)</span>包括了尝试到达每个节点且长度为<spanclass="math inline">\(t\)</span>的路线的长度的数量，无限重复下去可得到路径无限长时每个节点被访问的次数。</p><p><strong>聚类系数</strong></p><p>聚类系数通过一个节点的局部邻域中闭合三角形的比例度量节点的邻居节点聚类的紧密程度。聚类系数计算方法Local Variant 方法如下式所示： <span class="math display">\[c_u = \frac{|(v_1, v_2)\in \varepsilon: v_1, v_2 \in\mathcal{N}(u)|}{\binom{d_u}{2}}\]</span> 其中<span class="math inline">\(\mathcal{N}(u)=\{v\in\mathcal{V}: (u,v)\in \varepsilon\}\)</span>表示节点<spanclass="math inline">\(u\)</span>的邻居节点。</p><p><strong>闭合三角形、自我中心图、Motifs</strong></p><p><strong>闭合三角形</strong></p><h3 id="图层面的统计特征">图层面的统计特征</h3><p>节点袋</p><p>Weisfeiler-Lehman 核</p><p>Graphlets 和基于路径的方法</p><h2 id="邻域重叠检测">邻域重叠检测</h2><h2 id="图的拉普拉斯矩阵和图的谱方法">图的拉普拉斯矩阵和图的谱方法</h2>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex 字母类符号</title>
    <link href="/2023/05/24/latex-zi-mu-lei-fu-hao/"/>
    <url>/2023/05/24/latex-zi-mu-lei-fu-hao/</url>
    
    <content type="html"><![CDATA[<p>Caligraphic letters: <code>$\mathcal&#123;A&#125;$</code> etc.: <spanclass="math inline">\(\mathcal{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Mathbb letters: <code>$\mathbb&#123;A&#125;$</code>etc.: <spanclass="math inline">\(\mathbb{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Mathfrak letters: <code>$\mathfrak&#123;A&#125;$</code>etc.: <spanclass="math inline">\(\mathfrak{A B C D E F G H I J K L M N O P Q I S TU V W X Y Z}\)</span></p><p>Math Sans serif letters: <code>$\mathsf&#123;A&#125;$</code>etc.: <spanclass="math inline">\(\mathsf{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Math bold letters: <code>$\mathbf&#123;A&#125;$</code>etc.: <spanclass="math inline">\(\mathbf{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Math bold italic letters: define<code>\def\mathbi#1&#123;\textbf&#123;\em #1&#125;&#125;</code> then use<code>$\mathbi&#123;A&#125;$</code></p>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（一）图基础知识</title>
    <link href="/2023/05/24/tu-shen-jing-wang-luo-xue-xi-ri-ji-yi-tu-ji-chu-zhi-shi/"/>
    <url>/2023/05/24/tu-shen-jing-wang-luo-xue-xi-ri-ji-yi-tu-ji-chu-zhi-shi/</url>
    
    <content type="html"><![CDATA[<h2 id="图的定义">图的定义</h2><p>图由节点集合<spanclass="math inline">\(\mathcal{V}\)</span>和边集合<spanclass="math inline">\(\varepsilon\)</span>组成，记作<spanclass="math inline">\(\mathcal{G}=(\mathcal{V},\varepsilon)\)</span>，节点<spanclass="math inline">\(u\in \mathcal{V}\)</span>到节点<spanclass="math inline">\(v\in \mathcal{V}\)</span>的边表示为<spanclass="math inline">\((u,v)\in\varepsilon\)</span>.</p><h2 id="图的表示">图的表示</h2><p>图可以由邻接矩阵<span class="math inline">\(A\in\mathbb{R}^{|\mathcal{V}|\times|\mathcal{V}|}\)</span>表示，矩阵的行和列代表节点索引，矩阵元素<spanclass="math inline">\(A[u,v]\)</span>表示节点<spanclass="math inline">\(u\)</span>和节点<spanclass="math inline">\(v\)</span>的连接情况，如果<spanclass="math inline">\((u,v)\in \varepsilon\)</span>，则<spanclass="math inline">\(A[u,v]=1\)</span>，否则<spanclass="math inline">\(A[u,v]=0\)</span>。</p><h2 id="多关系图的分类">多关系图的分类</h2><p><strong>异构图</strong></p><p>异构图可以通过节点类型将节点划分为不相交的子集，即<spanclass="math inline">\(\mathcal{V} = \mathcal{V}_1 \bigcup \mathcal{V}_2\bigcup \cdots \bigcup \mathcal{V}_k\)</span>，其中<spanclass="math inline">\(\mathcal{V}_i \bigcap \mathcal{V}_j = \emptyset,\forall \neq j\)</span>。</p><p><strong>多重图</strong></p><p>多重图中的边只能连接不同类型的节点，即<spanclass="math inline">\((u,\tau_i,v)\in \varepsilon \rightarrow u \in\mathcal{V}_j,v \in \mathcal{V}_k \bigwedge j \neqk\)</span>。在多重图中，通常假设图可以被分解为<spanclass="math inline">\(k\)</span>个层级，每个节点可以属于一层或多层，每层代表唯一特定关系，表示本层内边的类型。</p><h2 id="图机器学习任务">图机器学习任务</h2><h3 id="节点预测">节点预测</h3><p>提供训练集真实的标签<span class="math inline">\(\mathcal{V}_{train}\subset \mathcal{V}\)</span>时，通过所有的节点<spanclass="math inline">\(u \in \mathcal{V}\)</span>预测标签<spanclass="math inline">\(y_u\)</span>应该属于哪种类型、类别或属性。</p><p>节点预测可通过显式利用节点之间的连接进行分类，如下几种节点之间的连接性质：</p><p><strong>同质性</strong></p><p>图中的节点与其邻居节点的属性相似，即节点有与邻居节点共享属性的趋势。</p><p><strong>异质性</strong></p><p>假定节点将优先连接到具有不同标签的节点。</p><p><strong>结构等价性</strong></p><p>具有相似局部结构的节点将具有相似的标签。</p><p><strong>节点预测是有监督还是半监督任务？</strong></p><p><strong>非标准的半监督任务。</strong></p><p>在半监督学习中，模型训练过程同时使用有标签数据和无标签数据，标准的半监督学习以独立同分布假设为前提，标准的监督学习在训练过程中不使用所有无标签的测试数据。节点分类任务中，图中节点全部都被使用，包括无标签节点，故节点分类任务是半监督学习，同时节点分类任务对一组相互连接的节点进行建模，打破了独立同分布假设，故节点分类是非标准的半监督任务。</p><h3 id="关系预测">关系预测</h3><p>给定一组节点<spanclass="math inline">\(\mathcal{V}\)</span>和部分边的集合<spanclass="math inline">\(\varepsilon_{train}\)</span>（<spanclass="math inline">\(\varepsilon_{train} \subset \varepsilon,\varepsilon\)</span>表示全体边的集合），利用这些给定信息推断缺失边的集合<spanclass="math inline">\((\varepsilon\)</span>  $ _{train})$。</p><h3 id="社区发现">社区发现</h3><p>通过输入一张图<spanclass="math inline">\(\mathcal{G}=(\mathcal{V},\varepsilon)\)</span>推断出潜在的社区结构。</p><p>社区发现常被类比为图领域的无监督学习中的聚类任务。</p><h3 id="图预测">图预测</h3><p>图预测包括对整张图进行分类、回归与聚类。</p><p>图分类或图回归任务中，数据集由多张不同图构成，图机器学习算法针对每张图进行独立预测，而不是预测图的组成部分。在图聚类任务中，目标是学习一个无监督的测量图与图之间相似性的策略。</p><p>图分类与图回归任务属于标准监督学习范畴。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
