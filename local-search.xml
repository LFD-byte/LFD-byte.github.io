<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>图神经网络学习日记（七）图神经网络（GNN）的实现</title>
    <link href="/2023/06/20/tu-shen-jing-wang-luo-xue-xi-ri-ji-qi-tu-shen-jing-wang-luo-gnn-de-shi-xian/"/>
    <url>/2023/06/20/tu-shen-jing-wang-luo-xue-xi-ri-ji-qi-tu-shen-jing-wang-luo-gnn-de-shi-xian/</url>
    
    <content type="html"><![CDATA[<h2 id="损失函数">损失函数</h2><p><strong>用于节点分类的 GNN</strong></p><p>以完全监督方式训练 GNN，使用 softmax分类函数和负对数似然损失来定义损失函数，如下式：</p><p><span class="math display">\[\mathcal{L} = \sum_{u \in \mathcal{V}_{train}} = -log(softmax(z_u,y_u))\]</span></p><p>其中，假设 <span class="math inline">\(y_u \in \mathbb{Z}^c\)</span>是一个独热向量，表示用于训练的节点 <span class="math inline">\(u \in\mathcal{V}_{train}\)</span> 的类。</p><p>在引用网络中，<span class="math inline">\(y_u\)</span>表示论文的主题，<span class="math inline">\(Softmax(z_u,y_u)\)</span>表示通过 softmax 函数计算节点属于类 <span class="math inline">\(y_u\)</span> 的概率，如下式：</p><p><span class="math display">\[softmax(z_u,y_u) = \sum_{i=1}^c y_u[i]\frac{e^{z_u^Tw_i}}{\sum_{j=1}^ce^{z_u^Tw_j}}\]</span></p><p>其中，<span class="math inline">\(w_i \in\mathbb{R}^d,i=1,\cdots,c\)</span> 是可训练的参数。</p><p><strong>用于图分类的 GNN</strong></p><p>图分类的损失函数值是通过一组有标记的训练图 <span class="math inline">\(\mathcal{T} ={\mathcal{G}_1,\cdots,\mathcal{G}_n}\)</span> 上学习的图嵌入 <span class="math inline">\(z_{\mathcal{G}_i}\)</span>计算，通常使用如下式定义的平方误差损失函数：</p><p><span class="math display">\[\mathcal{L} = \sum_{\mathcal{G} \in \mathcal{T}}\|MLP(z_{\mathcal{G}_i}) - y_{\mathcal{G}_i} \|_2^2\]</span></p><p>其中，MLP 是具有单一变量输出的密集连接的神经网络，<span class="math inline">\(y_{\mathcal{G}_i} \in \mathbb{R}\)</span> 是训练图<span class="math inline">\(\mathcal{G}_i\)</span> 的标签值。</p><p><strong>用于关系预测的 GNN</strong></p><p>深度图信息最大化（DGI）节点嵌入 <span class="math inline">\(z_u\)</span> 和图嵌入 <span class="math inline">\(z_{\mathcal{G}}\)</span>之间的互信息，损失函数如下式：</p><p><span class="math display">\[\mathcal{L} = -\sum_{u \in \mathcal{V}_{train}} \mathbb{E}_{\mathcal{G}}log(D(z_u,z_{\mathcal{G}})) +\gamma\mathbb{E}log(1-D(\tilde{z}_u,z_{\mathcal{G}}))\]</span></p><p>其中，<span class="math inline">\(z_u\)</span> 表示 GNN 根据图 <span class="math inline">\(\mathcal{G}\)</span> 生成的节点 <span class="math inline">\(u\)</span> 的嵌入，而 <span class="math inline">\(\tilde{z}_u\)</span> 表示根据 <span class="math inline">\(\mathcal{G}\)</span> 的损坏版本 <span class="math inline">\(\tilde{\mathcal{G}}\)</span> 生成的节点 <span class="math inline">\(u\)</span> 的嵌入。这里用 <span class="math inline">\(D\)</span>表示判别函数，它是一个被训练用以预测节点嵌入是基于真实的图 <span class="math inline">\(\mathcal{G}\)</span> 还是损坏版本的图 <span class="math inline">\(\tilde{\mathcal{G}}\)</span>生成的。在通常情况下，通过一种随机的方式（如打乱特征矩阵中的元素）修改节点特征或邻接矩阵，抑或是同时修改两者来破坏图。上述损失函数背后的思想是：GNN模型必须学会生成可以区分真是图和其损坏版本的节点嵌入。这一优化目标与最大化节点嵌入<span class="math inline">\(z_u\)</span> 和图嵌入 <span class="math inline">\(z_{\mathcal{G}}\)</span>之间的互信息密切相关。</p><h2 id="节点采样">节点采样</h2><p>基于节点级别消息传递的角度直接实现 GNN可能存在计算效率低的问题。例如，当多个节点共享邻域时，如果图中所有节点独立执行消息传递操作，最终可能会执行大量冗余计算。</p><p><strong>图级别的实现方法</strong></p><p><strong>子采样和小批量</strong></p><p>为了限制 GNN的内存占用并促进小批量训练方式，可以在消息传递过程中使用节点集的子集。从数学角度，可以认为这是在每个批次中为图中节点的子集运行节点级的GNN 公式。</p><p>挑战在于不能在不丢失信息的情况下简单地在图上的一部分节点中执行消息传递操作，每次删除节点时也会删除其关联的边，这无法保证选择的节点的随机子集会构成连接图，并且为每个小批次选择一个节点的随机自己会对模型性能产生严重的不利影响。</p><p>通过对节点邻域进行子采样的策略来克服此问题：首先为每个批次选择一组目标节点，然后递归采样这些节点的邻域以确保能够保持图的连通性。为了尽可能避免为一个批次采样过多节点的情况，建议使用固定容量对每个节点的邻域进行子采样以提高批量张量操作的效率。</p><h2 id="参数共享和正则化">参数共享和正则化</h2><p><strong>层间参数共享</strong></p><p>在 GNN 的所有聚合和更新函数中使用相同的函数。通常情况下，在6层以上的GNN 中最有效，并且通常与门控更新函数结合使用。</p><p><strong>边丢弃</strong></p><p>训练过程中随机删除（或屏蔽）邻接矩阵的边。直观来看，这将使 GNN不太容易过拟合，并且对邻接矩阵中的噪声更具鲁棒性。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（六）图神经网络（GNN）的实现</title>
    <link href="/2023/06/20/tu-shen-jing-wang-luo-xue-xi-ri-ji-liu-tu-shen-jing-wang-luo-gnn-de-shi-xian/"/>
    <url>/2023/06/20/tu-shen-jing-wang-luo-xue-xi-ri-ji-liu-tu-shen-jing-wang-luo-gnn-de-shi-xian/</url>
    
    <content type="html"><![CDATA[<h2 id="损失函数">损失函数</h2><p><strong>用于节点分类的 GNN</strong></p><p>以完全监督方式训练 GNN，使用 softmax分类函数和负对数似然损失来定义损失函数，如下式：</p><p><span class="math display">\[\mathcal{L} = \sum_{u \in \mathcal{V}_{train}} = -log(softmax(z_u,y_u))\]</span></p><p>其中，假设 <span class="math inline">\(y_u \in \mathbb{Z}^c\)</span>是一个独热向量，表示用于训练的节点 <span class="math inline">\(u \in\mathcal{V}_{train}\)</span> 的类。</p><p>在引用网络中，<span class="math inline">\(y_u\)</span>表示论文的主题，<span class="math inline">\(Softmax(z_u,y_u)\)</span>表示通过 softmax 函数计算节点属于类 <span class="math inline">\(y_u\)</span> 的概率，如下式：</p><p><span class="math display">\[softmax(z_u,y_u) = \sum_{i=1}^c y_u[i]\frac{e^{z_u^Tw_i}}{\sum_{j=1}^ce^{z_u^Tw_j}}\]</span></p><p>其中，<span class="math inline">\(w_i \in\mathbb{R}^d,i=1,\cdots,c\)</span> 是可训练的参数。</p><p><strong>用于图分类的 GNN</strong></p><p>图分类的损失函数值是通过一组有标记的训练图 <span class="math inline">\(\mathcal{T} ={\mathcal{G}_1,\cdots,\mathcal{G}_n}\)</span> 上学习的图嵌入 <span class="math inline">\(z_{\mathcal{G}_i}\)</span>计算，通常使用如下式定义的平方误差损失函数：</p><p><span class="math display">\[\mathcal{L} = \sum_{\mathcal{G} \in \mathcal{T}}\|MLP(z_{\mathcal{G}_i}) - y_{\mathcal{G}_i} \|_2^2\]</span></p><p>其中，MLP 是具有单一变量输出的密集连接的神经网络，<span class="math inline">\(y_{\mathcal{G}_i} \in \mathbb{R}\)</span> 是训练图<span class="math inline">\(\mathcal{G}_i\)</span> 的标签值。</p><p><strong>用于关系预测的 GNN</strong></p><p>深度图信息最大化（DGI）节点嵌入 <span class="math inline">\(z_u\)</span> 和图嵌入 <span class="math inline">\(z_{\mathcal{G}}\)</span>之间的互信息，损失函数如下式：</p><p><span class="math display">\[\mathcal{L} = -\sum_{u \in \mathcal{V}_{train}} \mathbb{E}_{\mathcal{G}}log(D(z_u,z_{\mathcal{G}})) +\gamma\mathbb{E}log(1-D(\tilde{z}_u,z_{\mathcal{G}}))\]</span></p><p>其中，<span class="math inline">\(z_u\)</span> 表示 GNN 根据图 <span class="math inline">\(\mathcal{G}\)</span> 生成的节点 <span class="math inline">\(u\)</span> 的嵌入，而 <span class="math inline">\(\tilde{z}_u\)</span> 表示根据 <span class="math inline">\(\mathcal{G}\)</span> 的损坏版本 <span class="math inline">\(\tilde{\mathcal{G}}\)</span> 生成的节点 <span class="math inline">\(u\)</span> 的嵌入。这里用 <span class="math inline">\(D\)</span>表示判别函数，它是一个被训练用以预测节点嵌入是基于真实的图 <span class="math inline">\(\mathcal{G}\)</span> 还是损坏版本的图 <span class="math inline">\(\tilde{\mathcal{G}}\)</span>生成的。在通常情况下，通过一种随机的方式（如打乱特征矩阵中的元素）修改节点特征或邻接矩阵，抑或是同时修改两者来破坏图。上述损失函数背后的思想是：GNN模型必须学会生成可以区分真是图和其损坏版本的节点嵌入。这一优化目标与最大化节点嵌入<span class="math inline">\(z_u\)</span> 和图嵌入 <span class="math inline">\(z_{\mathcal{G}}\)</span>之间的互信息密切相关。</p><h2 id="节点采样">节点采样</h2><p>基于节点级别消息传递的角度直接实现 GNN可能存在计算效率低的问题。例如，当多个节点共享邻域时，如果图中所有节点独立执行消息传递操作，最终可能会执行大量冗余计算。</p><p><strong>图级别的实现方法</strong></p><p><strong>子采样和小批量</strong></p><p>为了限制 GNN的内存占用并促进小批量训练方式，可以在消息传递过程中使用节点集的子集。从数学角度，可以认为这是在每个批次中为图中节点的子集运行节点级的GNN 公式。</p><p>挑战在于不能在不丢失信息的情况下简单地在图上的一部分节点中执行消息传递操作，每次删除节点时也会删除其关联的边，这无法保证选择的节点的随机子集会构成连接图，并且为每个小批次选择一个节点的随机自己会对模型性能产生严重的不利影响。</p><p>通过对节点邻域进行子采样的策略来克服此问题：首先为每个批次选择一组目标节点，然后递归采样这些节点的邻域以确保能够保持图的连通性。为了尽可能避免为一个批次采样过多节点的情况，建议使用固定容量对每个节点的邻域进行子采样以提高批量张量操作的效率。</p><h2 id="参数共享和正则化">参数共享和正则化</h2><p><strong>层间参数共享</strong></p><p>在 GNN 的所有聚合和更新函数中使用相同的函数。通常情况下，在6层以上的GNN 中最有效，并且通常与门控更新函数结合使用。</p><p><strong>边丢弃</strong></p><p>训练过程中随机删除（或屏蔽）邻接矩阵的边。直观来看，这将使 GNN不太容易过拟合，并且对邻接矩阵中的噪声更具鲁棒性。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率论（一） 贝叶斯公式</title>
    <link href="/2023/06/16/gai-lu-lun-yi-bei-xie-si-ding-li/"/>
    <url>/2023/06/16/gai-lu-lun-yi-bei-xie-si-ding-li/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>概率</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WandB 学习日记（一）Tutorials</title>
    <link href="/2023/06/12/wandb-xue-xi-ri-ji-yi-doc/"/>
    <url>/2023/06/12/wandb-xue-xi-ri-ji-yi-doc/</url>
    
    <content type="html"><![CDATA[<h2 id="track-experiments">Track experiments</h2><p>快速实验是机器学习的基础。在本教程中，我们使用 W&amp;B来跟踪和可视化实验，以便我们可以快速迭代和理解我们的结果。</p><h3 id="a-shared-dashboard-for-your-experiments">A shared dashboard foryour experiments</h3><p>只需几行代码，您就可以获得丰富的、交互式的、可共享的仪表板，您可以在这里看到<a href="https://wandb.ai/wandb/wandb_example?_gl=1*1ycseye*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MDE0OC41MC4wLjA.">自己的dashboard</a>。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedPell4Oo.png" alt="dashboard"><figcaption aria-hidden="true">dashboard</figcaption></figure><h3 id="data-privacy">Data &amp; Privacy</h3><p>我们非常重视安全性，我们的云托管 dashboard使用行业标准最佳加密实践。如果您正在使用无法离开企业集群的数据集，我们可以提供<a href="https://docs.wandb.com/self-hosted">本地安装</a>。</p><p>下载所有数据并将其导出到其他工具也很容易——例如在 Jupyter笔记本中进行自定义分析。下面是关于我们 <a href="https://docs.wandb.com/library/api">API</a> 的更多信息。</p><h3 id="install-wandb-library-and-login">Install <code>wandb</code>library and login</h3><p>首先安装库并登录到您的免费帐户。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">!pip install wandb -qU<br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-comment"># Log in to your W&amp;B account</span><br><span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-comment"># Log in to your W&amp;B account</span><br><span class="hljs-keyword">import</span> wandb<br>wandb.login()<br></code></pre></td></tr></tbody></table></figure><h3 id="run-an-experiment">Run an experiment</h3><p>开始新的运行并传入超参数进行跟踪</p><p>训练或评估的日志指标</p><p>在 dashboard 中可视化结果</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> random<br><br><span class="hljs-comment"># Launch 5 simulated experiments</span><br>total_runs = <span class="hljs-number">5</span><br><span class="hljs-keyword">for</span> run <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_runs):<br>  <span class="hljs-comment"># 🐝 1️⃣ Start a new run to track this script</span><br>  wandb.init(<br>      <span class="hljs-comment"># Set the project where this run will be logged</span><br>      project=<span class="hljs-string">"basic-intro"</span>, <br>      <span class="hljs-comment"># We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)</span><br>      name=<span class="hljs-string">f"experiment_<span class="hljs-subst">{run}</span>"</span>, <br>      <span class="hljs-comment"># Track hyperparameters and run metadata</span><br>      config={<br>      <span class="hljs-string">"learning_rate"</span>: <span class="hljs-number">0.02</span>,<br>      <span class="hljs-string">"architecture"</span>: <span class="hljs-string">"CNN"</span>,<br>      <span class="hljs-string">"dataset"</span>: <span class="hljs-string">"CIFAR-100"</span>,<br>      <span class="hljs-string">"epochs"</span>: <span class="hljs-number">10</span>,<br>      })<br>  <br>  <span class="hljs-comment"># This simple block simulates a training loop logging metrics</span><br>  epochs = <span class="hljs-number">10</span><br>  offset = random.random() / <span class="hljs-number">5</span><br>  <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, epochs):<br>      acc = <span class="hljs-number">1</span> - <span class="hljs-number">2</span> ** -epoch - random.random() / epoch - offset<br>      loss = <span class="hljs-number">2</span> ** -epoch + random.random() / epoch + offset<br>      <br>      <span class="hljs-comment"># 🐝 2️⃣ Log metrics from your script to W&amp;B</span><br>      wandb.log({<span class="hljs-string">"acc"</span>: acc, <span class="hljs-string">"loss"</span>: loss})<br>      <br>  <span class="hljs-comment"><code class="language-hljs python"><span class="hljs-keyword">import</span> random<br><br><span class="hljs-comment"># Launch 5 simulated experiments</span><br>total_runs = <span class="hljs-number">5</span><br><span class="hljs-keyword">for</span> run <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_runs):<br>  <span class="hljs-comment"># 🐝 1️⃣ Start a new run to track this script</span><br>  wandb.init(<br>      <span class="hljs-comment"># Set the project where this run will be logged</span><br>      project=<span class="hljs-string">"basic-intro"</span>, <br>      <span class="hljs-comment"># We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)</span><br>      name=<span class="hljs-string">f"experiment_<span class="hljs-subst">{run}</span>"</span>, <br>      <span class="hljs-comment"># Track hyperparameters and run metadata</span><br>      config={<br>      <span class="hljs-string">"learning_rate"</span>: <span class="hljs-number">0.02</span>,<br>      <span class="hljs-string">"architecture"</span>: <span class="hljs-string">"CNN"</span>,<br>      <span class="hljs-string">"dataset"</span>: <span class="hljs-string">"CIFAR-100"</span>,<br>      <span class="hljs-string">"epochs"</span>: <span class="hljs-number">10</span>,<br>      })<br>  <br>  <span class="hljs-comment"># This simple block simulates a training loop logging metrics</span><br>  epochs = <span class="hljs-number">10</span><br>  offset = random.random() / <span class="hljs-number">5</span><br>  <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, epochs):<br>      acc = <span class="hljs-number">1</span> - <span class="hljs-number">2</span> ** -epoch - random.random() / epoch - offset<br>      loss = <span class="hljs-number">2</span> ** -epoch + random.random() / epoch + offset<br>      <br>      <span class="hljs-comment"># 🐝 2️⃣ Log metrics from your script to W&amp;B</span><br>      wandb.log({<span class="hljs-string">"acc"</span>: acc, <span class="hljs-string">"loss"</span>: loss})<br>      <br>  <span class="hljs-comment"># Mark the run as finished</span><br>  wandb.finish()<br></code></pre></td></tr></tbody></table></figure><p>3️⃣ 当您运行此代码时，您可以通过单击上面的任何 👆 wandb链接找到您的交互式 dashboard。</p><h3 id="simple-pytorch-neural-network">Simple Pytorch NeuralNetwork</h3><p>运行此模型以训练一个简单的 MNIST分类器，然后单击项目页面链接以实时查看您的结果流到 W&amp;B 项目。</p><p>wandb 中的任何运行都会自动记录 <a href="https://docs.wandb.ai/ref/app/pages/run-page#charts-tab">metrics</a>,<a href="https://docs.wandb.ai/ref/app/pages/run-page#system-tab">systeminformation</a>, <a href="https://docs.wandb.ai/ref/app/pages/run-page#overview-tab">hyperparameters</a>,<a href="https://docs.wandb.ai/ref/app/pages/run-page#logs-tab">terminaloutput</a> ，您将看到一个包含模型输入和输出的交互式表格。</p><h4 id="set-up-dataloader">Set up Dataloader</h4><p>要运行此示例，我们需要安装 PyTorch。如果您使用的是 GoogleColab，则它已经预装。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">!pip install torch torchvision<br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> wandb<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch, torchvision<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T<br><br>device = <span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_dataloader</span>(<span class="hljs-params">is_train, batch_size, <span class="hljs-built_in">slice</span>=<span class="hljs-number">5</span></span>):<br>    <span class="hljs-string">"Get a training dataloader"</span><br>    full_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">"."</span>, train=is_train, transform=T.ToTensor(), download=<span class="hljs-literal">True</span>)<br>    sub_dataset = torch.utils.data.Subset(full_dataset, indices=<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(full_dataset), <span class="hljs-built_in">slice</span>))<br>    loader = torch.utils.data.DataLoader(dataset=sub_dataset, <br>                                         batch_size=batch_size, <br>                                         shuffle=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>, <br>                                         pin_memory=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> loader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>(<span class="hljs-params">dropout</span>):<br>    <span class="hljs-string">"A simple model"</span><br>    model = nn.Sequential(nn.Flatten(),<br>                         nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">256</span>),<br>                         nn.BatchNorm1d(<span class="hljs-number">256</span>),<br>                         nn.ReLU(),<br>                         nn.Dropout(dropout),<br>                         nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">10</span>)).to(device)<br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">validate_model</span>(<span class="hljs-params">model, valid_dl, loss_func, log_images=<span class="hljs-literal">False</span>, batch_idx=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">"Compute performance of the model on the validation dataset and log a wandb.Table"</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    val_loss = <span class="hljs-number">0.</span><br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        correct = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_dl):<br>            images, labels = images.to(device), labels.to(device)<br><br>            <span class="hljs-comment"># Forward pass ➡</span><br>            outputs = model(images)<br>            val_loss += loss_func(outputs, labels)*labels.size(<span class="hljs-number">0</span>)<br><br>            <span class="hljs-comment"># Compute accuracy and accumulate</span><br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>            <span class="hljs-comment"># Log one batch of images to the dashboard, always same batch_idx.</span><br>            <span class="hljs-keyword">if</span> i==batch_idx <span class="hljs-keyword">and</span> log_images:<br>                log_image_table(images, predicted, labels, outputs.softmax(dim=<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> val_loss / <span class="hljs-built_in">len</span>(valid_dl.dataset), correct / <span class="hljs-built_in">len</span>(valid_dl.dataset)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_image_table</span>(<span class="hljs-params">images, predicted, labels, probs</span>):<br>    <span class="hljs-string">"Log a wandb.Table with (img, pred, target, scores)"</span><br>    <span class="hljs-comment"># 🐝 Create a wandb Table to log images, labels and predictions to</span><br>    table = wandb.Table(columns=[<span class="hljs-string">"image"</span>, <span class="hljs-string">"pred"</span>, <span class="hljs-string">"target"</span>]+[<span class="hljs-string">f"score_<span class="hljs-subst">{i}</span>"</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)])<br>    <span class="hljs-keyword">for</span> img, pred, targ, prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(images.to(<span class="hljs-string">"cpu"</span>), predicted.to(<span class="hljs-string">"cpu"</span>), labels.to(<span class="hljs-string">"cpu"</span>), probs.to(<span class="hljs-string">"cpu"</span>)):<br>        table.add_data(wandb.Image(img[<span class="hljs-number">0</span>].numpy()*<span class="hljs-number">255</span>), pred, targ, *prob.numpy())<br>    wandb.log({<span class="hljs-string">"predictions_table"</span>:table}, commit=<span class="hljs-literal"><code class="language-hljs python"><span class="hljs-keyword">import</span> wandb<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch, torchvision<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T<br><br>device = <span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_dataloader</span>(<span class="hljs-params">is_train, batch_size, <span class="hljs-built_in">slice</span>=<span class="hljs-number">5</span></span>):<br>    <span class="hljs-string">"Get a training dataloader"</span><br>    full_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">"."</span>, train=is_train, transform=T.ToTensor(), download=<span class="hljs-literal">True</span>)<br>    sub_dataset = torch.utils.data.Subset(full_dataset, indices=<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(full_dataset), <span class="hljs-built_in">slice</span>))<br>    loader = torch.utils.data.DataLoader(dataset=sub_dataset, <br>                                         batch_size=batch_size, <br>                                         shuffle=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>, <br>                                         pin_memory=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> loader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>(<span class="hljs-params">dropout</span>):<br>    <span class="hljs-string">"A simple model"</span><br>    model = nn.Sequential(nn.Flatten(),<br>                         nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">256</span>),<br>                         nn.BatchNorm1d(<span class="hljs-number">256</span>),<br>                         nn.ReLU(),<br>                         nn.Dropout(dropout),<br>                         nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">10</span>)).to(device)<br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">validate_model</span>(<span class="hljs-params">model, valid_dl, loss_func, log_images=<span class="hljs-literal">False</span>, batch_idx=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">"Compute performance of the model on the validation dataset and log a wandb.Table"</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    val_loss = <span class="hljs-number">0.</span><br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        correct = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_dl):<br>            images, labels = images.to(device), labels.to(device)<br><br>            <span class="hljs-comment"># Forward pass ➡</span><br>            outputs = model(images)<br>            val_loss += loss_func(outputs, labels)*labels.size(<span class="hljs-number">0</span>)<br><br>            <span class="hljs-comment"># Compute accuracy and accumulate</span><br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>            <span class="hljs-comment"># Log one batch of images to the dashboard, always same batch_idx.</span><br>            <span class="hljs-keyword">if</span> i==batch_idx <span class="hljs-keyword">and</span> log_images:<br>                log_image_table(images, predicted, labels, outputs.softmax(dim=<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> val_loss / <span class="hljs-built_in">len</span>(valid_dl.dataset), correct / <span class="hljs-built_in">len</span>(valid_dl.dataset)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_image_table</span>(<span class="hljs-params">images, predicted, labels, probs</span>):<br>    <span class="hljs-string">"Log a wandb.Table with (img, pred, target, scores)"</span><br>    <span class="hljs-comment"># 🐝 Create a wandb Table to log images, labels and predictions to</span><br>    table = wandb.Table(columns=[<span class="hljs-string">"image"</span>, <span class="hljs-string">"pred"</span>, <span class="hljs-string">"target"</span>]+[<span class="hljs-string">f"score_<span class="hljs-subst">{i}</span>"</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)])<br>    <span class="hljs-keyword">for</span> img, pred, targ, prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(images.to(<span class="hljs-string">"cpu"</span>), predicted.to(<span class="hljs-string">"cpu"</span>), labels.to(<span class="hljs-string">"cpu"</span>), probs.to(<span class="hljs-string">"cpu"</span>)):<br>        table.add_data(wandb.Image(img[<span class="hljs-number">0</span>].numpy()*<span class="hljs-number">255</span>), pred, targ, *prob.numpy())<br>    wandb.log({<span class="hljs-string">"predictions_table"</span>:table}, commit=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></tbody></table></figure><h4 id="train-your-model">Train Your Model</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-comment"># Launch 5 experiments, trying different dropout rates</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-comment"># 🐝 initialise a wandb run</span><br>    wandb.init(<br>        project=<span class="hljs-string">"pytorch-intro"</span>,<br>        config={<br>            <span class="hljs-string">"epochs"</span>: <span class="hljs-number">10</span>,<br>            <span class="hljs-string">"batch_size"</span>: <span class="hljs-number">128</span>,<br>            <span class="hljs-string">"lr"</span>: <span class="hljs-number">1e-3</span>,<br>            <span class="hljs-string">"dropout"</span>: random.uniform(<span class="hljs-number">0.01</span>, <span class="hljs-number">0.80</span>),<br>            })<br>    <br>    <span class="hljs-comment"># Copy your config </span><br>    config = wandb.config<br><br>    <span class="hljs-comment"># Get the data</span><br>    train_dl = get_dataloader(is_train=<span class="hljs-literal">True</span>, batch_size=config.batch_size)<br>    valid_dl = get_dataloader(is_train=<span class="hljs-literal">False</span>, batch_size=<span class="hljs-number">2</span>*config.batch_size)<br>    n_steps_per_epoch = math.ceil(<span class="hljs-built_in">len</span>(train_dl.dataset) / config.batch_size)<br>    <br>    <span class="hljs-comment"># A simple MLP model</span><br>    model = get_model(config.dropout)<br><br>    <span class="hljs-comment"># Make the loss and optimizer</span><br>    loss_func = nn.CrossEntropyLoss()<br>    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)<br><br>   <span class="hljs-comment"># Training</span><br>    example_ct = <span class="hljs-number">0</span><br>    step_ct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>        model.train()<br>        <span class="hljs-keyword">for</span> step, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dl):<br>            images, labels = images.to(device), labels.to(device)<br><br>            outputs = model(images)<br>            train_loss = loss_func(outputs, labels)<br>            optimizer.zero_grad()<br>            train_loss.backward()<br>            optimizer.step()<br>            <br>            example_ct += <span class="hljs-built_in">len</span>(images)<br>            metrics = {<span class="hljs-string">"train/train_loss"</span>: train_loss, <br>                       <span class="hljs-string">"train/epoch"</span>: (step + <span class="hljs-number">1</span> + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, <br>                       <span class="hljs-string">"train/example_ct"</span>: example_ct}<br>            <br>            <span class="hljs-keyword">if</span> step + <span class="hljs-number">1</span> &lt; n_steps_per_epoch:<br>                <span class="hljs-comment"># 🐝 Log train metrics to wandb </span><br>                wandb.log(metrics)<br>                <br>            step_ct += <span class="hljs-number">1</span><br><br>        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-<span class="hljs-number">1</span>)))<br><br>        <span class="hljs-comment"># 🐝 Log train and validation metrics to wandb</span><br>        val_metrics = {<span class="hljs-string">"val/val_loss"</span>: val_loss, <br>                       <span class="hljs-string">"val/val_accuracy"</span>: accuracy}<br>        wandb.log({**metrics, **val_metrics})<br>        <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Train Loss: <span class="hljs-subst">{train_loss:<span class="hljs-number">.3</span>f}</span>, Valid Loss: <span class="hljs-subst">{val_loss:3f}</span>, Accuracy: <span class="hljs-subst">{accuracy:<span class="hljs-number">.2</span>f}</span>"</span>)<br><br>    <span class="hljs-comment"># If you had a test set, this is how you could log it as a Summary metric</span><br>    wandb.summary[<span class="hljs-string">'test_accuracy'</span>] = <span class="hljs-number">0.8</span><br><br>    <span class="hljs-comment"><code class="language-hljs python"><span class="hljs-comment"># Launch 5 experiments, trying different dropout rates</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-comment"># 🐝 initialise a wandb run</span><br>    wandb.init(<br>        project=<span class="hljs-string">"pytorch-intro"</span>,<br>        config={<br>            <span class="hljs-string">"epochs"</span>: <span class="hljs-number">10</span>,<br>            <span class="hljs-string">"batch_size"</span>: <span class="hljs-number">128</span>,<br>            <span class="hljs-string">"lr"</span>: <span class="hljs-number">1e-3</span>,<br>            <span class="hljs-string">"dropout"</span>: random.uniform(<span class="hljs-number">0.01</span>, <span class="hljs-number">0.80</span>),<br>            })<br>    <br>    <span class="hljs-comment"># Copy your config </span><br>    config = wandb.config<br><br>    <span class="hljs-comment"># Get the data</span><br>    train_dl = get_dataloader(is_train=<span class="hljs-literal">True</span>, batch_size=config.batch_size)<br>    valid_dl = get_dataloader(is_train=<span class="hljs-literal">False</span>, batch_size=<span class="hljs-number">2</span>*config.batch_size)<br>    n_steps_per_epoch = math.ceil(<span class="hljs-built_in">len</span>(train_dl.dataset) / config.batch_size)<br>    <br>    <span class="hljs-comment"># A simple MLP model</span><br>    model = get_model(config.dropout)<br><br>    <span class="hljs-comment"># Make the loss and optimizer</span><br>    loss_func = nn.CrossEntropyLoss()<br>    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)<br><br>   <span class="hljs-comment"># Training</span><br>    example_ct = <span class="hljs-number">0</span><br>    step_ct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>        model.train()<br>        <span class="hljs-keyword">for</span> step, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dl):<br>            images, labels = images.to(device), labels.to(device)<br><br>            outputs = model(images)<br>            train_loss = loss_func(outputs, labels)<br>            optimizer.zero_grad()<br>            train_loss.backward()<br>            optimizer.step()<br>            <br>            example_ct += <span class="hljs-built_in">len</span>(images)<br>            metrics = {<span class="hljs-string">"train/train_loss"</span>: train_loss, <br>                       <span class="hljs-string">"train/epoch"</span>: (step + <span class="hljs-number">1</span> + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, <br>                       <span class="hljs-string">"train/example_ct"</span>: example_ct}<br>            <br>            <span class="hljs-keyword">if</span> step + <span class="hljs-number">1</span> &lt; n_steps_per_epoch:<br>                <span class="hljs-comment"># 🐝 Log train metrics to wandb </span><br>                wandb.log(metrics)<br>                <br>            step_ct += <span class="hljs-number">1</span><br><br>        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-<span class="hljs-number">1</span>)))<br><br>        <span class="hljs-comment"># 🐝 Log train and validation metrics to wandb</span><br>        val_metrics = {<span class="hljs-string">"val/val_loss"</span>: val_loss, <br>                       <span class="hljs-string">"val/val_accuracy"</span>: accuracy}<br>        wandb.log({**metrics, **val_metrics})<br>        <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Train Loss: <span class="hljs-subst">{train_loss:<span class="hljs-number">.3</span>f}</span>, Valid Loss: <span class="hljs-subst">{val_loss:3f}</span>, Accuracy: <span class="hljs-subst">{accuracy:<span class="hljs-number">.2</span>f}</span>"</span>)<br><br>    <span class="hljs-comment"># If you had a test set, this is how you could log it as a Summary metric</span><br>    wandb.summary[<span class="hljs-string">'test_accuracy'</span>] = <span class="hljs-number">0.8</span><br><br>    <span class="hljs-comment"># 🐝 Close your wandb run </span><br>    wandb.finish()<br></code></pre></td></tr></tbody></table></figure><p>您现在已经使用 wandb 训练了您的第一个模型！ 👆 单击上面的 wandb链接查看您的指标</p><h3 id="try-wb-alerts">Try W&amp;B Alerts</h3><p><strong><a href="https://docs.wandb.ai/guides/track/alert">W&amp;BAlerts</a></strong> 允许您将从 Python 代码触发的警报发送到您的 Slack或电子邮件。第一次发送 Slack 或电子邮件警报时，需要执行 2个步骤，这些警报由您的代码触发：</p><p>1) 在你的 W&amp;B <a href="https://wandb.ai/settings">UserSettings</a> 开启警报</p><p>2) 添加 <code>wandb.alert()</code> 到你的代码:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre class=" language-hljs python">wandb.alert(<br>    title=<span class="hljs-string">"Low accuracy"</span>, <br>    text=<span class="hljs-string"><code class="language-hljs python">wandb.alert(<br>    title=<span class="hljs-string">"Low accuracy"</span>, <br>    text=<span class="hljs-string">f"Accuracy is below the acceptable threshold"</span><br>)<br></code></pre></td></tr></tbody></table></figure><p>请参阅下面的最小示例以了解如何使用 wandb.alert，您可以在此处找到 <a href="https://docs.wandb.ai/guides/track/alert">W&amp;BAlerts</a>的完整文档</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-comment"># Start a wandb run</span><br>wandb.init(project=<span class="hljs-string">"pytorch-intro"</span>)<br><br><span class="hljs-comment"># Simulating a model training loop</span><br>acc_threshold = <span class="hljs-number">0.3</span><br><span class="hljs-keyword">for</span> training_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<br><br>    <span class="hljs-comment"># Generate a random number for accuracy</span><br>    accuracy = <span class="hljs-built_in">round</span>(random.random() + random.random(), <span class="hljs-number">3</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Accuracy is: <span class="hljs-subst">{accuracy}</span>, <span class="hljs-subst">{acc_threshold}</span>'</span>)<br>    <br>    <span class="hljs-comment"># 🐝 Log accuracy to wandb</span><br>    wandb.log({<span class="hljs-string">"Accuracy"</span>: accuracy})<br><br>    <span class="hljs-comment"># 🔔 If the accuracy is below the threshold, fire a W&amp;B Alert and stop the run</span><br>    <span class="hljs-keyword">if</span> accuracy &lt;= acc_threshold:<br>        <span class="hljs-comment"># 🐝 Send the wandb Alert</span><br>        wandb.alert(<br>            title=<span class="hljs-string">'Low Accuracy'</span>,<br>            text=<span class="hljs-string">f'Accuracy <span class="hljs-subst">{accuracy}</span> at step <span class="hljs-subst">{training_step}</span> is below the acceptable theshold, <span class="hljs-subst">{acc_threshold}</span>'</span>,<br>        )<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Alert triggered'</span>)<br>        <span class="hljs-keyword">break</span><br><br><span class="hljs-comment"><code class="language-hljs python"><span class="hljs-comment"># Start a wandb run</span><br>wandb.init(project=<span class="hljs-string">"pytorch-intro"</span>)<br><br><span class="hljs-comment"># Simulating a model training loop</span><br>acc_threshold = <span class="hljs-number">0.3</span><br><span class="hljs-keyword">for</span> training_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<br><br>    <span class="hljs-comment"># Generate a random number for accuracy</span><br>    accuracy = <span class="hljs-built_in">round</span>(random.random() + random.random(), <span class="hljs-number">3</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Accuracy is: <span class="hljs-subst">{accuracy}</span>, <span class="hljs-subst">{acc_threshold}</span>'</span>)<br>    <br>    <span class="hljs-comment"># 🐝 Log accuracy to wandb</span><br>    wandb.log({<span class="hljs-string">"Accuracy"</span>: accuracy})<br><br>    <span class="hljs-comment"># 🔔 If the accuracy is below the threshold, fire a W&amp;B Alert and stop the run</span><br>    <span class="hljs-keyword">if</span> accuracy &lt;= acc_threshold:<br>        <span class="hljs-comment"># 🐝 Send the wandb Alert</span><br>        wandb.alert(<br>            title=<span class="hljs-string">'Low Accuracy'</span>,<br>            text=<span class="hljs-string">f'Accuracy <span class="hljs-subst">{accuracy}</span> at step <span class="hljs-subst">{training_step}</span> is below the acceptable theshold, <span class="hljs-subst">{acc_threshold}</span>'</span>,<br>        )<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Alert triggered'</span>)<br>        <span class="hljs-keyword">break</span><br><br><span class="hljs-comment"># Mark the run as finished (useful in Jupyter notebooks)</span><br>wandb.finish()<br></code></pre></td></tr></tbody></table></figure><h2 id="visualize-predictions">Visualize predictions</h2><p>这包括如何在训练过程中使用 PyTorch 对 MNIST数据进行跟踪、可视化和比较模型预测。</p><p>你将学到如何：</p><ol type="1"><li>在模型训练或评估期间将指标、图像、文本等记录到<code>wandb.Table()</code></li><li>查看、排序、筛选、分组、加入、交互式查询和探索这些表</li><li>比较模型预测或结果：动态地跨越特定图像、超参数/模型版本或时间步长。</li></ol><h3 id="examples">Examples</h3><h4 id="compare-predicted-scores-for-specific-images">Compare predictedscores for specific images</h4><p><a href="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*6z1980*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#compare-predictions-after-1-vs-5-epochs">实例：比较1 和 5 个训练周期后的预测</a></p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedNMme6Qj.png" alt="1 epoch vs 5 epochs of training"><figcaption aria-hidden="true">1 epoch vs 5 epochs oftraining</figcaption></figure><p>直方图比较了两个模型之间的每类分数。每个直方图中顶部的绿色条代表模型“CNN-2,1 epoch”（id 0），它只训练了 1 个 epoch。底部的紫色条代表模型“CNN-2, 5epochs” (id 1)，它训练了 5 个epochs。图像被过滤到模型不一致的情况。例如，在第一行中，“4”在 1个时期后在所有可能的数字中获得高分，但在 5个时期后，它在正确标签上得分最高，而在其余部分得分非常低。</p><h4 id="focus-on-top-errors-over-time">Focus on top errors overtime</h4><p><a href="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*1nxbzl7*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#top-errors-over-time">实例→</a></p><p>查看完整测试数据的不正确预测（过滤 "guess" != "truth"的行）。请注意，在 1 个训练时期后有 229 个错误猜测，但在 5 个时期后只有98 个。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined7g8nodn.png" alt="side by side, 1 vs 5 epochs of training"><figcaption aria-hidden="true">side by side, 1 vs 5 epochs oftraining</figcaption></figure><h4 id="compare-model-performance-and-find-patterns">Compare modelperformance and find patterns</h4><p><a href="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*8r828v*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#false-positives-grouped-by-guess">查看实例中的完整详细信息→</a></p><p>过滤出正确答案，然后按猜测分组，以查看错误分类图像的示例和真实标签的基本分布——并排显示两个模型。具有2X layer sizes和学习率的模型变体在左侧，基线在右侧。请注意，对于每个猜测的类，基线都会犯更多的错误。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedi5PP9AE.png" alt="grouped errors for baseline vs double variant"><figcaption aria-hidden="true">grouped errors for baseline vs doublevariant</figcaption></figure><h3 id="sign-up-or-login">Sign up or login</h3><p><a href="https://wandb.ai/login">Sign up or login</a> W&amp;B以在浏览器中查看您的实验并与之互动。</p><p>在此示例中，我们使用 Google Colab作为方便的托管环境，但您可以从任何地方运行自己的训练脚本，并使用 W&amp;B的实验跟踪工具可视化指标。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">!pip install wandb -qqq<br></code></pre></td></tr></tbody></table></figure><p>登录您的帐户</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> wandb<br>wandb.login()<br><br>WANDB_PROJECT = <span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">import</span> wandb<br>wandb.login()<br><br>WANDB_PROJECT = <span class="hljs-string">"mnist-viz"</span><br></code></pre></td></tr></tbody></table></figure><h3 id="setup">0. Setup</h3><p>安装依赖项，下载 MNIST，并使用 PyTorch 创建训练和测试数据集。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T <br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br>device = <span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span><br><br><span class="hljs-comment"># create train and test dataloaders</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_dataloader</span>(<span class="hljs-params">is_train, batch_size, <span class="hljs-built_in">slice</span>=<span class="hljs-number">5</span></span>):<br>    <span class="hljs-string">"Get a training dataloader"</span><br>    ds = torchvision.datasets.MNIST(root=<span class="hljs-string">"."</span>, train=is_train, transform=T.ToTensor(), download=<span class="hljs-literal">True</span>)<br>    loader = torch.utils.data.DataLoader(dataset=ds, <br>                                         batch_size=batch_size, <br>                                         shuffle=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>, <br>                                         pin_memory=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T <br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br>device = <span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span><br><br><span class="hljs-comment"># create train and test dataloaders</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_dataloader</span>(<span class="hljs-params">is_train, batch_size, <span class="hljs-built_in">slice</span>=<span class="hljs-number">5</span></span>):<br>    <span class="hljs-string">"Get a training dataloader"</span><br>    ds = torchvision.datasets.MNIST(root=<span class="hljs-string">"."</span>, train=is_train, transform=T.ToTensor(), download=<span class="hljs-literal">True</span>)<br>    loader = torch.utils.data.DataLoader(dataset=ds, <br>                                         batch_size=batch_size, <br>                                         shuffle=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>, <br>                                         pin_memory=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> loader<br></code></pre></td></tr></tbody></table></figure><h3 id="define-the-model-and-training-schedule">1. Define the model andtraining schedule</h3><ul><li>设置要运行的纪元数，其中每个纪元包含一个训练步骤和一个验证（测试）步骤。（可选）配置每个测试步骤要记录的数据量。这里要可视化的批次数和每批次的图像数设置得较低，以简化演示。</li><li>定义一个简单的卷积神经网络（遵循 <a href="https://github.com/yunjey/pytorch-tutorial">pytorch-tutorial</a>代码）。</li><li>使用 PyTorch 加载训练集和测试集</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-comment"># Number of epochs to run</span><br><span class="hljs-comment"># Each epoch includes a training step and a test step, so this sets</span><br><span class="hljs-comment"># the number of tables of test predictions to log</span><br>EPOCHS = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># Number of batches to log from the test data for each test step</span><br><span class="hljs-comment"># (default set low to simplify demo)</span><br>NUM_BATCHES_TO_LOG = <span class="hljs-number">10</span> <span class="hljs-comment">#79</span><br><br><span class="hljs-comment"># Number of images to log per test batch</span><br><span class="hljs-comment"># (default set low to simplify demo)</span><br>NUM_IMAGES_PER_BATCH = <span class="hljs-number">32</span> <span class="hljs-comment">#128</span><br><br><span class="hljs-comment"># training configuration and hyperparameters</span><br>NUM_CLASSES = <span class="hljs-number">10</span><br>BATCH_SIZE = <span class="hljs-number">32</span><br>LEARNING_RATE = <span class="hljs-number">0.001</span><br>L1_SIZE = <span class="hljs-number">32</span><br>L2_SIZE = <span class="hljs-number">64</span><br><span class="hljs-comment"># changing this may require changing the shape of adjacent layers</span><br>CONV_KERNEL_SIZE = <span class="hljs-number">5</span><br><br><span class="hljs-comment"># define a two-layer convolutional neural network</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br>        self.layer1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, L1_SIZE, CONV_KERNEL_SIZE, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(L1_SIZE),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.layer2 = nn.Sequential(<br>            nn.Conv2d(L1_SIZE, L2_SIZE, CONV_KERNEL_SIZE, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(L2_SIZE),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.fc = nn.Linear(<span class="hljs-number">7</span>*<span class="hljs-number">7</span>*L2_SIZE, NUM_CLASSES)<br>        self.softmax = nn.Softmax(NUM_CLASSES)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># uncomment to see the shape of a given layer:</span><br>        <span class="hljs-comment">#print("x: ", x.size())</span><br>        out = self.layer1(x)<br>        out = self.layer2(out)<br>        out = out.reshape(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.fc(out)<br>        <span class="hljs-keyword">return</span> out<br><br>train_loader = get_dataloader(is_train=<span class="hljs-literal">True</span>, batch_size=BATCH_SIZE)<br>test_loader = get_dataloader(is_train=<span class="hljs-literal">False</span>, batch_size=<span class="hljs-number">2</span>*BATCH_SIZE)<br><br>device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string"><code class="language-hljs python"><span class="hljs-comment"># Number of epochs to run</span><br><span class="hljs-comment"># Each epoch includes a training step and a test step, so this sets</span><br><span class="hljs-comment"># the number of tables of test predictions to log</span><br>EPOCHS = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># Number of batches to log from the test data for each test step</span><br><span class="hljs-comment"># (default set low to simplify demo)</span><br>NUM_BATCHES_TO_LOG = <span class="hljs-number">10</span> <span class="hljs-comment">#79</span><br><br><span class="hljs-comment"># Number of images to log per test batch</span><br><span class="hljs-comment"># (default set low to simplify demo)</span><br>NUM_IMAGES_PER_BATCH = <span class="hljs-number">32</span> <span class="hljs-comment">#128</span><br><br><span class="hljs-comment"># training configuration and hyperparameters</span><br>NUM_CLASSES = <span class="hljs-number">10</span><br>BATCH_SIZE = <span class="hljs-number">32</span><br>LEARNING_RATE = <span class="hljs-number">0.001</span><br>L1_SIZE = <span class="hljs-number">32</span><br>L2_SIZE = <span class="hljs-number">64</span><br><span class="hljs-comment"># changing this may require changing the shape of adjacent layers</span><br>CONV_KERNEL_SIZE = <span class="hljs-number">5</span><br><br><span class="hljs-comment"># define a two-layer convolutional neural network</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br>        self.layer1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, L1_SIZE, CONV_KERNEL_SIZE, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(L1_SIZE),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.layer2 = nn.Sequential(<br>            nn.Conv2d(L1_SIZE, L2_SIZE, CONV_KERNEL_SIZE, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(L2_SIZE),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.fc = nn.Linear(<span class="hljs-number">7</span>*<span class="hljs-number">7</span>*L2_SIZE, NUM_CLASSES)<br>        self.softmax = nn.Softmax(NUM_CLASSES)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># uncomment to see the shape of a given layer:</span><br>        <span class="hljs-comment">#print("x: ", x.size())</span><br>        out = self.layer1(x)<br>        out = self.layer2(out)<br>        out = out.reshape(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.fc(out)<br>        <span class="hljs-keyword">return</span> out<br><br>train_loader = get_dataloader(is_train=<span class="hljs-literal">True</span>, batch_size=BATCH_SIZE)<br>test_loader = get_dataloader(is_train=<span class="hljs-literal">False</span>, batch_size=<span class="hljs-number">2</span>*BATCH_SIZE)<br><br>device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<br></code></pre></td></tr></tbody></table></figure><h3 id="run-training-and-log-test-predictions">2. Run training and logtest predictions</h3><p>对于每个时期，运行一个训练步骤和一个测试步骤。对于每个测试步骤，创建一个wandb.Table()来存储测试预测。这些可以在您的浏览器中进行可视化、动态查询和并排比较。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-comment"># ✨ W&amp;B: Initialize a new run to track this model's training</span><br>wandb.init(project=<span class="hljs-string">"table-quickstart"</span>)<br><br><span class="hljs-comment"># ✨ W&amp;B: Log hyperparameters using config</span><br>cfg = wandb.config<br>cfg.update({<span class="hljs-string">"epochs"</span> : EPOCHS, <span class="hljs-string">"batch_size"</span>: BATCH_SIZE, <span class="hljs-string">"lr"</span> : LEARNING_RATE,<br>            <span class="hljs-string">"l1_size"</span> : L1_SIZE, <span class="hljs-string">"l2_size"</span>: L2_SIZE,<br>            <span class="hljs-string">"conv_kernel"</span> : CONV_KERNEL_SIZE,<br>            <span class="hljs-string">"img_count"</span> : <span class="hljs-built_in">min</span>(<span class="hljs-number">10000</span>, NUM_IMAGES_PER_BATCH*NUM_BATCHES_TO_LOG)})<br><br><span class="hljs-comment"># define model, loss, and optimizer</span><br>model = ConvNet(NUM_CLASSES).to(device)<br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)<br><br><span class="hljs-comment"># convenience funtion to log predictions for a batch of test images</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_test_predictions</span>(<span class="hljs-params">images, labels, outputs, predicted, test_table, log_counter</span>):<br>  <span class="hljs-comment"># obtain confidence scores for all classes</span><br>  scores = F.softmax(outputs.data, dim=<span class="hljs-number">1</span>)<br>  log_scores = scores.cpu().numpy()<br>  log_images = images.cpu().numpy()<br>  log_labels = labels.cpu().numpy()<br>  log_preds = predicted.cpu().numpy()<br>  <span class="hljs-comment"># adding ids based on the order of the images</span><br>  _<span class="hljs-built_in">id</span> = <span class="hljs-number">0</span><br>  <span class="hljs-keyword">for</span> i, l, p, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(log_images, log_labels, log_preds, log_scores):<br>    <span class="hljs-comment"># add required info to data table:</span><br>    <span class="hljs-comment"># id, image pixels, model's guess, true label, scores for all classes</span><br>    img_id = <span class="hljs-built_in">str</span>(_<span class="hljs-built_in">id</span>) + <span class="hljs-string">"_"</span> + <span class="hljs-built_in">str</span>(log_counter)<br>    test_table.add_data(img_id, wandb.Image(i), p, l, *s)<br>    _<span class="hljs-built_in">id</span> += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> _<span class="hljs-built_in">id</span> == NUM_IMAGES_PER_BATCH:<br>      <span class="hljs-keyword">break</span><br><br><span class="hljs-comment"># train the model</span><br>total_step = <span class="hljs-built_in">len</span>(train_loader)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):<br>    <span class="hljs-comment"># training step</span><br>    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        images = images.to(device)<br>        labels = labels.to(device)<br>        <span class="hljs-comment"># forward pass</span><br>        outputs = model(images)<br>        loss = criterion(outputs, labels)<br>        <span class="hljs-comment"># backward and optimize</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>  <br>        <span class="hljs-comment"># ✨ W&amp;B: Log loss over training steps, visualized in the UI live</span><br>        wandb.log({<span class="hljs-string">"loss"</span> : loss})<br>        <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span> (<span class="hljs-string">'Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'</span><br>                .<span class="hljs-built_in">format</span>(epoch+<span class="hljs-number">1</span>, EPOCHS, i+<span class="hljs-number">1</span>, total_step, loss.item()))<br>            <br><br>    <span class="hljs-comment"># ✨ W&amp;B: Create a Table to store predictions for each test step</span><br>    columns=[<span class="hljs-string">"id"</span>, <span class="hljs-string">"image"</span>, <span class="hljs-string">"guess"</span>, <span class="hljs-string">"truth"</span>]<br>    <span class="hljs-keyword">for</span> digit <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>      columns.append(<span class="hljs-string">"score_"</span> + <span class="hljs-built_in">str</span>(digit))<br>    test_table = wandb.Table(columns=columns)<br><br>    <span class="hljs-comment"># test the model</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    log_counter = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        correct = <span class="hljs-number">0</span><br>        total = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:<br>            images = images.to(device)<br>            labels = labels.to(device)<br>            outputs = model(images)<br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> log_counter &lt; NUM_BATCHES_TO_LOG:<br>              log_test_predictions(images, labels, outputs, predicted, test_table, log_counter)<br>              log_counter += <span class="hljs-number">1</span><br>            total += labels.size(<span class="hljs-number">0</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>        acc = <span class="hljs-number">100</span> * correct / total<br>        <span class="hljs-comment"># ✨ W&amp;B: Log accuracy across training epochs, to visualize in the UI</span><br>        wandb.log({<span class="hljs-string">"epoch"</span> : epoch, <span class="hljs-string">"acc"</span> : acc})<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Test Accuracy of the model on the 10000 test images: {} %'</span>.<span class="hljs-built_in">format</span>(acc))<br><br>    <span class="hljs-comment"># ✨ W&amp;B: Log predictions table to wandb</span><br>    wandb.log({<span class="hljs-string">"test_predictions"</span> : test_table})<br><br><span class="hljs-comment"><code class="language-hljs python"><span class="hljs-comment"># ✨ W&amp;B: Initialize a new run to track this model's training</span><br>wandb.init(project=<span class="hljs-string">"table-quickstart"</span>)<br><br><span class="hljs-comment"># ✨ W&amp;B: Log hyperparameters using config</span><br>cfg = wandb.config<br>cfg.update({<span class="hljs-string">"epochs"</span> : EPOCHS, <span class="hljs-string">"batch_size"</span>: BATCH_SIZE, <span class="hljs-string">"lr"</span> : LEARNING_RATE,<br>            <span class="hljs-string">"l1_size"</span> : L1_SIZE, <span class="hljs-string">"l2_size"</span>: L2_SIZE,<br>            <span class="hljs-string">"conv_kernel"</span> : CONV_KERNEL_SIZE,<br>            <span class="hljs-string">"img_count"</span> : <span class="hljs-built_in">min</span>(<span class="hljs-number">10000</span>, NUM_IMAGES_PER_BATCH*NUM_BATCHES_TO_LOG)})<br><br><span class="hljs-comment"># define model, loss, and optimizer</span><br>model = ConvNet(NUM_CLASSES).to(device)<br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)<br><br><span class="hljs-comment"># convenience funtion to log predictions for a batch of test images</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_test_predictions</span>(<span class="hljs-params">images, labels, outputs, predicted, test_table, log_counter</span>):<br>  <span class="hljs-comment"># obtain confidence scores for all classes</span><br>  scores = F.softmax(outputs.data, dim=<span class="hljs-number">1</span>)<br>  log_scores = scores.cpu().numpy()<br>  log_images = images.cpu().numpy()<br>  log_labels = labels.cpu().numpy()<br>  log_preds = predicted.cpu().numpy()<br>  <span class="hljs-comment"># adding ids based on the order of the images</span><br>  _<span class="hljs-built_in">id</span> = <span class="hljs-number">0</span><br>  <span class="hljs-keyword">for</span> i, l, p, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(log_images, log_labels, log_preds, log_scores):<br>    <span class="hljs-comment"># add required info to data table:</span><br>    <span class="hljs-comment"># id, image pixels, model's guess, true label, scores for all classes</span><br>    img_id = <span class="hljs-built_in">str</span>(_<span class="hljs-built_in">id</span>) + <span class="hljs-string">"_"</span> + <span class="hljs-built_in">str</span>(log_counter)<br>    test_table.add_data(img_id, wandb.Image(i), p, l, *s)<br>    _<span class="hljs-built_in">id</span> += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> _<span class="hljs-built_in">id</span> == NUM_IMAGES_PER_BATCH:<br>      <span class="hljs-keyword">break</span><br><br><span class="hljs-comment"># train the model</span><br>total_step = <span class="hljs-built_in">len</span>(train_loader)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):<br>    <span class="hljs-comment"># training step</span><br>    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        images = images.to(device)<br>        labels = labels.to(device)<br>        <span class="hljs-comment"># forward pass</span><br>        outputs = model(images)<br>        loss = criterion(outputs, labels)<br>        <span class="hljs-comment"># backward and optimize</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>  <br>        <span class="hljs-comment"># ✨ W&amp;B: Log loss over training steps, visualized in the UI live</span><br>        wandb.log({<span class="hljs-string">"loss"</span> : loss})<br>        <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span> (<span class="hljs-string">'Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'</span><br>                .<span class="hljs-built_in">format</span>(epoch+<span class="hljs-number">1</span>, EPOCHS, i+<span class="hljs-number">1</span>, total_step, loss.item()))<br>            <br><br>    <span class="hljs-comment"># ✨ W&amp;B: Create a Table to store predictions for each test step</span><br>    columns=[<span class="hljs-string">"id"</span>, <span class="hljs-string">"image"</span>, <span class="hljs-string">"guess"</span>, <span class="hljs-string">"truth"</span>]<br>    <span class="hljs-keyword">for</span> digit <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>      columns.append(<span class="hljs-string">"score_"</span> + <span class="hljs-built_in">str</span>(digit))<br>    test_table = wandb.Table(columns=columns)<br><br>    <span class="hljs-comment"># test the model</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    log_counter = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        correct = <span class="hljs-number">0</span><br>        total = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:<br>            images = images.to(device)<br>            labels = labels.to(device)<br>            outputs = model(images)<br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> log_counter &lt; NUM_BATCHES_TO_LOG:<br>              log_test_predictions(images, labels, outputs, predicted, test_table, log_counter)<br>              log_counter += <span class="hljs-number">1</span><br>            total += labels.size(<span class="hljs-number">0</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>        acc = <span class="hljs-number">100</span> * correct / total<br>        <span class="hljs-comment"># ✨ W&amp;B: Log accuracy across training epochs, to visualize in the UI</span><br>        wandb.log({<span class="hljs-string">"epoch"</span> : epoch, <span class="hljs-string">"acc"</span> : acc})<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Test Accuracy of the model on the 10000 test images: {} %'</span>.<span class="hljs-built_in">format</span>(acc))<br><br>    <span class="hljs-comment"># ✨ W&amp;B: Log predictions table to wandb</span><br>    wandb.log({<span class="hljs-string">"test_predictions"</span> : test_table})<br><br><span class="hljs-comment"># ✨ W&B: Mark the run as complete (useful for multi-cell notebook)</span><br>wandb.finish()<br></code></pre></td></tr></tbody></table></figure><h2 id="tune-hyperparameters">Tune hyperparameters</h2><p><a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W&amp;B.ipynb">在此处试用Colab Notebook →</a></p><p>在高维超参数空间中搜索以找到性能最高的模型可能会很快变得笨拙。超参数扫描提供了一种有组织且高效的方式来进行模型大逃杀并选择最准确的模型。他们通过自动搜索超参数值的组合（例如learning rate, batch size, number of hidden layers, optimizertype）来找到最佳值来实现这一点。</p><p>在本教程中，我们将了解如何使用 Weights &amp; Biases 通过 3个简单步骤运行复杂的超参数扫描。</p><h3 id="follow-along-with-a-video-tutorial">Follow along with a <a href="http://wandb.me/sweeps-video">video tutorial</a>!</h3><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedWVKkMWw.png" alt="tune hyperparameters"><figcaption aria-hidden="true">tune hyperparameters</figcaption></figure><h3 id="setup-1">Setup</h3><p>首先安装实验跟踪库并设置您的免费 W&amp;B 帐户：</p><ol type="1"><li>使用 <code>pip install</code> 安装</li><li><code>import</code> Python 所需依赖</li><li><code>.login()</code> 这样您就可以将指标记录到您的项目中</li></ol><p>如果您以前从未使用过 Weights &amp;Biases，登录电话会给您一个注册帐户的链接。 W&amp;B可免费用于个人和学术项目！</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">!pip install wandb -Uq<br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">import</span> wandb<br><br>wandb.login()<br></code></pre></td></tr></tbody></table></figure><h3 id="step-1.-define-the-sweep">Step 1️⃣. Define the Sweep</h3><p>从根本上说，Sweep将尝试一堆超参数值的策略与评估它们的代码结合在一起。您只需要以<a href="https://docs.wandb.com/sweeps/configuration">配置</a>的形式定义您的策略。</p><p>当您像这样在笔记本中设置 Sweep时，该配置对象是一个嵌套字典。当您通过命令行运行 Sweep时，配置对象是一个 <a href="https://docs.wandb.com/sweeps/quickstart#2-sweep-config">YAMLfile</a>。</p><p>让我们一起了解 Sweep配置的定义。我们会慢慢来，这样我们就有机会解释每个组件。在典型的 Sweep管道中，此步骤将在单个分配中完成。</p><h4 id="pick-a-method">Pick a <code>method</code></h4><p>我们需要定义的第一件事是选择新参数值的 <code>method</code>。</p><p>我们提供以下搜索 <code>methods</code>：</p><ul><li><strong><code>grid</code> Search </strong>–迭代超参数值的每个组合。非常有效，但计算量大。</li><li><strong><code>random</code> Search</strong> – 根据提供的<code>distribution</code> 随机选择每个新组合。出乎意料的有效！</li><li><strong><code>bayesian</code> Search</strong> –创建一个度量分数作为超参数函数的概率模型，并选择具有提高度量的高概率的参数。适用于少量连续参数但扩展性差。</li></ul><p><code>random</code> 方法：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs python">sweep_config = {<br>    <span class="hljs-string">'method'</span>: <span class="hljs-string"><code class="language-hljs python">sweep_config = {<br>    <span class="hljs-string">'method'</span>: <span class="hljs-string">'random'</span><br>    }<br></code></pre></td></tr></tbody></table></figure><p>对于 <code>bayesian</code> Sweeps，您还需要告诉我们一些关于您的metric的信息。我们需要知道它的名称，以便我们可以在模型输出中找到它，我们需要知道您的目标是最小化它（例如，如果它是squared error）还是最大化它（例如，如果它是 accuracy）。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs python">metric = {<br>    <span class="hljs-string">'name'</span>: <span class="hljs-string">'loss'</span>,<br>    <span class="hljs-string">'goal'</span>: <span class="hljs-string">'minimize'</span>   <br>    }<br><br>sweep_config[<span class="hljs-string"><code class="language-hljs python">metric = {<br>    <span class="hljs-string">'name'</span>: <span class="hljs-string">'loss'</span>,<br>    <span class="hljs-string">'goal'</span>: <span class="hljs-string">'minimize'</span>   <br>    }<br><br>sweep_config[<span class="hljs-string">'metric'</span>] = metric<br></code></pre></td></tr></tbody></table></figure><p>如果您没有运行 <code>bayesian</code>Sweep，则不必这样做，但无论如何将其包含在您的 <code>sweep_config</code>中并不是一个坏主意，以防您以后改变主意。记录这样的事情也是很好的再现性实践，以防万一您或其他人在6 个月或 6 年后回到您的 Sweep 并且不知道 <code>val_G_batch</code>应该是高还是低。</p><h4 id="name-the-hyperparameters">Name thehyper<code>parameters</code></h4><p>一旦您选择了一种 <code>method</code>来尝试超参数的新值，您需要定义这些 <code>parameters</code>是什么</p><p>大多数时候，这一步很简单：您只需为 <code>parameter</code>命名并指定参数的合法 <code>values</code> 列表。</p><p>例如，当我们为我们的网络选择 <code>optimizer</code>时，只有有限数量的选项。在这里，我们坚持使用两个最受欢迎的选择，<code>adam</code>和<code>sgd</code>。即使对于具有潜在无限选项的超参数，通常也只尝试几个选择<code>values</code> 才有意义，就像我们在此处对隐藏层<code>layer_size</code> 和 <code>dropout</code> 所做的那样。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre class=" language-hljs python">parameters_dict = {<br>    <span class="hljs-string">'optimizer'</span>: {<br>        <span class="hljs-string">'values'</span>: [<span class="hljs-string">'adam'</span>, <span class="hljs-string">'sgd'</span>]<br>        },<br>    <span class="hljs-string">'fc_layer_size'</span>: {<br>        <span class="hljs-string">'values'</span>: [<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>]<br>        },<br>    <span class="hljs-string">'dropout'</span>: {<br>          <span class="hljs-string">'values'</span>: [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>]<br>        },<br>    }<br><br>sweep_config[<span class="hljs-string"><code class="language-hljs python">parameters_dict = {<br>    <span class="hljs-string">'optimizer'</span>: {<br>        <span class="hljs-string">'values'</span>: [<span class="hljs-string">'adam'</span>, <span class="hljs-string">'sgd'</span>]<br>        },<br>    <span class="hljs-string">'fc_layer_size'</span>: {<br>        <span class="hljs-string">'values'</span>: [<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>]<br>        },<br>    <span class="hljs-string">'dropout'</span>: {<br>          <span class="hljs-string">'values'</span>: [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>]<br>        },<br>    }<br><br>sweep_config[<span class="hljs-string">'parameters'</span>] = parameters_dict<br></code></pre></td></tr></tbody></table></figure><p>通常情况下，有些超参数我们不想在此 Sweep 中改变，但我们仍希望在我们的<code>sweep_config</code> 中设置它们。</p><p>在那种情况下，我们直接设置 <code>value</code>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre class=" language-hljs python">parameters_dict.update({<br>    <span class="hljs-string">'epochs'</span>: {<br>        <span class="hljs-string">'value'</span>: <span class="hljs-number"><code class="language-hljs python">parameters_dict.update({<br>    <span class="hljs-string">'epochs'</span>: {<br>        <span class="hljs-string">'value'</span>: <span class="hljs-number">1</span>}<br>    })<br></code></pre></td></tr></tbody></table></figure><p>对于 <code>grid</code> 搜索，这就是您所需要的。</p><p>对于 <code>random</code> 搜索，参数的所有 <code>values</code>在给定运行中被选择的可能性相同。</p><p>如果这样做不行，您可以改为指定命名 <code>distribution</code>及其参数，例如 <code>normal</code> 分布的均值 <code>mu</code> 和标准差<code>sigma</code>。</p><p>在<a href="https://docs.wandb.com/sweeps/configuration#distributions">此处</a>查看有关如何设置随机变量分布的更多信息。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre class=" language-hljs python">parameters_dict.update({<br>    <span class="hljs-string">'learning_rate'</span>: {<br>        <span class="hljs-comment"># a flat distribution between 0 and 0.1</span><br>        <span class="hljs-string">'distribution'</span>: <span class="hljs-string">'uniform'</span>,<br>        <span class="hljs-string">'min'</span>: <span class="hljs-number">0</span>,<br>        <span class="hljs-string">'max'</span>: <span class="hljs-number">0.1</span><br>      },<br>    <span class="hljs-string">'batch_size'</span>: {<br>        <span class="hljs-comment"># integers between 32 and 256</span><br>        <span class="hljs-comment"># with evenly-distributed logarithms </span><br>        <span class="hljs-string">'distribution'</span>: <span class="hljs-string">'q_log_uniform_values'</span>,<br>        <span class="hljs-string">'q'</span>: <span class="hljs-number">8</span>,<br>        <span class="hljs-string">'min'</span>: <span class="hljs-number">32</span>,<br>        <span class="hljs-string">'max'</span>: <span class="hljs-number"><code class="language-hljs python">parameters_dict.update({<br>    <span class="hljs-string">'learning_rate'</span>: {<br>        <span class="hljs-comment"># a flat distribution between 0 and 0.1</span><br>        <span class="hljs-string">'distribution'</span>: <span class="hljs-string">'uniform'</span>,<br>        <span class="hljs-string">'min'</span>: <span class="hljs-number">0</span>,<br>        <span class="hljs-string">'max'</span>: <span class="hljs-number">0.1</span><br>      },<br>    <span class="hljs-string">'batch_size'</span>: {<br>        <span class="hljs-comment"># integers between 32 and 256</span><br>        <span class="hljs-comment"># with evenly-distributed logarithms </span><br>        <span class="hljs-string">'distribution'</span>: <span class="hljs-string">'q_log_uniform_values'</span>,<br>        <span class="hljs-string">'q'</span>: <span class="hljs-number">8</span>,<br>        <span class="hljs-string">'min'</span>: <span class="hljs-number">32</span>,<br>        <span class="hljs-string">'max'</span>: <span class="hljs-number">256</span>,<br>      }<br>    })<br></code></pre></td></tr></tbody></table></figure><p>当我们完成后，<code>sweep_config</code>是一个嵌套的字典，它准确地指定了我们有兴趣尝试哪些<code>parameters</code> 以及我们将使用什么 <code>method</code>来尝试它们。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">import</span> pprint<br><br>pprint.pprint(sweep_config)<br></code></pre></td></tr></tbody></table></figure><p>但这不是所有的配置选项！</p><p>例如，我们还提供了使用 <a href="https://arxiv.org/pdf/1603.06560.pdf">HyperBand</a> 调度算法<code>early_terminate</code> 运行的选项。在<a href="https://docs.wandb.com/sweeps/configuration#stopping-criteria">这里</a>查看更多。</p><p>您可以在<a href="https://docs.wandb.com/library/sweeps/configuration">此处</a>找到所有配置选项的列表，并在<a href="https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion">此处</a>找到大量YAML 格式的示例。</p><h3 id="step-2.-initialize-the-sweep">Step 2️⃣. Initialize the Sweep</h3><p>一旦您定义了搜索策略，就该设置一些东西来实现它了。</p><p>负责我们 Sweep 的 clockwork taskmaster 被称为 <em>SweepController</em>。每次运行完成时，它将发出一组新的指令来描述要执行的新运行。这些指令由实际执行运行的<em>agents</em> 获取。</p><p>在典型的 Sweep 中，Controller位于我们的机器上，而完成运行的代理位于您的机器上，如下图所示。这种分工使得只需添加更多机器来运行代理就可以非常容易地扩展Sweeps！</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedzlbw3vQ.png" alt="sweeps-diagram"><figcaption aria-hidden="true">sweeps-diagram</figcaption></figure><p>我们可以通过使用适当的 <code>sweep_config</code> 和<code>project</code> 名称调用 <code>wandb.sweep</code> 来结束 SweepController。</p><p>此函数返回一个 <code>sweep_id</code>，我们稍后将使用它来将 agents分配给此 Controller。</p><p>旁注：在命令行上，此功能被替换为</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">wandb sweep config.yaml<br></code></pre></td></tr></tbody></table></figure><p><a href="https://docs.wandb.com/sweeps/quickstart">了解更多关于在命令行中使用Sweeps ➡</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs python">sweep_id = wandb.sweep(sweep_config, project=<span class="hljs-string"><code class="language-hljs python">sweep_id = wandb.sweep(sweep_config, project=<span class="hljs-string">"pytorch-sweeps-demo"</span>)<br></code></pre></td></tr></tbody></table></figure><h3 id="step-3.-run-the-sweep-agent">Step 3️⃣. Run the Sweep agent</h3><h4 id="define-your-training-procedure">Define Your TrainingProcedure</h4><p>在我们实际执行 sweep 之前，我们需要定义使用这些值的训练过程。</p><p>在下面的函数中，我们在 PyTorch中定义了一个简单的全连接神经网络，并添加了以下 <code>wandb</code>工具来记录模型指标、可视化性能和输出并跟踪我们的实验：</p><ul><li><a href="https://docs.wandb.com/library/init"><strong><code>wandb.init()</code></strong></a>– 初始化新的 W&amp;B 运行。每次运行都是训练功能的一次执行。</li><li><a href="https://docs.wandb.com/library/config"><strong><code>wandb.config</code></strong></a>– 将所有超参数保存在配置对象中，以便记录它们。在<a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb">此处</a>阅读有关如何使用<code>wandb.config</code> 的更多信息。</li><li><a href="https://docs.wandb.com/library/log"><strong><code>wandb.log()</code></strong></a>– 将模型行为记录到 W&amp;B。在这里，我们只记录性能；有关可以使用<code>wandb.log</code> 记录的所有其他富媒体，请参阅此 <a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_(Almost)_Anything_with_W%26B_Media.ipynb">Colab</a>。</li></ul><p>有关使用 PyTorch 检测 W&amp;B 的更多详细信息，请参阅此 <a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb">Colab</a>。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><br>device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-comment"># Initialize a new wandb run</span><br>    <span class="hljs-keyword">with</span> wandb.init(config=config):<br>        <span class="hljs-comment"># If called by wandb.agent, as below,</span><br>        <span class="hljs-comment"># this config will be set by Sweep Controller</span><br>        config = wandb.config<br><br>        loader = build_dataset(config.batch_size)<br>        network = build_network(config.fc_layer_size, config.dropout)<br>        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)<br><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>            avg_loss = train_epoch(network, loader, optimizer)<br>            wandb.log({<span class="hljs-string">"loss"</span>: avg_loss, <span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><br>device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-comment"># Initialize a new wandb run</span><br>    <span class="hljs-keyword">with</span> wandb.init(config=config):<br>        <span class="hljs-comment"># If called by wandb.agent, as below,</span><br>        <span class="hljs-comment"># this config will be set by Sweep Controller</span><br>        config = wandb.config<br><br>        loader = build_dataset(config.batch_size)<br>        network = build_network(config.fc_layer_size, config.dropout)<br>        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)<br><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>            avg_loss = train_epoch(network, loader, optimizer)<br>            wandb.log({<span class="hljs-string">"loss"</span>: avg_loss, <span class="hljs-string">"epoch"</span>: epoch})           <br></code></pre></td></tr></tbody></table></figure><p>这个单元格定义了我们训练过程的四个部分：<code>build_dataset</code>,<code>build_network</code>, <code>build_optimizer</code>和<code>train_epoch</code>.</p><p>所有这些都是基本 PyTorch 管道的标准部分，它们的实现不受使用 W&amp;B的影响，因此我们不会对它们发表评论。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_dataset</span>(<span class="hljs-params">batch_size</span>):<br>   <br>    transform = transforms.Compose(<br>        [transforms.ToTensor(),<br>         transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))])<br>    <span class="hljs-comment"># download MNIST training dataset</span><br>    dataset = datasets.MNIST(<span class="hljs-string">"."</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>,<br>                             transform=transform)<br>    sub_dataset = torch.utils.data.Subset(<br>        dataset, indices=<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">5</span>))<br>    loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size)<br><br>    <span class="hljs-keyword">return</span> loader<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_network</span>(<span class="hljs-params">fc_layer_size, dropout</span>):<br>    network = nn.Sequential(  <span class="hljs-comment"># fully-connected, single hidden layer</span><br>        nn.Flatten(),<br>        nn.Linear(<span class="hljs-number">784</span>, fc_layer_size), nn.ReLU(),<br>        nn.Dropout(dropout),<br>        nn.Linear(fc_layer_size, <span class="hljs-number">10</span>),<br>        nn.LogSoftmax(dim=<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">return</span> network.to(device)<br>        <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_optimizer</span>(<span class="hljs-params">network, optimizer, learning_rate</span>):<br>    <span class="hljs-keyword">if</span> optimizer == <span class="hljs-string">"sgd"</span>:<br>        optimizer = optim.SGD(network.parameters(),<br>                              lr=learning_rate, momentum=<span class="hljs-number">0.9</span>)<br>    <span class="hljs-keyword">elif</span> optimizer == <span class="hljs-string">"adam"</span>:<br>        optimizer = optim.Adam(network.parameters(),<br>                               lr=learning_rate)<br>    <span class="hljs-keyword">return</span> optimizer<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch</span>(<span class="hljs-params">network, loader, optimizer</span>):<br>    cumu_loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> _, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>        data, target = data.to(device), target.to(device)<br>        optimizer.zero_grad()<br><br>        <span class="hljs-comment"># ➡ Forward pass</span><br>        loss = F.nll_loss(network(data), target)<br>        cumu_loss += loss.item()<br><br>        <span class="hljs-comment"># ⬅ Backward pass + weight update</span><br>        loss.backward()<br>        optimizer.step()<br><br>        wandb.log({<span class="hljs-string">"batch loss"</span>: loss.item()})<br><br>    <span class="hljs-keyword">return</span> cumu_loss / <span class="hljs-built_in"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_dataset</span>(<span class="hljs-params">batch_size</span>):<br>   <br>    transform = transforms.Compose(<br>        [transforms.ToTensor(),<br>         transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))])<br>    <span class="hljs-comment"># download MNIST training dataset</span><br>    dataset = datasets.MNIST(<span class="hljs-string">"."</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>,<br>                             transform=transform)<br>    sub_dataset = torch.utils.data.Subset(<br>        dataset, indices=<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">5</span>))<br>    loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size)<br><br>    <span class="hljs-keyword">return</span> loader<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_network</span>(<span class="hljs-params">fc_layer_size, dropout</span>):<br>    network = nn.Sequential(  <span class="hljs-comment"># fully-connected, single hidden layer</span><br>        nn.Flatten(),<br>        nn.Linear(<span class="hljs-number">784</span>, fc_layer_size), nn.ReLU(),<br>        nn.Dropout(dropout),<br>        nn.Linear(fc_layer_size, <span class="hljs-number">10</span>),<br>        nn.LogSoftmax(dim=<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">return</span> network.to(device)<br>        <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_optimizer</span>(<span class="hljs-params">network, optimizer, learning_rate</span>):<br>    <span class="hljs-keyword">if</span> optimizer == <span class="hljs-string">"sgd"</span>:<br>        optimizer = optim.SGD(network.parameters(),<br>                              lr=learning_rate, momentum=<span class="hljs-number">0.9</span>)<br>    <span class="hljs-keyword">elif</span> optimizer == <span class="hljs-string">"adam"</span>:<br>        optimizer = optim.Adam(network.parameters(),<br>                               lr=learning_rate)<br>    <span class="hljs-keyword">return</span> optimizer<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch</span>(<span class="hljs-params">network, loader, optimizer</span>):<br>    cumu_loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> _, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>        data, target = data.to(device), target.to(device)<br>        optimizer.zero_grad()<br><br>        <span class="hljs-comment"># ➡ Forward pass</span><br>        loss = F.nll_loss(network(data), target)<br>        cumu_loss += loss.item()<br><br>        <span class="hljs-comment"># ⬅ Backward pass + weight update</span><br>        loss.backward()<br>        optimizer.step()<br><br>        wandb.log({<span class="hljs-string">"batch loss"</span>: loss.item()})<br><br>    <span class="hljs-keyword">return</span> cumu_loss / <span class="hljs-built_in">len</span>(loader)<br></code></pre></td></tr></tbody></table></figure><p>现在，我们准备开始 sweeping 了！</p><p>Sweep Controllers，就像我们通过运行 <code>wandb.sweep</code>制作的控制器一样，坐等有人要求他们提供 <code>config</code>来试用。</p><p>某人是 <code>agent</code>，他们是用 <code>wandb.agent</code>创建的。要开始，agent 只需要知道</p><ol type="1"><li>它是 (<code>sweep_id</code>) 的一部分</li><li>它应该运行哪个函数（这里是 <code>train</code>）</li><li>（可选）有多少配置要求控制器（<code>count</code>）</li></ol><p>仅供参考，您可以在不同的计算资源上启动具有相同 <code>sweep_id</code>的多个 <code>agent</code>，Controller 将确保它们根据<code>sweep_config</code>中制定的策略协同工作。这使得在尽可能多的节点上扩展 Sweeps变得轻而易举！</p><p>旁注：在命令行上，此功能被替换为</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">wandb agent sweep_id<br></code></pre></td></tr></tbody></table></figure><p><a href="https://docs.wandb.com/sweeps/quickstart">了解更多关于在命令行中使用Sweeps ➡</a></p><p>下面的单元格将启动一个运行 <code>train</code> 5 次的<code>agent</code>，使用 Sweep Controller返回的随机生成的超参数值。执行时间不到 5 分钟。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs python">wandb.agent(sweep_id, train, count=<span class="hljs-number"><code class="language-hljs python">wandb.agent(sweep_id, train, count=<span class="hljs-number">5</span>)<br></code></pre></td></tr></tbody></table></figure><h3 id="visualize-sweep-results">Visualize Sweep Results</h3><h4 id="parallel-coordinates-plot">Parallel Coordinates Plot</h4><p>此图将超参数值映射到模型指标。它对于磨练导致最佳模型性能的超参数组合很有用。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined5e190366778ad831455f9af2_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695138341_image.png" alt="hyperparameters map to metrics"><figcaption aria-hidden="true">hyperparameters map tometrics</figcaption></figure><h4 id="hyperparameter-importance-plot">Hyperparameter ImportancePlot</h4><p>超参数重要性图表明哪些超参数是指标的最佳预测因子。我们报告特征重要性（来自随机森林模型）和相关性（隐式线性模型）。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined5e190367778ad820b35f9af5_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695757573_image.png" alt="parameter importance"><figcaption aria-hidden="true">parameter importance</figcaption></figure><p>这些可视化可以通过磨练最重要的参数（和值范围）来帮助您节省运行昂贵的超参数优化的时间和资源，因此值得进一步探索。</p><h3 id="get-your-hands-dirty-with-sweeps">Get your hands dirty withsweeps</h3><p>我们创建了一个简单的训练脚本和一些 <a href="https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion">sweepconfigs</a> 风格供您使用。我们强烈建议您尝试一下。</p><p>该存储库还提供了一些示例，可帮助您尝试更高级的 sweep 功能，例如 <a href="https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/us0ifmrf?workspace=user-lavanyashukla&amp;_gl=1*1h57q6p*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjYyMjIyOS43LjAuMTY4NjYyMjIyOS42MC4wLjA.">BayesianHyperband</a> 和 <a href="https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/xbs2wm5e?workspace=user-lavanyashukla&amp;_gl=1*5hk37j*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjYyMjIyOS43LjAuMTY4NjYyMjIyOS42MC4wLjA.">Hyperopt</a>。</p><h2 id="track-models-and-datasets">Track models and datasets</h2><p><a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W&amp;B_Artifacts.ipynb">在此处试用Colab Notebook →</a></p><p>在此笔记本中，我们将向您展示如何使用 W&amp;B Artifacts 跟踪您的 ML实验管道。</p><h4 id="follow-along-with-a-video-tutorial-1">Follow along with a <a href="http://tiny.cc/wb-artifacts-video">video tutorial</a>!</h4><h5 id="what-are-artifacts-and-why-should-i-care">What are Artifacts andWhy Should I Care?</h5><p>“artifact”，如希腊双耳瓶🏺，是一个生产的对象——一个过程的输出。在 ML中，最重要的工件是 <em>datasets</em> 和 <em>models</em>。</p><p>而且，就像 <a href="https://indianajones.fandom.com/wiki/Cross_of_Coronado">Cross ofCoronado</a>一样，这些重要的文物属于博物馆！也就是说，应该对它们进行分类和组织，以便您、您的团队和整个ML 社区可以向它们学习。毕竟，那些不跟踪训练的人注定要重蹈覆辙。</p><p>使用我们的 Artifacts API，您可以将 <code>Artifacts</code> 记录为W&amp;B <code>Runs</code> 的输出，或使用 <code>Artifacts</code> 作为<code>Runs</code>的输入，如此图所示，其中训练运行接受数据集并生成模型。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedsimple%20artifact%20diagram%202.png" alt="simple artifact diagram"><figcaption aria-hidden="true">simple artifact diagram</figcaption></figure><p>由于一次运行可以使用另一次的输出作为输入，因此 Artifacts 和 Runs一起形成了一个有向图——实际上是一个二分 <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a>！ --带有 <code>Artifact</code>s 和 <code>Run</code>s 的节点，以及将<code>Run</code>s 连接到它们消耗或生产的 <code>Artifact</code>s的箭头。</p><h3 id="install-and-import">0️⃣ Install and Import</h3><p>Artifacts 是我们 Python 库的一部分，从 <code>0.9.2</code>版开始。</p><p>与 ML Python 堆栈的大多数部分一样，它可以通过 <code>pip</code>获得。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-comment"><code class="language-hljs python"><span class="hljs-comment"># Compatible with wandb version 0.9.2+</span><br>!pip install wandb -qqq<br>!apt install tree<br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> wandb<br></code></pre></td></tr></tbody></table></figure><h3 id="log-a-dataset">1️⃣ Log a Dataset</h3><p>首先，让我们定义一些 Artifacts。</p><p>此示例基于此 PyTorch <a href="https://github.com/pytorch/examples/tree/master/mnist/">"BasicMNIST Example"</a>，但可以在 <a href="http://wandb.me/artifacts-colab">TensorFlow</a>、任何其他框架或纯Python 中轻松完成。</p><p>我们从 <code>Dataset</code>s 开始：</p><ul><li>一个 <code>train</code>ing set，用于选择参数，</li><li>一个 <code>validation</code> set，用于选择超参数，</li><li>一个 <code>test</code>ing set，用于评估最终模型</li></ul><p>下面的第一个单元格定义了这三个数据集。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> random <br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset<br><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># Ensure deterministic behavior</span><br>torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>random.seed(<span class="hljs-number">0</span>)<br>torch.manual_seed(<span class="hljs-number">0</span>)<br>torch.cuda.manual_seed_all(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># Device configuration</span><br>device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<br><br><span class="hljs-comment"># Data parameters</span><br>num_classes = <span class="hljs-number">10</span><br>input_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br><br><span class="hljs-comment"># drop slow mirror from list of MNIST mirrors</span><br>torchvision.datasets.MNIST.mirrors = [mirror <span class="hljs-keyword">for</span> mirror <span class="hljs-keyword">in</span> torchvision.datasets.MNIST.mirrors<br>                                      <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> mirror.startswith(<span class="hljs-string">"http://yann.lecun.com"</span>)]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load</span>(<span class="hljs-params">train_size=<span class="hljs-number">50_000</span></span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    # Load the data</span><br><span class="hljs-string">    """</span><br><br>    <span class="hljs-comment"># the data, split between train and test sets</span><br>    train = torchvision.datasets.MNIST(<span class="hljs-string">"./"</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>)<br>    test = torchvision.datasets.MNIST(<span class="hljs-string">"./"</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>)<br>    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)<br><br>    <span class="hljs-comment"># split off a validation set for hyperparameter tuning</span><br>    x_train, x_val = x_train[:train_size], x_train[train_size:]<br>    y_train, y_val = y_train[:train_size], y_train[train_size:]<br><br>    training_set = TensorDataset(x_train, y_train)<br>    validation_set = TensorDataset(x_val, y_val)<br>    test_set = TensorDataset(x_test, y_test)<br><br>    datasets = [training_set, validation_set, test_set]<br><br>    <span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">import</span> random <br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset<br><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># Ensure deterministic behavior</span><br>torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>random.seed(<span class="hljs-number">0</span>)<br>torch.manual_seed(<span class="hljs-number">0</span>)<br>torch.cuda.manual_seed_all(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># Device configuration</span><br>device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<br><br><span class="hljs-comment"># Data parameters</span><br>num_classes = <span class="hljs-number">10</span><br>input_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br><br><span class="hljs-comment"># drop slow mirror from list of MNIST mirrors</span><br>torchvision.datasets.MNIST.mirrors = [mirror <span class="hljs-keyword">for</span> mirror <span class="hljs-keyword">in</span> torchvision.datasets.MNIST.mirrors<br>                                      <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> mirror.startswith(<span class="hljs-string">"http://yann.lecun.com"</span>)]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load</span>(<span class="hljs-params">train_size=<span class="hljs-number">50_000</span></span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    # Load the data</span><br><span class="hljs-string">    """</span><br><br>    <span class="hljs-comment"># the data, split between train and test sets</span><br>    train = torchvision.datasets.MNIST(<span class="hljs-string">"./"</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>)<br>    test = torchvision.datasets.MNIST(<span class="hljs-string">"./"</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>)<br>    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)<br><br>    <span class="hljs-comment"># split off a validation set for hyperparameter tuning</span><br>    x_train, x_val = x_train[:train_size], x_train[train_size:]<br>    y_train, y_val = y_train[:train_size], y_train[train_size:]<br><br>    training_set = TensorDataset(x_train, y_train)<br>    validation_set = TensorDataset(x_val, y_val)<br>    test_set = TensorDataset(x_test, y_test)<br><br>    datasets = [training_set, validation_set, test_set]<br><br>    <span class="hljs-keyword">return</span> datasets<br></code></pre></td></tr></tbody></table></figure><p>这建立了一个模式，我们将在这个例子中看到重复：将数据记录为工件的代码包裹在生成该数据的代码周围。在这种情况下，用于<code>load</code>ing 数据的代码与用于 <code>load_and_log</code>ging数据的代码分开。</p><p>这是很好的做法！</p><p>为了将这些数据集记录为工件，我们只需要</p><ol type="1"><li>使用 <code>wandb.init</code> 创建 <code>Run</code>，(L4)</li><li>为数据集 (L10) 创建一个 <code>Artifact</code>，以及</li><li>保存并记录相关 <code>file</code>s（L20、L23）。</li></ol><p>查看下面代码单元的示例，然后展开后面的部分以了解更多详细信息。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_and_log</span>():<br><br>    <span class="hljs-comment"># 🚀 start a run, with a type to label it and a project it can call home</span><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"load-data"</span>) <span class="hljs-keyword">as</span> run:<br>        <br>        datasets = load()  <span class="hljs-comment"># separate code for loading the datasets</span><br>        names = [<span class="hljs-string">"training"</span>, <span class="hljs-string">"validation"</span>, <span class="hljs-string">"test"</span>]<br><br>        <span class="hljs-comment"># 🏺 create our Artifact</span><br>        raw_data = wandb.Artifact(<br>            <span class="hljs-string">"mnist-raw"</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">"dataset"</span>,<br>            description=<span class="hljs-string">"Raw MNIST dataset, split into train/val/test"</span>,<br>            metadata={<span class="hljs-string">"source"</span>: <span class="hljs-string">"torchvision.datasets.MNIST"</span>,<br>                      <span class="hljs-string">"sizes"</span>: [<span class="hljs-built_in">len</span>(dataset) <span class="hljs-keyword">for</span> dataset <span class="hljs-keyword">in</span> datasets]})<br><br>        <span class="hljs-keyword">for</span> name, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(names, datasets):<br>            <span class="hljs-comment"># 🐣 Store a new file in the artifact, and write something into its contents.</span><br>            <span class="hljs-keyword">with</span> raw_data.new_file(name + <span class="hljs-string">".pt"</span>, mode=<span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> file:<br>                x, y = data.tensors<br>                torch.save((x, y), file)<br><br>        <span class="hljs-comment"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_and_log</span>():<br><br>    <span class="hljs-comment"># 🚀 start a run, with a type to label it and a project it can call home</span><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"load-data"</span>) <span class="hljs-keyword">as</span> run:<br>        <br>        datasets = load()  <span class="hljs-comment"># separate code for loading the datasets</span><br>        names = [<span class="hljs-string">"training"</span>, <span class="hljs-string">"validation"</span>, <span class="hljs-string">"test"</span>]<br><br>        <span class="hljs-comment"># 🏺 create our Artifact</span><br>        raw_data = wandb.Artifact(<br>            <span class="hljs-string">"mnist-raw"</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">"dataset"</span>,<br>            description=<span class="hljs-string">"Raw MNIST dataset, split into train/val/test"</span>,<br>            metadata={<span class="hljs-string">"source"</span>: <span class="hljs-string">"torchvision.datasets.MNIST"</span>,<br>                      <span class="hljs-string">"sizes"</span>: [<span class="hljs-built_in">len</span>(dataset) <span class="hljs-keyword">for</span> dataset <span class="hljs-keyword">in</span> datasets]})<br><br>        <span class="hljs-keyword">for</span> name, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(names, datasets):<br>            <span class="hljs-comment"># 🐣 Store a new file in the artifact, and write something into its contents.</span><br>            <span class="hljs-keyword">with</span> raw_data.new_file(name + <span class="hljs-string">".pt"</span>, mode=<span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> file:<br>                x, y = data.tensors<br>                torch.save((x, y), file)<br><br>        <span class="hljs-comment"># ✍️ Save the artifact to W&B.</span><br>        run.log_artifact(raw_data)<br><br>load_and_log()<br></code></pre></td></tr></tbody></table></figure><h4 id="wandb.init">🚀 <code>wandb.init</code></h4><p>当我们制作将要产生 <code>Artifact</code>s 的<code>Run</code>时，我们需要说明它属于哪个 <code>project</code>。</p><p>根据您的工作流程，项目可能大到<code>car-that-drives-itself</code>，也可能小到<code>iterative-architecture-experiment-117</code>。</p><p>Depending on your workflow, a project might be as big as<code>car-that-drives-itself</code> or as small as<code>iterative-architecture-experiment-117</code>.</p><blockquote><p>👍规则：如果可以，请将所有共享 <code>Artifact</code>s 的<code>Run</code>s 保留在一个项目中。这使事情变得简单，但不要担心<code>Artifact</code>s 可以跨项目移植！</p></blockquote><p>为了帮助跟踪您可能运行的所有不同类型的作业，在进行 <code>Run</code>s时提供 <code>job_type</code> 很有用。这可以使您的 Artifacts图表保持整洁。</p><blockquote><p>👍规则：<code>job_type</code>应该是描述性的，并且对应于你的管道的单个步骤。在这里，我们将<code>load</code>ing 数据与 <code>preprocess</code>ing 数据分开。</p></blockquote><h4 id="wandb.artifact">🏺 <code>wandb.Artifact</code></h4><p>要将某物记录为 <code>Artifact</code>，我们必须首先创建一个<code>Artifact</code> 对象。</p><p>每个 <code>Artifact</code> 都有一个<code>name</code>——这是第一个参数设置的名称。</p><blockquote><p>👍的规则：<code>name</code>应该是描述性的，但易于记忆和输入——我们喜欢使用连字符分隔的名称，并与代码中的变量名相对应。</p></blockquote><p>它也有一个 <code>type</code>。就像 <code>Run</code>s 的<code>job_types</code> 一样，它用于组织 <code>Run</code>s 和<code>Artifact</code>s 的图表。</p><blockquote><p>👍的规则：<code>type</code> 应该简单：比<code>mnist-data-YYYYMMDD</code> 更像 <code>dataset</code> 或<code>model</code>。</p></blockquote><p>您还可以附加 <code>description</code> 和一些<code>metadata</code>，作为字典。<code>metadata</code> 只需要可序列化为JSON。</p><blockquote><p>👍规则：<code>metadata</code>应尽可能具有描述性。</p></blockquote><h4 id="artifact.new_file-and-run.log_artifact">🐣<code>artifact.new_file</code> and ✍️ <code>run.log_artifact</code></h4><p>一旦我们创建了一个 <code>Artifact</code>对象，我们需要向它添加文件。</p><p>您没看错：带有 s 的 <em>files</em>。<code>Artifact</code>s的结构类似于目录，包含文件和子目录。</p><blockquote><p>👍规则：只要有必要，将 <code>Artifact</code>的内容拆分为多个文件。如果需要扩展，这将有所帮助！</p></blockquote><p>我们使用 <code>new_file</code> 方法同时写入文件并将其附加到<code>Artifact</code>。下面，我们将使用 <code>add_file</code>方法，它将这两个步骤分开</p><p>添加完所有文件后，我们需要将 <code>log_artifact</code> 添加到 <a href="https://wandb.ai/?_gl=1*r07jdw*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjY1MzgzNC45LjEuMTY4NjY1MzgzOS41NS4wLjA.">wandb.ai</a>。</p><p>您会注意到一些 URL 出现在输出中，包括一个用于运行页面的URL。您可以在此处查看 <code>Run</code> 结果，包括已记录的任何<code>Artifact</code>s。</p><p>我们将在下面看到一些示例，这些示例可以更好地利用“运行”页面的其他组件。</p><h3 id="use-a-logged-dataset-artifact">2️⃣ Use a Logged DatasetArtifact</h3><p>与博物馆中的 artifacts 不同，W&amp;B 中的 <code>Artifact</code>s旨在使用，而不仅仅是存储。</p><p>让我们看看它是什么样的。</p><p>下面的单元格定义了一个管道步骤，该步骤接收原始数据集并使用它来生成<code>preprocess</code>ed 数据集：<code>normalize</code>d和正确整形。</p><p>再次注意，我们从与 <code>wandb</code>接口的代码中分离出了代码的主体，即 <code>preprocess</code>。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess</span>(<span class="hljs-params">dataset, normalize=<span class="hljs-literal">True</span>, expand_dims=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    ## Prepare the data</span><br><span class="hljs-string">    """</span><br>    x, y = dataset.tensors<br><br>    <span class="hljs-keyword">if</span> normalize:<br>        <span class="hljs-comment"># Scale images to the [0, 1] range</span><br>        x = x.<span class="hljs-built_in">type</span>(torch.float32) / <span class="hljs-number">255</span><br><br>    <span class="hljs-keyword">if</span> expand_dims:<br>        <span class="hljs-comment"># Make sure images have shape (1, 28, 28)</span><br>        x = torch.unsqueeze(x, <span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess</span>(<span class="hljs-params">dataset, normalize=<span class="hljs-literal">True</span>, expand_dims=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    ## Prepare the data</span><br><span class="hljs-string">    """</span><br>    x, y = dataset.tensors<br><br>    <span class="hljs-keyword">if</span> normalize:<br>        <span class="hljs-comment"># Scale images to the [0, 1] range</span><br>        x = x.<span class="hljs-built_in">type</span>(torch.float32) / <span class="hljs-number">255</span><br><br>    <span class="hljs-keyword">if</span> expand_dims:<br>        <span class="hljs-comment"># Make sure images have shape (1, 28, 28)</span><br>        x = torch.unsqueeze(x, <span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword">return</span> TensorDataset(x, y)<br></code></pre></td></tr></tbody></table></figure><p>现在是使用 <code>wandb.Artifact</code> 日志记录这个<code>preprocess</code> 步骤的代码。</p><p>请注意，下面的示例都 <code>use</code>s 了一个新的<code>Artifact</code>，并将其 <code>log</code>s下来，这与上一步相同。<code>Artifact</code>s 既是 <code>Run</code>s的输入又是输出！</p><p>我们使用一个新的<code>job_type</code>，<code>preprocess-data</code>，来明确这是一个不同于之前的job。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_and_log</span>(<span class="hljs-params">steps</span>):<br><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"preprocess-data"</span>) <span class="hljs-keyword">as</span> run:<br><br>        processed_data = wandb.Artifact(<br>            <span class="hljs-string">"mnist-preprocess"</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">"dataset"</span>,<br>            description=<span class="hljs-string">"Preprocessed MNIST dataset"</span>,<br>            metadata=steps)<br>         <br>        <span class="hljs-comment"># ✔️ declare which artifact we'll be using</span><br>        raw_data_artifact = run.use_artifact(<span class="hljs-string">'mnist-raw:latest'</span>)<br><br>        <span class="hljs-comment"># 📥 if need be, download the artifact</span><br>        raw_dataset = raw_data_artifact.download()<br>        <br>        <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">"training"</span>, <span class="hljs-string">"validation"</span>, <span class="hljs-string">"test"</span>]:<br>            raw_split = read(raw_dataset, split)<br>            processed_dataset = preprocess(raw_split, **steps)<br><br>            <span class="hljs-keyword">with</span> processed_data.new_file(split + <span class="hljs-string">".pt"</span>, mode=<span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> file:<br>                x, y = processed_dataset.tensors<br>                torch.save((x, y), file)<br><br>        run.log_artifact(processed_data)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read</span>(<span class="hljs-params">data_dir, split</span>):<br>    filename = split + <span class="hljs-string">".pt"</span><br>    x, y = torch.load(os.path.join(data_dir, filename))<br><br>    <span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_and_log</span>(<span class="hljs-params">steps</span>):<br><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"preprocess-data"</span>) <span class="hljs-keyword">as</span> run:<br><br>        processed_data = wandb.Artifact(<br>            <span class="hljs-string">"mnist-preprocess"</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">"dataset"</span>,<br>            description=<span class="hljs-string">"Preprocessed MNIST dataset"</span>,<br>            metadata=steps)<br>         <br>        <span class="hljs-comment"># ✔️ declare which artifact we'll be using</span><br>        raw_data_artifact = run.use_artifact(<span class="hljs-string">'mnist-raw:latest'</span>)<br><br>        <span class="hljs-comment"># 📥 if need be, download the artifact</span><br>        raw_dataset = raw_data_artifact.download()<br>        <br>        <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">"training"</span>, <span class="hljs-string">"validation"</span>, <span class="hljs-string">"test"</span>]:<br>            raw_split = read(raw_dataset, split)<br>            processed_dataset = preprocess(raw_split, **steps)<br><br>            <span class="hljs-keyword">with</span> processed_data.new_file(split + <span class="hljs-string">".pt"</span>, mode=<span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> file:<br>                x, y = processed_dataset.tensors<br>                torch.save((x, y), file)<br><br>        run.log_artifact(processed_data)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read</span>(<span class="hljs-params">data_dir, split</span>):<br>    filename = split + <span class="hljs-string">".pt"</span><br>    x, y = torch.load(os.path.join(data_dir, filename))<br><br>    <span class="hljs-keyword">return</span> TensorDataset(x, y)<br></code></pre></td></tr></tbody></table></figure><p>这里要注意的一件事是预处理的 <code>steps</code> 作为<code>metadata</code> 与 <code>preprocessed_data</code> 一起保存。</p><p>如果您想让您的实验可重现，捕获大量元数据是个好主意！</p><p>此外，即使我们的数据集是"<code>large artifact</code>"，<code>download</code>步骤也可以在不到一秒的时间内完成。</p><p>展开下面的 markdown 单元格以了解详细信息。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre class=" language-hljs python">steps = {<span class="hljs-string">"normalize"</span>: <span class="hljs-literal">True</span>,<br>         <span class="hljs-string">"expand_dims"</span>: <span class="hljs-literal"><code class="language-hljs python">steps = {<span class="hljs-string">"normalize"</span>: <span class="hljs-literal">True</span>,<br>         <span class="hljs-string">"expand_dims"</span>: <span class="hljs-literal">True</span>}<br><br>preprocess_and_log(steps)<br></code></pre></td></tr></tbody></table></figure><h4 id="run.use_artifact">✔️ <code>run.use_artifact</code></h4><p>这些步骤比较简单。消费者只需要知道 <code>Artifact</code>的<code>name</code>，再加上 bit more。</p><p>“bit more” 是您想要的 <code>Artifact</code> 的特定版本的<code>alias</code>。</p><p>默认情况下，最后上传的版本被标记为<code>latest</code>。否则，您可以选择带有<code>v0</code>/<code>v1</code>等的旧版本，或者您可以提供自己的别名，例如 <code>best</code> 或<code>jit-script</code>。就像 <a href="https://hub.docker.com/">DockerHub</a> 标签一样，别名与名称用 <code>:</code> 分隔，所以我们想要的<code>Artifact</code> 是 <code>mnist-raw:latest</code>。</p><blockquote><p>👍规则：保持别名简短而甜美。当您想要满足某些属性的<code>Artifact</code> 时，请使用自定义 <code>alias</code>es，如<code>latest</code> 或 <code>best</code></p></blockquote><h4 id="artifact.download">📥 <code>artifact.download</code></h4><p>现在，您可能正在担心 <code>download</code>调用。如果我们再下载一份，内存的负担会不会加倍？</p><p>别担心，朋友。在我们实际下载任何东西之前，我们会检查本地是否有正确的版本。使用和版本控制<code>git</code> 和 <a href="https://en.wikipedia.org/wiki/Torrent_file">torrenting</a>相同的技术：hashing。</p><p>随着 <code>Artifact</code>s 的创建和记录，工作目录中名为<code>artifacts</code> 的文件夹将开始填充子目录，每个<code>Artifact</code>一个。使用 <code>!tree artifacts</code>检查其内容：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs python"><code class="language-hljs python">!tree artifacts<br></code></pre></td></tr></tbody></table></figure><h4 id="the-artifacts-page-on-wandb.ai">🌐 The Artifacts page on <a href="https://wandb.ai/">wandb.ai</a></h4><p>现在我们已经记录并使用了一个 <code>Artifact</code>，让我们检查一下Run 页面上的 Artifacts 选项卡。</p><p>从<code>wandb</code> 输出导航到运行页面URL，然后从左侧边栏中选择“工件”选项卡（它是带有数据库图标的选项卡，看起来像三个冰球叠在一起）。</p><p>单击 "Input Artifacts" 表或 "Output Artifacts"表中的一行，然后查看选项卡（"Overview", "Metadata"）以查看记录的有关<code>Artifact</code> 的所有内容。</p><p>我们特别喜欢 "Graph View"。默认情况下，它显示一个图表，其中<code>Artifact</code>s 的 <code>type</code>s 和 <code>Run</code> 的<code>job_types</code> 是两种类型的节点，箭头代表消费和生产。</p><h3 id="log-a-model">3️⃣ Log a Model</h3><p>这足以了解 <code>Artifact</code>s 的 API如何工作，但让我们按照这个示例一直到管道的末尾，以便我们可以了解<code>Artifact</code>s 如何改进您的 ML 工作流程。</p><p>这里的第一个单元格在 PyTorch 中构建了一个 DNN<code>model</code>——一个非常简单的 ConvNet。</p><p>我们将从初始化 <code>model</code>开始，而不是训练它。这样，我们可以重复训练，同时保持其他一切不变。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> floor<br><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_layer_sizes=[<span class="hljs-number">32</span>, <span class="hljs-number">64</span>],</span><br><span class="hljs-params">                  kernel_sizes=[<span class="hljs-number">3</span>],</span><br><span class="hljs-params">                  activation=<span class="hljs-string">"ReLU"</span>,</span><br><span class="hljs-params">                  pool_sizes=[<span class="hljs-number">2</span>],</span><br><span class="hljs-params">                  dropout=<span class="hljs-number">0.5</span>,</span><br><span class="hljs-params">                  num_classes=num_classes,</span><br><span class="hljs-params">                  input_shape=input_shape</span>):<br>      <br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br><br>        self.layer1 = nn.Sequential(<br>              nn.Conv2d(in_channels=input_shape[<span class="hljs-number">0</span>], out_channels=hidden_layer_sizes[<span class="hljs-number">0</span>], kernel_size=kernel_sizes[<span class="hljs-number">0</span>]),<br>              <span class="hljs-built_in">getattr</span>(nn, activation)(),<br>              nn.MaxPool2d(kernel_size=pool_sizes[<span class="hljs-number">0</span>])<br>        )<br>        self.layer2 = nn.Sequential(<br>              nn.Conv2d(in_channels=hidden_layer_sizes[<span class="hljs-number">0</span>], out_channels=hidden_layer_sizes[-<span class="hljs-number">1</span>], kernel_size=kernel_sizes[-<span class="hljs-number">1</span>]),<br>              <span class="hljs-built_in">getattr</span>(nn, activation)(),<br>              nn.MaxPool2d(kernel_size=pool_sizes[-<span class="hljs-number">1</span>])<br>        )<br>        self.layer3 = nn.Sequential(<br>              nn.Flatten(),<br>              nn.Dropout(dropout)<br>        )<br><br>        fc_input_dims = floor((input_shape[<span class="hljs-number">1</span>] - kernel_sizes[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>) / pool_sizes[<span class="hljs-number">0</span>]) <span class="hljs-comment"># layer 1 output size</span><br>        fc_input_dims = floor((fc_input_dims - kernel_sizes[-<span class="hljs-number">1</span>] + <span class="hljs-number">1</span>) / pool_sizes[-<span class="hljs-number">1</span>]) <span class="hljs-comment"># layer 2 output size</span><br>        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-<span class="hljs-number">1</span>] <span class="hljs-comment"># layer 3 output size</span><br><br>        self.fc = nn.Linear(fc_input_dims, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.fc(x)<br>        <span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> floor<br><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_layer_sizes=[<span class="hljs-number">32</span>, <span class="hljs-number">64</span>],</span><br><span class="hljs-params">                  kernel_sizes=[<span class="hljs-number">3</span>],</span><br><span class="hljs-params">                  activation=<span class="hljs-string">"ReLU"</span>,</span><br><span class="hljs-params">                  pool_sizes=[<span class="hljs-number">2</span>],</span><br><span class="hljs-params">                  dropout=<span class="hljs-number">0.5</span>,</span><br><span class="hljs-params">                  num_classes=num_classes,</span><br><span class="hljs-params">                  input_shape=input_shape</span>):<br>      <br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br><br>        self.layer1 = nn.Sequential(<br>              nn.Conv2d(in_channels=input_shape[<span class="hljs-number">0</span>], out_channels=hidden_layer_sizes[<span class="hljs-number">0</span>], kernel_size=kernel_sizes[<span class="hljs-number">0</span>]),<br>              <span class="hljs-built_in">getattr</span>(nn, activation)(),<br>              nn.MaxPool2d(kernel_size=pool_sizes[<span class="hljs-number">0</span>])<br>        )<br>        self.layer2 = nn.Sequential(<br>              nn.Conv2d(in_channels=hidden_layer_sizes[<span class="hljs-number">0</span>], out_channels=hidden_layer_sizes[-<span class="hljs-number">1</span>], kernel_size=kernel_sizes[-<span class="hljs-number">1</span>]),<br>              <span class="hljs-built_in">getattr</span>(nn, activation)(),<br>              nn.MaxPool2d(kernel_size=pool_sizes[-<span class="hljs-number">1</span>])<br>        )<br>        self.layer3 = nn.Sequential(<br>              nn.Flatten(),<br>              nn.Dropout(dropout)<br>        )<br><br>        fc_input_dims = floor((input_shape[<span class="hljs-number">1</span>] - kernel_sizes[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>) / pool_sizes[<span class="hljs-number">0</span>]) <span class="hljs-comment"># layer 1 output size</span><br>        fc_input_dims = floor((fc_input_dims - kernel_sizes[-<span class="hljs-number">1</span>] + <span class="hljs-number">1</span>) / pool_sizes[-<span class="hljs-number">1</span>]) <span class="hljs-comment"># layer 2 output size</span><br>        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-<span class="hljs-number">1</span>] <span class="hljs-comment"># layer 3 output size</span><br><br>        self.fc = nn.Linear(fc_input_dims, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.fc(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></tbody></table></figure><p>在这里，我们使用 W&amp;B 来跟踪运行，因此使用<code>wandb.config</code> 对象来存储所有超参数。</p><p>该 <code>config</code> 对象的 <code>dict</code>ionary版本是一个非常有用的 <code>metadata</code>，所以一定要包含它！</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model_and_log</span>(<span class="hljs-params">config</span>):<br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"initialize"</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        config = wandb.config<br>        <br>        model = ConvNet(**config)<br><br>        model_artifact = wandb.Artifact(<br>            <span class="hljs-string">"convnet"</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">"model"</span>,<br>            description=<span class="hljs-string">"Simple AlexNet style CNN"</span>,<br>            metadata=<span class="hljs-built_in">dict</span>(config))<br><br>        torch.save(model.state_dict(), <span class="hljs-string">"initialized_model.pth"</span>)<br>        <span class="hljs-comment"># ➕ another way to add a file to an Artifact</span><br>        model_artifact.add_file(<span class="hljs-string">"initialized_model.pth"</span>)<br><br>        wandb.save(<span class="hljs-string">"initialized_model.pth"</span>)<br><br>        run.log_artifact(model_artifact)<br><br>model_config = {<span class="hljs-string">"hidden_layer_sizes"</span>: [<span class="hljs-number">32</span>, <span class="hljs-number">64</span>],<br>                <span class="hljs-string">"kernel_sizes"</span>: [<span class="hljs-number">3</span>],<br>                <span class="hljs-string">"activation"</span>: <span class="hljs-string">"ReLU"</span>,<br>                <span class="hljs-string">"pool_sizes"</span>: [<span class="hljs-number">2</span>],<br>                <span class="hljs-string">"dropout"</span>: <span class="hljs-number">0.5</span>,<br>                <span class="hljs-string">"num_classes"</span>: <span class="hljs-number"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model_and_log</span>(<span class="hljs-params">config</span>):<br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"initialize"</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        config = wandb.config<br>        <br>        model = ConvNet(**config)<br><br>        model_artifact = wandb.Artifact(<br>            <span class="hljs-string">"convnet"</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">"model"</span>,<br>            description=<span class="hljs-string">"Simple AlexNet style CNN"</span>,<br>            metadata=<span class="hljs-built_in">dict</span>(config))<br><br>        torch.save(model.state_dict(), <span class="hljs-string">"initialized_model.pth"</span>)<br>        <span class="hljs-comment"># ➕ another way to add a file to an Artifact</span><br>        model_artifact.add_file(<span class="hljs-string">"initialized_model.pth"</span>)<br><br>        wandb.save(<span class="hljs-string">"initialized_model.pth"</span>)<br><br>        run.log_artifact(model_artifact)<br><br>model_config = {<span class="hljs-string">"hidden_layer_sizes"</span>: [<span class="hljs-number">32</span>, <span class="hljs-number">64</span>],<br>                <span class="hljs-string">"kernel_sizes"</span>: [<span class="hljs-number">3</span>],<br>                <span class="hljs-string">"activation"</span>: <span class="hljs-string">"ReLU"</span>,<br>                <span class="hljs-string">"pool_sizes"</span>: [<span class="hljs-number">2</span>],<br>                <span class="hljs-string">"dropout"</span>: <span class="hljs-number">0.5</span>,<br>                <span class="hljs-string">"num_classes"</span>: <span class="hljs-number">10</span>}<br><br>build_model_and_log(model_config)<br></code></pre></td></tr></tbody></table></figure><h4 id="artifact.add_file">➕ <code>artifact.add_file</code></h4><p>与在数据集日志记录示例中同时编写 <code>new_file</code> 并将其添加到<code>Artifact</code> 不同，我们还可以一步写入文件（此处为<code>torch.save</code>），然后在另一步中将它们 <code>add</code> 到<code>Artifact</code>。</p><blockquote><p>👍规则：尽可能使用 <code>new_file</code>，以防止重复。</p></blockquote><h3 id="use-a-logged-model-artifact">4️⃣ Use a Logged Model Artifact</h3><p>就像我们可以在 <code>dataset</code> 上调用 <code>use_artifact</code>一样，我们可以在我们的 <code>initialized_model</code>上调用它以在另一个运行中使用它。</p><p>这一次，让我们 <code>train</code> <code>model</code>。</p><p>有关更多详细信息，请查看我们关于 <a href="http://wandb.me/pytorch-colab">instrumenting W&amp;B withPyTorch</a> 的 Colab。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model, train_loader, valid_loader, config</span>):<br>    optimizer = <span class="hljs-built_in">getattr</span>(torch.optim, config.optimizer)(model.parameters())<br>    model.train()<br>    example_ct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>        <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>            data, target = data.to(device), target.to(device)<br>            optimizer.zero_grad()<br>            output = model(data)<br>            loss = F.cross_entropy(output, target)<br>            loss.backward()<br>            optimizer.step()<br><br>            example_ct += <span class="hljs-built_in">len</span>(data)<br><br>            <span class="hljs-keyword">if</span> batch_idx % config.batch_log_interval == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">'Train Epoch: {} [{}/{} ({:.0%})]\tLoss: {:.6f}'</span>.<span class="hljs-built_in">format</span>(<br>                    epoch, batch_idx * <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(train_loader.dataset),<br>                    batch_idx / <span class="hljs-built_in">len</span>(train_loader), loss.item()))<br>                <br>                train_log(loss, example_ct, epoch)<br><br>        <span class="hljs-comment"># evaluate the model on the validation set at each epoch</span><br>        loss, accuracy = test(model, valid_loader)  <br>        test_log(loss, accuracy, example_ct, epoch)<br><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">model, test_loader</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    test_loss = <span class="hljs-number">0</span><br>    correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            test_loss += F.cross_entropy(output, target, reduction=<span class="hljs-string">'sum'</span>)  <span class="hljs-comment"># sum up batch loss</span><br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># get the index of the max log-probability</span><br>            correct += pred.eq(target.view_as(pred)).<span class="hljs-built_in">sum</span>()<br><br>    test_loss /= <span class="hljs-built_in">len</span>(test_loader.dataset)<br><br>    accuracy = <span class="hljs-number">100.</span> * correct / <span class="hljs-built_in">len</span>(test_loader.dataset)<br>    <br>    <span class="hljs-keyword">return</span> test_loss, accuracy<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_log</span>(<span class="hljs-params">loss, example_ct, epoch</span>):<br>    loss = <span class="hljs-built_in">float</span>(loss)<br><br>    <span class="hljs-comment"># where the magic happens</span><br>    wandb.log({<span class="hljs-string">"epoch"</span>: epoch, <span class="hljs-string">"train/loss"</span>: loss}, step=example_ct)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Loss after "</span> + <span class="hljs-built_in">str</span>(example_ct).zfill(<span class="hljs-number">5</span>) + <span class="hljs-string">f" examples: <span class="hljs-subst">{loss:<span class="hljs-number">.3</span>f}</span>"</span>)<br>    <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_log</span>(<span class="hljs-params">loss, accuracy, example_ct, epoch</span>):<br>    loss = <span class="hljs-built_in">float</span>(loss)<br>    accuracy = <span class="hljs-built_in">float</span>(accuracy)<br><br>    <span class="hljs-comment"># where the magic happens</span><br>    wandb.log({<span class="hljs-string">"epoch"</span>: epoch, <span class="hljs-string">"validation/loss"</span>: loss, <span class="hljs-string">"validation/accuracy"</span>: accuracy}, step=example_ct)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Loss/accuracy after "</span> + <span class="hljs-built_in">str</span>(example_ct).zfill(<span class="hljs-number">5</span>) + <span class="hljs-string">f" examples: <span class="hljs-subst">{loss:<span class="hljs-number">.3</span>f}</span>/<span class="hljs-subst">{accuracy:<span class="hljs-number"><code class="language-hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model, train_loader, valid_loader, config</span>):<br>    optimizer = <span class="hljs-built_in">getattr</span>(torch.optim, config.optimizer)(model.parameters())<br>    model.train()<br>    example_ct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>        <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>            data, target = data.to(device), target.to(device)<br>            optimizer.zero_grad()<br>            output = model(data)<br>            loss = F.cross_entropy(output, target)<br>            loss.backward()<br>            optimizer.step()<br><br>            example_ct += <span class="hljs-built_in">len</span>(data)<br><br>            <span class="hljs-keyword">if</span> batch_idx % config.batch_log_interval == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">'Train Epoch: {} [{}/{} ({:.0%})]\tLoss: {:.6f}'</span>.<span class="hljs-built_in">format</span>(<br>                    epoch, batch_idx * <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(train_loader.dataset),<br>                    batch_idx / <span class="hljs-built_in">len</span>(train_loader), loss.item()))<br>                <br>                train_log(loss, example_ct, epoch)<br><br>        <span class="hljs-comment"># evaluate the model on the validation set at each epoch</span><br>        loss, accuracy = test(model, valid_loader)  <br>        test_log(loss, accuracy, example_ct, epoch)<br><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">model, test_loader</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    test_loss = <span class="hljs-number">0</span><br>    correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            test_loss += F.cross_entropy(output, target, reduction=<span class="hljs-string">'sum'</span>)  <span class="hljs-comment"># sum up batch loss</span><br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># get the index of the max log-probability</span><br>            correct += pred.eq(target.view_as(pred)).<span class="hljs-built_in">sum</span>()<br><br>    test_loss /= <span class="hljs-built_in">len</span>(test_loader.dataset)<br><br>    accuracy = <span class="hljs-number">100.</span> * correct / <span class="hljs-built_in">len</span>(test_loader.dataset)<br>    <br>    <span class="hljs-keyword">return</span> test_loss, accuracy<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_log</span>(<span class="hljs-params">loss, example_ct, epoch</span>):<br>    loss = <span class="hljs-built_in">float</span>(loss)<br><br>    <span class="hljs-comment"># where the magic happens</span><br>    wandb.log({<span class="hljs-string">"epoch"</span>: epoch, <span class="hljs-string">"train/loss"</span>: loss}, step=example_ct)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Loss after "</span> + <span class="hljs-built_in">str</span>(example_ct).zfill(<span class="hljs-number">5</span>) + <span class="hljs-string">f" examples: <span class="hljs-subst">{loss:<span class="hljs-number">.3</span>f}</span>"</span>)<br>    <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_log</span>(<span class="hljs-params">loss, accuracy, example_ct, epoch</span>):<br>    loss = <span class="hljs-built_in">float</span>(loss)<br>    accuracy = <span class="hljs-built_in">float</span>(accuracy)<br><br>    <span class="hljs-comment"># where the magic happens</span><br>    wandb.log({<span class="hljs-string">"epoch"</span>: epoch, <span class="hljs-string">"validation/loss"</span>: loss, <span class="hljs-string">"validation/accuracy"</span>: accuracy}, step=example_ct)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Loss/accuracy after "</span> + <span class="hljs-built_in">str</span>(example_ct).zfill(<span class="hljs-number">5</span>) + <span class="hljs-string">f" examples: <span class="hljs-subst">{loss:<span class="hljs-number">.3</span>f}</span>/<span class="hljs-subst">{accuracy:<span class="hljs-number">.3</span>f}</span>"</span>)<br></code></pre></td></tr></tbody></table></figure><p>这次我们将运行两个独立的 <code>Artifact</code> 生产<code>Run</code>s。</p><p>一旦第一个完成 <code>train</code>ing<code>model</code>，第二个将通过评估其在 <code>test_dataset</code>上的性能来使用 <code>trained-model</code> <code>Artifact</code>。</p><p>此外，我们将提取网络最混乱的 32个示例——在这些示例中，<code>categorical_crossentropy</code> 最高。</p><p>这是诊断数据集和模型问题的好方法！</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">model, test_loader</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    ## Evaluate the trained model</span><br><span class="hljs-string">    """</span><br><br>    loss, accuracy = test(model, test_loader)<br>    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)<br><br>    <span class="hljs-keyword">return</span> loss, accuracy, highest_losses, hardest_examples, true_labels, predictions<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_hardest_k_examples</span>(<span class="hljs-params">model, testing_set, k=<span class="hljs-number">32</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br><br>    loader = DataLoader(testing_set, <span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># get the losses and predictions for each item in the dataset</span><br>    losses = <span class="hljs-literal">None</span><br>    predictions = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            loss = F.cross_entropy(output, target)<br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            <br>            <span class="hljs-keyword">if</span> losses <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                losses = loss.view((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>                predictions = pred<br>            <span class="hljs-keyword">else</span>:<br>                losses = torch.cat((losses, loss.view((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))), <span class="hljs-number">0</span>)<br>                predictions = torch.cat((predictions, pred), <span class="hljs-number">0</span>)<br><br>    argsort_loss = torch.argsort(losses, dim=<span class="hljs-number">0</span>)<br><br>    highest_k_losses = losses[argsort_loss[-k:]]<br>    hardest_k_examples = testing_set[argsort_loss[-k:]][<span class="hljs-number">0</span>]<br>    true_labels = testing_set[argsort_loss[-k:]][<span class="hljs-number">1</span>]<br>    predicted_labels = predictions[argsort_loss[-k:]]<br><br>    <span class="hljs-keyword"><code class="language-hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">model, test_loader</span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    ## Evaluate the trained model</span><br><span class="hljs-string">    """</span><br><br>    loss, accuracy = test(model, test_loader)<br>    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)<br><br>    <span class="hljs-keyword">return</span> loss, accuracy, highest_losses, hardest_examples, true_labels, predictions<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_hardest_k_examples</span>(<span class="hljs-params">model, testing_set, k=<span class="hljs-number">32</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br><br>    loader = DataLoader(testing_set, <span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># get the losses and predictions for each item in the dataset</span><br>    losses = <span class="hljs-literal">None</span><br>    predictions = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            loss = F.cross_entropy(output, target)<br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            <br>            <span class="hljs-keyword">if</span> losses <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                losses = loss.view((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>                predictions = pred<br>            <span class="hljs-keyword">else</span>:<br>                losses = torch.cat((losses, loss.view((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))), <span class="hljs-number">0</span>)<br>                predictions = torch.cat((predictions, pred), <span class="hljs-number">0</span>)<br><br>    argsort_loss = torch.argsort(losses, dim=<span class="hljs-number">0</span>)<br><br>    highest_k_losses = losses[argsort_loss[-k:]]<br>    hardest_k_examples = testing_set[argsort_loss[-k:]][<span class="hljs-number">0</span>]<br>    true_labels = testing_set[argsort_loss[-k:]][<span class="hljs-number">1</span>]<br>    predicted_labels = predictions[argsort_loss[-k:]]<br><br>    <span class="hljs-keyword">return</span> highest_k_losses, hardest_k_examples, true_labels, predicted_labels<br></code></pre></td></tr></tbody></table></figure><p>这些日志记录功能不会添加任何新的 <code>Artifact</code>功能，因此我们不会对其进行评论：我们只是在使用、下载和记录<code>Artifact</code>s。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_and_log</span>(<span class="hljs-params">config</span>):<br><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"train"</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        config = wandb.config<br><br>        data = run.use_artifact(<span class="hljs-string">'mnist-preprocess:latest'</span>)<br>        data_dir = data.download()<br><br>        training_dataset =  read(data_dir, <span class="hljs-string">"training"</span>)<br>        validation_dataset = read(data_dir, <span class="hljs-string">"validation"</span>)<br><br>        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)<br>        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)<br>        <br>        model_artifact = run.use_artifact(<span class="hljs-string">"convnet:latest"</span>)<br>        model_dir = model_artifact.download()<br>        model_path = os.path.join(model_dir, <span class="hljs-string">"initialized_model.pth"</span>)<br>        model_config = model_artifact.metadata<br>        config.update(model_config)<br><br>        model = ConvNet(**model_config)<br>        model.load_state_dict(torch.load(model_path))<br>        model = model.to(device)<br> <br>        train(model, train_loader, validation_loader, config)<br><br>        model_artifact = wandb.Artifact(<br>            <span class="hljs-string">"trained-model"</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">"model"</span>,<br>            description=<span class="hljs-string">"Trained NN model"</span>,<br>            metadata=<span class="hljs-built_in">dict</span>(model_config))<br><br>        torch.save(model.state_dict(), <span class="hljs-string">"trained_model.pth"</span>)<br>        model_artifact.add_file(<span class="hljs-string">"trained_model.pth"</span>)<br>        wandb.save(<span class="hljs-string">"trained_model.pth"</span>)<br><br>        run.log_artifact(model_artifact)<br><br>    <span class="hljs-keyword">return</span> model<br><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_and_log</span>(<span class="hljs-params">config=<span class="hljs-literal">None</span></span>):<br>    <br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"report"</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        data = run.use_artifact(<span class="hljs-string">'mnist-preprocess:latest'</span>)<br>        data_dir = data.download()<br>        testing_set = read(data_dir, <span class="hljs-string">"test"</span>)<br><br>        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>        model_artifact = run.use_artifact(<span class="hljs-string">"trained-model:latest"</span>)<br>        model_dir = model_artifact.download()<br>        model_path = os.path.join(model_dir, <span class="hljs-string">"trained_model.pth"</span>)<br>        model_config = model_artifact.metadata<br><br>        model = ConvNet(**model_config)<br>        model.load_state_dict(torch.load(model_path))<br>        model.to(device)<br><br>        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)<br><br>        run.summary.update({<span class="hljs-string">"loss"</span>: loss, <span class="hljs-string">"accuracy"</span>: accuracy})<br><br>        wandb.log({<span class="hljs-string">"high-loss-examples"</span>:<br>            [wandb.Image(hard_example, caption=<span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(pred)) + <span class="hljs-string">","</span> +  <span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(label)))<br>             <span class="hljs-keyword">for</span> hard_example, pred, label <span class="hljs-keyword">in</span> <span class="hljs-built_in"><code class="language-hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_and_log</span>(<span class="hljs-params">config</span>):<br><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"train"</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        config = wandb.config<br><br>        data = run.use_artifact(<span class="hljs-string">'mnist-preprocess:latest'</span>)<br>        data_dir = data.download()<br><br>        training_dataset =  read(data_dir, <span class="hljs-string">"training"</span>)<br>        validation_dataset = read(data_dir, <span class="hljs-string">"validation"</span>)<br><br>        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)<br>        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)<br>        <br>        model_artifact = run.use_artifact(<span class="hljs-string">"convnet:latest"</span>)<br>        model_dir = model_artifact.download()<br>        model_path = os.path.join(model_dir, <span class="hljs-string">"initialized_model.pth"</span>)<br>        model_config = model_artifact.metadata<br>        config.update(model_config)<br><br>        model = ConvNet(**model_config)<br>        model.load_state_dict(torch.load(model_path))<br>        model = model.to(device)<br> <br>        train(model, train_loader, validation_loader, config)<br><br>        model_artifact = wandb.Artifact(<br>            <span class="hljs-string">"trained-model"</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">"model"</span>,<br>            description=<span class="hljs-string">"Trained NN model"</span>,<br>            metadata=<span class="hljs-built_in">dict</span>(model_config))<br><br>        torch.save(model.state_dict(), <span class="hljs-string">"trained_model.pth"</span>)<br>        model_artifact.add_file(<span class="hljs-string">"trained_model.pth"</span>)<br>        wandb.save(<span class="hljs-string">"trained_model.pth"</span>)<br><br>        run.log_artifact(model_artifact)<br><br>    <span class="hljs-keyword">return</span> model<br><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_and_log</span>(<span class="hljs-params">config=<span class="hljs-literal">None</span></span>):<br>    <br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">"artifacts-example"</span>, job_type=<span class="hljs-string">"report"</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        data = run.use_artifact(<span class="hljs-string">'mnist-preprocess:latest'</span>)<br>        data_dir = data.download()<br>        testing_set = read(data_dir, <span class="hljs-string">"test"</span>)<br><br>        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>        model_artifact = run.use_artifact(<span class="hljs-string">"trained-model:latest"</span>)<br>        model_dir = model_artifact.download()<br>        model_path = os.path.join(model_dir, <span class="hljs-string">"trained_model.pth"</span>)<br>        model_config = model_artifact.metadata<br><br>        model = ConvNet(**model_config)<br>        model.load_state_dict(torch.load(model_path))<br>        model.to(device)<br><br>        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)<br><br>        run.summary.update({<span class="hljs-string">"loss"</span>: loss, <span class="hljs-string">"accuracy"</span>: accuracy})<br><br>        wandb.log({<span class="hljs-string">"high-loss-examples"</span>:<br>            [wandb.Image(hard_example, caption=<span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(pred)) + <span class="hljs-string">","</span> +  <span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(label)))<br>             <span class="hljs-keyword">for</span> hard_example, pred, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(hardest_examples, preds, true_labels)]})<br></code></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre class=" language-hljs python">train_config = {<span class="hljs-string">"batch_size"</span>: <span class="hljs-number">128</span>,<br>                <span class="hljs-string">"epochs"</span>: <span class="hljs-number">5</span>,<br>                <span class="hljs-string">"batch_log_interval"</span>: <span class="hljs-number">25</span>,<br>                <span class="hljs-string">"optimizer"</span>: <span class="hljs-string"><code class="language-hljs python">train_config = {<span class="hljs-string">"batch_size"</span>: <span class="hljs-number">128</span>,<br>                <span class="hljs-string">"epochs"</span>: <span class="hljs-number">5</span>,<br>                <span class="hljs-string">"batch_log_interval"</span>: <span class="hljs-number">25</span>,<br>                <span class="hljs-string">"optimizer"</span>: <span class="hljs-string">"Adam"</span>}<br><br>model = train_and_log(train_config)<br>evaluate_and_log()<br></code></pre></td></tr></tbody></table></figure><h4 id="the-graph-view">🔁 The Graph View</h4><p>请注意，我们更改了 <code>Artifact</code> 的 <code>type</code>：这些<code>Run</code>s 使用的是模型，而不是数据集。在 Artifacts页面的图形视图中，生产模型的 <code>Run</code>s 将与生成<code>dataset</code>s 的运行分开。</p><p>去看看吧！和以前一样，您需要前往 Run 页面，从左侧栏中选择 "Artifacts"选项卡，选择一个 <code>Artifact</code>，然后单击 "Graph View"选项卡。</p><h4 id="exploded-graphs">💣 Exploded Graphs</h4><p>您可能已经注意到标有“爆炸”的按钮。不要点击它，因为它会在 W&amp;B总部您不起眼的作者办公桌下引爆一枚小炸弹！</p><p>只是在开玩笑。它以更温和的方式“分解”图表：<code>Artifact</code>s 和<code>Run</code>s 在单个实例级别而不是类型级别分离：节点不是<code>dataset</code> 和 <code>load-data</code>，而是<code>dataset:mnist-raw:v1</code> 和<code>load-data:sunny-smoke-1</code>，等等。</p><p>这提供了对您的管道的全面洞察，记录的指标、元数据等都触手可及——您仅受限于您选择与我们一起记录的内容。</p>]]></content>
    
    
    
    <tags>
      
      <tag>WandB</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（三）Typora + PicGo+ 腾讯云图床</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-san-typora-teng-xun-yun-tu-chuang/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-san-typora-teng-xun-yun-tu-chuang/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（二）Matery 主题</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-er-matery-zhu-ti/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-er-matery-zhu-ti/</url>
    
    <content type="html"><![CDATA[<h2 id="下载">下载</h2><p>本主题<strong>推荐你使用 Hexo 5.0.0及以上的版本</strong>。如果，你已经有一个自己的 <a href="https://hexo.io/zh-cn/">Hexo</a> 博客了，建议你将 Hexo升级到最新稳定的版本。</p><p>点击 <a href="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master">这里</a>下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将<code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的<code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用<code>git clone</code> 命令来下载:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash">git <span class="hljs-built_in"><code class="language-hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/blinkfox/hexo-theme-matery.git<br></code></pre></td></tr></tbody></table></figure><h2 id="配置">配置</h2><h3 id="切换主题">切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的 <code>theme</code>的值：<code>theme: hexo-theme-matery</code></p><h4 id="config.yml-文件的其它修改建议"><code>_config.yml</code>文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主<code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code>的倍数，如：<code>12</code>、<code>18</code>等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为<code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页">新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客<code>source</code> 目录下还没有 <code>categories/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash">hexo new page <span class="hljs-string"><code class="language-hljs bash">hexo new page <span class="hljs-string">"categories"</span><br></code></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件<code>/source/categories/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">categories</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"categories"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"categories"</span><br><span class="hljs-meta"><code class="language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">categories</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"categories"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"categories"</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></tbody></table></figure><h3 id="新建标签-tags-页">新建标签 tags 页</h3><p><code>tags</code> 页是用来展示所有标签的页面，如果在你的博客<code>source</code> 目录下还没有 <code>tags/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash">hexo new page <span class="hljs-string"><code class="language-hljs bash">hexo new page <span class="hljs-string">"tags"</span><br></code></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件<code>/source/tags/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">tags</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 18:23:38</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"tags"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"tags"</span><br><span class="hljs-meta"><code class="language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">tags</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 18:23:38</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"tags"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"tags"</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></tbody></table></figure><h3 id="新建关于我-about-页">新建关于我 about 页</h3><p><code>about</code>页是用来展示<strong>关于我和我的博客</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>about/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash">hexo new page <span class="hljs-string"><code class="language-hljs bash">hexo new page <span class="hljs-string">"about"</span><br></code></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件<code>/source/about/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">about</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"about"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"about"</span><br><span class="hljs-meta"><code class="language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">about</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"about"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"about"</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></tbody></table></figure><h3 id="新建留言板-contact-页可选的">新建留言板 contact页（可选的）</h3><p><code>contact</code>页是用来展示<strong>留言板</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>contact/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash">hexo new page <span class="hljs-string"><code class="language-hljs bash">hexo new page <span class="hljs-string">"contact"</span><br></code></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件<code>/source/contact/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">contact</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"contact"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"contact"</span><br><span class="hljs-meta"><code class="language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">contact</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"contact"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"contact"</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></tbody></table></figure><blockquote><p><strong>注</strong>：本留言板功能依赖于第三方评论系统，请<strong>激活</strong>你的评论系统才有效果。并且在主题的<code>_config.yml</code> 文件中，第 <code>19</code> 至 <code>21</code>行的“<strong>菜单</strong>”配置，取消关于留言板的注释即可。</p></blockquote><h3 id="新建友情链接-friends-页可选的">新建友情链接 friends页（可选的）</h3><p><code>friends</code>页是用来展示<strong>友情链接</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>friends/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash">hexo new page <span class="hljs-string"><code class="language-hljs bash">hexo new page <span class="hljs-string">"friends"</span><br></code></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件<code>/source/friends/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">friends</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-12-12 21:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"friends"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"friends"</span><br><span class="hljs-meta"><code class="language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">friends</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-12-12 21:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"friends"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"friends"</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></tbody></table></figure><p>同时，在你的博客 <code>source</code> 目录下新建 <code>_data</code>目录，在 <code>_data</code> 目录中新建 <code>friends.json</code>文件，文件内容如下所示：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre class=" language-hljs json"><span class="hljs-punctuation">[</span><span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"avatar"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"http://image.luokangyuan.com/1_qq_27922023.jpg"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"码酱"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"introduction"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"我不是大佬，只是在追寻大佬的脚步"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"url"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"http://luokangyuan.com/"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"前去学习"</span><br><span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"avatar"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"http://image.luokangyuan.com/4027734.jpeg"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"闪烁之狐"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"introduction"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"url"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"https://blinkfox.github.io/"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"前去学习"</span><br><span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"avatar"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"http://image.luokangyuan.com/avatar.jpg"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"ja_rome"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"introduction"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"平凡的脚步也可以走出伟大的行程"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"url"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"https://me.csdn.net/jlh912008548"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"前去学习"</span><br><span class="hljs-punctuation">}</span><span class="hljs-punctuation"><code class="language-hljs json"><span class="hljs-punctuation">[</span><span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"avatar"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"http://image.luokangyuan.com/1_qq_27922023.jpg"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"码酱"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"introduction"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"我不是大佬，只是在追寻大佬的脚步"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"url"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"http://luokangyuan.com/"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"前去学习"</span><br><span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"avatar"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"http://image.luokangyuan.com/4027734.jpeg"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"闪烁之狐"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"introduction"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"url"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"https://blinkfox.github.io/"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"前去学习"</span><br><span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"avatar"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"http://image.luokangyuan.com/avatar.jpg"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"ja_rome"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"introduction"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"平凡的脚步也可以走出伟大的行程"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"url"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"https://me.csdn.net/jlh912008548"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"前去学习"</span><br><span class="hljs-punctuation">}</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></tbody></table></figure><h3 id="新建-404-页">新建 404 页</h3><p>如果在你的博客 <code>source</code> 目录下还没有 <code>404.md</code>文件，那么你就需要新建一个。编辑你刚刚新建的页面文件<code>/source/404.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-number">404</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"404"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"404"</span><br><span class="hljs-attr">description:</span> <span class="hljs-string">"Oops～，我崩溃了！找不到你想要的页面 :("</span><br><span class="hljs-meta"><code class="language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-number">404</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">"404"</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">"404"</span><br><span class="hljs-attr">description:</span> <span class="hljs-string">"Oops～，我崩溃了！找不到你想要的页面 :("</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></tbody></table></figure><h3 id="菜单导航配置">菜单导航配置</h3><h4 id="配置基本菜单导航的名称路径url和图标icon.">配置基本菜单导航的名称、路径url和图标icon.</h4><p>1.菜单导航名称可以是中文也可以是英文(如：<code>Index</code>或<code>主页</code>)2.图标icon 可以在<a href="https://fontawesome.com/icons">FontAwesome</a> 中查找</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-attr">Index:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-home</span><br>  <span class="hljs-attr">Tags:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/tags</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-tags</span><br>  <span class="hljs-attr">Categories:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/categories</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-bookmark</span><br>  <span class="hljs-attr">Archives:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/archives</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-archive</span><br>  <span class="hljs-attr">About:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/about</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-user-circle</span><br>  <span class="hljs-attr">Friends:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/friends</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string"><code class="language-hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-attr">Index:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-home</span><br>  <span class="hljs-attr">Tags:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/tags</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-tags</span><br>  <span class="hljs-attr">Categories:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/categories</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-bookmark</span><br>  <span class="hljs-attr">Archives:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/archives</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-archive</span><br>  <span class="hljs-attr">About:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/about</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-user-circle</span><br>  <span class="hljs-attr">Friends:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/friends</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-address-book</span><br></code></pre></td></tr></tbody></table></figure><h4 id="二级菜单配置方法">二级菜单配置方法</h4><p>如果你需要二级菜单则可以在原基本菜单导航的基础上如下操作</p><ol type="1"><li>在需要添加二级菜单的一级菜单下添加<code>children</code>关键字(如:<code>About</code>菜单下添加<code>children</code>)<br></li><li>在<code>children</code>下创建二级菜单的名称name,路径url和图标icon.<br></li><li>注意每个二级菜单模块前要加 <code>-</code>.<br></li><li>注意缩进格式</li></ol><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-attr">Index:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-home</span><br>  <span class="hljs-attr">Tags:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/tags</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-tags</span><br>  <span class="hljs-attr">Categories:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/categories</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-bookmark</span><br>  <span class="hljs-attr">Archives:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/archives</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-archive</span><br>  <span class="hljs-attr">About:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/about</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-user-circle-o</span><br>  <span class="hljs-attr">Friends:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/friends</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-address-book</span><br>  <span class="hljs-attr">Medias:</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-list</span><br>    <span class="hljs-attr">children:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Music</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/music</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-music</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Movies</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/movies</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-film</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Books</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/books</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-book</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Galleries</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/galleries</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string"><code class="language-hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-attr">Index:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-home</span><br>  <span class="hljs-attr">Tags:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/tags</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-tags</span><br>  <span class="hljs-attr">Categories:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/categories</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-bookmark</span><br>  <span class="hljs-attr">Archives:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/archives</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-archive</span><br>  <span class="hljs-attr">About:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/about</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-user-circle-o</span><br>  <span class="hljs-attr">Friends:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/friends</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-address-book</span><br>  <span class="hljs-attr">Medias:</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-list</span><br>    <span class="hljs-attr">children:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Music</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/music</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-music</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Movies</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/movies</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-film</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Books</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/books</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-book</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Galleries</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/galleries</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-image</span><br></code></pre></td></tr></tbody></table></figure><h3 id="代码高亮">代码高亮</h3><p>从 Hexo5.0 版本开始自带了 <code>prismjs</code>代码语法高亮的支持，本主题对此进行了改造支持。</p><p>如果你的博客中曾经安装过 <code>hexo-prism-plugin</code>的插件，那么你须要执行 <code>npm uninstall hexo-prism-plugin</code>来卸载掉它，否则生成的代码中会有 <code>&amp;#123;</code> 和<code>&amp;#125;</code> 的转义字符。</p><p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中<code>highlight.enable</code> 的值为 <code>false</code>，并将<code>prismjs.enable</code> 的值设置为<code>true</code>，主要配置如下：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">highlight:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">line_number:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">auto_detect:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">tab_replace:</span> <span class="hljs-string">''</span><br>  <span class="hljs-attr">wrap:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">hljs:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">prismjs:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">preprocess:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">line_number:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">tab_replace:</span> <span class="hljs-string"><code class="language-hljs yaml"><span class="hljs-attr">highlight:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">line_number:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">auto_detect:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">tab_replace:</span> <span class="hljs-string">''</span><br>  <span class="hljs-attr">wrap:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">hljs:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">prismjs:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">preprocess:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">line_number:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">tab_replace:</span> <span class="hljs-string">''</span><br></code></pre></td></tr></tbody></table></figure><p>主题中默认的 <code>prismjs</code> 主题是<code>Tomorrow Night</code>，如果你想定制自己的主题，可以前往 <a href="https://prismjs.com/download.html">prismjs 下载页面</a>定制下载自己喜欢的主题 <code>css</code> 文件，然后将此 css主题文件取名为 <code>prism.css</code>，替换掉<code>hexo-theme-matery</code> 主题文件夹中的<code>source/libs/prism/prism.css</code> 文件即可。</p><h3 id="搜索">搜索</h3><p>本主题中还使用到了 <a href="https://github.com/wzpan/hexo-generator-search">hexo-generator-search</a>的 Hexo 插件来做内容搜索，安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">npm install hexo-generator-search --save<br></code></pre></td></tr></tbody></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">search:</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">search.xml</span><br>  <span class="hljs-attr">field:</span> <span class="hljs-string"><code class="language-hljs yaml"><span class="hljs-attr">search:</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">search.xml</span><br>  <span class="hljs-attr">field:</span> <span class="hljs-string">post</span><br></code></pre></td></tr></tbody></table></figure><h3 id="中文链接转拼音建议安装">中文链接转拼音（建议安装）</h3><p>如果你的文章名称是中文的，那么 Hexo默认生成的永久链接也会有中文，这样不利于 <code>SEO</code>，且<code>gitment</code> 评论对中文链接也不支持。我们可以用 <a href="https://github.com/viko16/hexo-permalink-pinyin">hexo-permalink-pinyin</a>Hexo 插件使在生成文章时生成中文拼音的永久链接。</p><p>安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">npm i hexo-permalink-pinyin --save<br></code></pre></td></tr></tbody></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">permalink_pinyin:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">separator:</span> <span class="hljs-string">'-'</span> <span class="hljs-comment"><code class="language-hljs yaml"><span class="hljs-attr">permalink_pinyin:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">separator:</span> <span class="hljs-string">'-'</span> <span class="hljs-comment"># default: '-'</span><br></code></pre></td></tr></tbody></table></figure><blockquote><p><strong>注</strong>：除了此插件外，<a href="https://github.com/rozbo/hexo-abbrlink">hexo-abbrlink</a>插件也可以生成非中文的链接。</p></blockquote><h3 id="文章字数统计插件建议安装">文章字数统计插件（建议安装）</h3><p>如果你想要在文章中显示文章字数、阅读时长信息，可以安装 <a href="https://github.com/willin/hexo-wordcount">hexo-wordcount</a>插件。</p><p>安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">npm i --save hexo-wordcount<br></code></pre></td></tr></tbody></table></figure><p>然后只需在本主题下的 <code>_config.yml</code>文件中，将各个文章字数相关的配置激活即可：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">postInfo:</span><br>  <span class="hljs-attr">date:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">update:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">wordCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 设置文章字数统计为 true.</span><br>  <span class="hljs-attr">totalCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 设置站点文章总字数统计为 true.</span><br>  <span class="hljs-attr">min2read:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 阅读时长.</span><br>  <span class="hljs-attr">readCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"><code class="language-hljs yaml"><span class="hljs-attr">postInfo:</span><br>  <span class="hljs-attr">date:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">update:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">wordCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 设置文章字数统计为 true.</span><br>  <span class="hljs-attr">totalCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 设置站点文章总字数统计为 true.</span><br>  <span class="hljs-attr">min2read:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 阅读时长.</span><br>  <span class="hljs-attr">readCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 阅读次数.</span><br></code></pre></td></tr></tbody></table></figure><h3 id="添加emoji表情支持可选的">添加emoji表情支持（可选的）</h3><p>本主题新增了对<code>emoji</code>表情的支持，使用到了 <a href="https://npm.taobao.org/package/hexo-filter-github-emojis">hexo-filter-github-emojis</a>的 Hexo 插件来支持<code>emoji</code>表情的生成，把对应的<code>markdown emoji</code>语法（<code>::</code>,例如：<code>:smile:</code>）转变成会跳跃的<code>emoji</code>表情，安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">npm install hexo-filter-github-emojis --save<br></code></pre></td></tr></tbody></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">githubEmojis:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">className:</span> <span class="hljs-string">github-emoji</span><br>  <span class="hljs-attr">inject:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">styles:</span><br>  <span class="hljs-attr"><code class="language-hljs yaml"><span class="hljs-attr">githubEmojis:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">className:</span> <span class="hljs-string">github-emoji</span><br>  <span class="hljs-attr">inject:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">styles:</span><br>  <span class="hljs-attr">customEmojis:</span><br></code></pre></td></tr></tbody></table></figure><p>执行 <code>hexo clean &amp;&amp; hexo g</code>重新生成博客文件，然后就可以在文章中对应位置看到你用<code>emoji</code>语法写的表情了。</p><h3 id="添加-rss-订阅支持可选的">添加 RSS 订阅支持（可选的）</h3><p>本主题中还使用到了 <a href="https://github.com/hexojs/hexo-generator-feed">hexo-generator-feed</a>的 Hexo 插件来做 <code>RSS</code>，安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">npm install hexo-generator-feed --save<br></code></pre></td></tr></tbody></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">feed:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">atom</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">atom.xml</span><br>  <span class="hljs-attr">limit:</span> <span class="hljs-number">20</span><br>  <span class="hljs-attr">hub:</span><br>  <span class="hljs-attr">content:</span><br>  <span class="hljs-attr">content_limit:</span> <span class="hljs-number">140</span><br>  <span class="hljs-attr">content_limit_delim:</span> <span class="hljs-string">' '</span><br>  <span class="hljs-attr">order_by:</span> <span class="hljs-string"><code class="language-hljs yaml"><span class="hljs-attr">feed:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">atom</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">atom.xml</span><br>  <span class="hljs-attr">limit:</span> <span class="hljs-number">20</span><br>  <span class="hljs-attr">hub:</span><br>  <span class="hljs-attr">content:</span><br>  <span class="hljs-attr">content_limit:</span> <span class="hljs-number">140</span><br>  <span class="hljs-attr">content_limit_delim:</span> <span class="hljs-string">' '</span><br>  <span class="hljs-attr">order_by:</span> <span class="hljs-string">-date</span><br></code></pre></td></tr></tbody></table></figure><p>执行 <code>hexo clean &amp;&amp; hexo g</code>重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到<code>atom.xml</code> 文件，说明你已经安装成功了。</p><h3 id="添加-daovoice-在线聊天功能可选的">添加 <a href="http://www.daovoice.io/">DaoVoice</a> 在线聊天功能（可选的）</h3><p>前往 <a href="http://www.daovoice.io/">DaoVoice</a> 官网注册并且获取<code>app_id</code>，并将 <code>app_id</code> 填入主题的<code>_config.yml</code> 文件中。</p><h3 id="添加-tidio-在线聊天功能可选的">添加 <a href="https://www.tidio.com/">Tidio</a> 在线聊天功能（可选的）</h3><p>前往 <a href="https://www.tidio.com/">Tidio</a> 官网注册并且获取<code>Public Key</code>，并将 <code>Public Key</code> 填入主题的<code>_config.yml</code> 文件中。</p><h3 id="修改页脚">修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的<code>/layout/_partial/footer.ejs</code>文件中，包括站点、使用的主题、访问量等。</p><h3 id="添加中文繁简转换">添加中文繁简转换</h3><p>在主题的 <code>_config.yml</code> 文件中，开启 translate 为enable。</p><blockquote><p>开启中文繁简转换如下修改。默认不开启。 实例演示： <a href="https://blog.17lai.site">繁简转换</a> 底下 footer 栏</p></blockquote><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-attr">translate:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal"><code class="language-hljs yaml"><span class="hljs-attr">translate:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></tbody></table></figure><h3 id="修改社交链接">修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持<code>QQ</code>、<code>GitHub</code> 和邮箱等的配置，你可以在主题文件的<code>/layout/_partial/social-link.ejs</code>文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre class=" language-hljs html">&lt;% if (theme.socialLink.github) { %&gt;<br>    <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">"&lt;%= theme.socialLink.github %&gt;"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"tooltipped"</span> <span class="hljs-attr">target</span>=<span class="hljs-string">"_blank"</span> <span class="hljs-attr">data-tooltip</span>=<span class="hljs-string">"访问我的GitHub"</span> <span class="hljs-attr">data-position</span>=<span class="hljs-string">"top"</span> <span class="hljs-attr">data-delay</span>=<span class="hljs-string">"50"</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">i</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"fab fa-github"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">i</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name"><code class="language-hljs html">&lt;% if (theme.socialLink.github) { %&gt;<br>    <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">"&lt;%= theme.socialLink.github %&gt;"</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"tooltipped"</span> <span class="hljs-attr">target</span>=<span class="hljs-string">"_blank"</span> <span class="hljs-attr">data-tooltip</span>=<span class="hljs-string">"访问我的GitHub"</span> <span class="hljs-attr">data-position</span>=<span class="hljs-string">"top"</span> <span class="hljs-attr">data-delay</span>=<span class="hljs-string">"50"</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">i</span> <span class="hljs-attr">class</span>=<span class="hljs-string">"fab fa-github"</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">i</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">a</span>></span><br><% } %><br></code></pre></td></tr></tbody></table></figure><p>其中，社交图标（如：<code>fa-github</code>）你可以在 <a href="https://fontawesome.com/icons">Font Awesome</a>中搜索找到。以下是常用社交图标的标识，供你参考：</p><ul><li>Facebook: <code>fab fa-facebook</code></li><li>Twitter: <code>fab fa-twitter</code></li><li>Google-plus: <code>fab fa-google-plus</code></li><li>Linkedin: <code>fab fa-linkedin</code></li><li>Tumblr: <code>fab fa-tumblr</code></li><li>Medium: <code>fab fa-medium</code></li><li>Slack: <code>fab fa-slack</code></li><li>Sina Weibo: <code>fab fa-weibo</code></li><li>Wechat: <code>fab fa-weixin</code></li><li>QQ: <code>fab fa-qq</code></li><li>Zhihu: <code>fab fa-zhihu</code></li></ul><blockquote><p><strong>注意</strong>: 本主题中使用的 <code>Font Awesome</code>版本为 <code>5.11.0</code>。</p></blockquote><h3 id="修改打赏的二维码图片">修改打赏的二维码图片</h3><p>在主题文件的 <code>source/medias/reward</code>文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p><h3 id="配置音乐播放器可选的">配置音乐播放器（可选的）</h3><p>要支持音乐播放，在主题的 <code>_config.yml</code>配置文件中激活music配置即可：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-comment"># 是否在首页显示音乐</span><br><span class="hljs-attr">music:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">title:</span>         <span class="hljs-comment"># 非吸底模式有效</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">show:</span> <span class="hljs-string">听听音乐</span><br>  <span class="hljs-attr">server:</span> <span class="hljs-string">netease</span>   <span class="hljs-comment"># require music platform: netease, tencent, kugou, xiami, baidu</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">playlist</span>    <span class="hljs-comment"># require song, playlist, album, search, artist</span><br>  <span class="hljs-attr">id:</span> <span class="hljs-number">503838841</span>     <span class="hljs-comment"># require song id / playlist id / album id / search keyword</span><br>  <span class="hljs-attr">fixed:</span> <span class="hljs-literal">false</span>      <span class="hljs-comment"># 开启吸底模式</span><br>  <span class="hljs-attr">autoplay:</span> <span class="hljs-literal">false</span>   <span class="hljs-comment"># 是否自动播放</span><br>  <span class="hljs-attr">theme:</span> <span class="hljs-string">'#42b983'</span><br>  <span class="hljs-attr">loop:</span> <span class="hljs-string">'all'</span>       <span class="hljs-comment"># 音频循环播放, 可选值: 'all', 'one', 'none'</span><br>  <span class="hljs-attr">order:</span> <span class="hljs-string">'random'</span>   <span class="hljs-comment"># 音频循环顺序, 可选值: 'list', 'random'</span><br>  <span class="hljs-attr">preload:</span> <span class="hljs-string">'auto'</span>   <span class="hljs-comment"># 预加载，可选值: 'none', 'metadata', 'auto'</span><br>  <span class="hljs-attr">volume:</span> <span class="hljs-number">0.7</span>       <span class="hljs-comment"># 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效</span><br>  <span class="hljs-attr">listFolded:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"><code class="language-hljs yaml"><span class="hljs-comment"># 是否在首页显示音乐</span><br><span class="hljs-attr">music:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">title:</span>         <span class="hljs-comment"># 非吸底模式有效</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">show:</span> <span class="hljs-string">听听音乐</span><br>  <span class="hljs-attr">server:</span> <span class="hljs-string">netease</span>   <span class="hljs-comment"># require music platform: netease, tencent, kugou, xiami, baidu</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">playlist</span>    <span class="hljs-comment"># require song, playlist, album, search, artist</span><br>  <span class="hljs-attr">id:</span> <span class="hljs-number">503838841</span>     <span class="hljs-comment"># require song id / playlist id / album id / search keyword</span><br>  <span class="hljs-attr">fixed:</span> <span class="hljs-literal">false</span>      <span class="hljs-comment"># 开启吸底模式</span><br>  <span class="hljs-attr">autoplay:</span> <span class="hljs-literal">false</span>   <span class="hljs-comment"># 是否自动播放</span><br>  <span class="hljs-attr">theme:</span> <span class="hljs-string">'#42b983'</span><br>  <span class="hljs-attr">loop:</span> <span class="hljs-string">'all'</span>       <span class="hljs-comment"># 音频循环播放, 可选值: 'all', 'one', 'none'</span><br>  <span class="hljs-attr">order:</span> <span class="hljs-string">'random'</span>   <span class="hljs-comment"># 音频循环顺序, 可选值: 'list', 'random'</span><br>  <span class="hljs-attr">preload:</span> <span class="hljs-string">'auto'</span>   <span class="hljs-comment"># 预加载，可选值: 'none', 'metadata', 'auto'</span><br>  <span class="hljs-attr">volume:</span> <span class="hljs-number">0.7</span>       <span class="hljs-comment"># 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效</span><br>  <span class="hljs-attr">listFolded:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># 列表默认折叠</span><br></code></pre></td></tr></tbody></table></figure><blockquote><p><code>server</code>可选<code>netease</code>（网易云音乐），<code>tencent</code>（QQ音乐），<code>kugou</code>（酷狗音乐），<code>xiami</code>（虾米音乐），</p><p><code>baidu</code>（百度音乐）。</p><p><code>type</code>可选<code>song</code>（歌曲），<code>playlist</code>（歌单），<code>album</code>（专辑），<code>search</code>（搜索关键字），<code>artist</code>（歌手）</p><p><code>id</code>获取方法示例:浏览器打开网易云音乐，点击我喜欢的音乐歌单，浏览器地址栏后面会有一串数字，<code>playlist</code>的<code>id</code></p><p>即为这串数字。</p></blockquote><h3 id="添加note">添加note</h3><blockquote><p><a href="https://blog.17lai.site/posts/cf0f47fd/#tag-note">演示</a></p></blockquote><h4 id="usage">Usage</h4><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs html"><code class="language-hljs html">{% note [class] [no-icon] [summary] %}<br>Any content (support inline tags too).<br>{% endnote %}<br></code></pre></td></tr></tbody></table></figure><ul><li><code>[class]</code> : <em>Optional parameter.</em> Supportedvalues: default | primary | success | info | warning | danger.</li><li><code>[no-icon]</code> : <em>Optional parameter.</em> Disable iconin note.</li><li><code>[summary]</code> : <em>Optional parameter.</em> Optionalsummary of the note.</li></ul><p>All parameters are optional.</p><h4 id="example">example</h4><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre class=" language-hljs html"><code class="language-hljs html">{% note %}<br>#### Header<br>(without define class style)<br>{% endnote %}<br></code></pre></td></tr></tbody></table></figure><h3 id="添加button">添加button</h3><blockquote><p><a href="https://blog.17lai.site/posts/cf0f47fd/#tag-button">演示</a></p></blockquote><h4 id="usage-1">Usage</h4><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs html"><code class="language-hljs html">{% button url, text, icon [class], [title] %}<br></code></pre></td></tr></tbody></table></figure><p>or</p><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs html"><code class="language-hljs html">{% btn url, text, icon [class], [title] %}<br></code></pre></td></tr></tbody></table></figure><ul><li><code>url</code> : Absolute or relative path to URL.</li><li><code>text</code> : Button text. Required if no icon specified.</li><li><code>icon</code> : Font Awesome icon name. Required if no textspecified.</li><li><code>[class]</code> : <em>Optional parameter.</em> Font Awesomeclass(es): <code>fa-fw</code> | <code>fa-lg</code> | <code>fa-2x</code>| <code>fa-3x</code> | <code>fa-4x</code> | <code>fa-5x</code></li><li><code>[title]</code> : <em>Optional parameter.</em> Tooltip atmouseover.</li></ul><h4 id="examples">Examples</h4><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs html"><code class="language-hljs html">{% button #, Text %}<br></code></pre></td></tr></tbody></table></figure><h2 id="文章-front-matter-介绍">文章 Front-matter 介绍</h2><h3 id="front-matter-选项详解">Front-matter 选项详解</h3><p><code>Front-matter</code>选项中的所有内容均为<strong>非必填</strong>的。但我仍然建议至少填写<code>title</code> 和 <code>date</code> 的值。</p><table><thead><tr class="header"><th>配置选项</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>title</td><td><code>Markdown</code> 的文件标题</td><td>文章标题，强烈建议填写此选项</td></tr><tr class="even"><td>date</td><td>文件创建时的日期时间</td><td>发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr class="odd"><td>author</td><td>根 <code>_config.yml</code> 中的 <code>author</code></td><td>文章作者</td></tr><tr class="even"><td>img</td><td><code>featureImages</code> 中的某个值</td><td>文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如:<code>http://xxx.com/xxx.jpg</code></td></tr><tr class="odd"><td>top</td><td><code>true</code></td><td>推荐文章（文章是否置顶），如果 <code>top</code> 值为<code>true</code>，则会作为首页推荐文章</td></tr><tr class="even"><td>hide</td><td><code>false</code></td><td>隐藏文章，如果<code>hide</code>值为<code>true</code>，则文章不会在首页显示</td></tr><tr class="odd"><td>cover</td><td><code>false</code></td><td><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr class="even"><td>coverImg</td><td>无</td><td><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr class="odd"><td>password</td><td>无</td><td>文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置<code>password</code> 的值，该值必须是用 <code>SHA256</code>加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code>中激活了 <code>verifyPassword</code> 选项</td></tr><tr class="even"><td>toc</td><td><code>true</code></td><td>是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的<code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr class="odd"><td>mathjax</td><td><code>false</code></td><td>是否开启数学公式支持 ，本文章是否开启<code>mathjax</code>，且需要在主题的 <code>_config.yml</code>文件中也需要开启才行</td></tr><tr class="even"><td>summary</td><td>无</td><td>文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr class="odd"><td>categories</td><td>无</td><td>文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr class="even"><td>tags</td><td>无</td><td>文章标签，一篇文章可以多个标签</td></tr><tr class="odd"><td>keywords</td><td>文章标题</td><td>文章关键字，SEO 时需要</td></tr><tr class="even"><td>reprintPolicy</td><td>cc_by</td><td>文章转载规则， 可以是 cc_by, cc_by_nd, cc_by_sa, cc_by_nc,cc_by_nc_nd, cc_by_nc_sa, cc0, noreprint 或 pay 中的一个</td></tr></tbody></table><blockquote><p><strong>注意</strong>: 1. 如果 <code>img</code>属性不填写的话，文章特色图会根据文章标题的 <code>hashcode</code>的值取余，然后选取主题中对应的特色图片，从而达到让所有文章的特色图<strong>各有特色</strong>。2. <code>date</code> 的值尽量保证每篇文章是唯一的，因为本主题中<code>Gitalk</code> 和 <code>Gitment</code> 识别 <code>id</code> 是通过<code>date</code> 的值来作为唯一标识的。 3.如果要对文章设置阅读验证密码的功能，不仅要在 Front-matter 中设置采用了SHA256 加密的 password 的值，还需要在主题的 <code>_config.yml</code>中激活了配置。有些在线的 SHA256 加密的地址，可供你使用：<a href="http://tool.oschina.net/encrypt?type=2">开源中国在线工具</a>、<a href="http://encode.chahuo.com/">chahuo</a>、<a href="http://tool.chinaz.com/tools/hash.aspx">站长工具</a>。 4.您可以在文章md文件的 front-matter 中指定 reprintPolicy来给单个文章配置转载规则</p></blockquote><p>以下为文章的 <code>Front-matter</code> 示例。</p><h3 id="最简示例">最简示例</h3><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">typora-vue-theme主题介绍</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-07 09:25:00</span><br><span class="hljs-meta"><code class="language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">typora-vue-theme主题介绍</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-07 09:25:00</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></tbody></table></figure><h3 id="最全示例">最全示例</h3><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre class=" language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">typora-vue-theme主题介绍</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-07 09:25:00</span><br><span class="hljs-attr">author:</span> <span class="hljs-string">赵奇</span><br><span class="hljs-attr">img:</span> <span class="hljs-string">/source/images/xxx.jpg</span><br><span class="hljs-attr">top:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">hide:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">cover:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">coverImg:</span> <span class="hljs-string">/images/1.jpg</span><br><span class="hljs-attr">password:</span> <span class="hljs-string">8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92</span><br><span class="hljs-attr">toc:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">mathjax:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">summary:</span> <span class="hljs-string">这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</span><br><span class="hljs-attr">categories:</span> <span class="hljs-string">Markdown</span><br><span class="hljs-attr">tags:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Typora</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Markdown</span><br><span class="hljs-meta"><code class="language-hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">typora-vue-theme主题介绍</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-07 09:25:00</span><br><span class="hljs-attr">author:</span> <span class="hljs-string">赵奇</span><br><span class="hljs-attr">img:</span> <span class="hljs-string">/source/images/xxx.jpg</span><br><span class="hljs-attr">top:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">hide:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">cover:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">coverImg:</span> <span class="hljs-string">/images/1.jpg</span><br><span class="hljs-attr">password:</span> <span class="hljs-string">8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92</span><br><span class="hljs-attr">toc:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">mathjax:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">summary:</span> <span class="hljs-string">这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</span><br><span class="hljs-attr">categories:</span> <span class="hljs-string">Markdown</span><br><span class="hljs-attr">tags:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Typora</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Markdown</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></tbody></table></figure><h2 id="自定制修改">自定制修改</h2><p>在本主题的 <code>_config.yml</code>中可以修改部分自定义信息，有以下几个部分：</p><ul><li>菜单</li><li>我的梦想</li><li>首页的音乐播放器和视频播放器配置</li><li>是否显示推荐文章名称和按钮配置</li><li><code>favicon</code> 和 <code>Logo</code></li><li>个人信息</li><li>TOC 目录</li><li>文章打赏信息</li><li>复制文章内容时追加版权信息</li><li>MathJax</li><li>文章字数统计、阅读时长</li><li>点击页面的'爱心'效果</li><li>我的项目</li><li>我的技能</li><li>我的相册</li><li><code>Gitalk</code>、<code>Gitment</code>、<code>Valine</code> 和<code>disqus</code> 评论配置</li><li><a href="http://busuanzi.ibruce.info/">不蒜子统计</a>和谷歌分析（<code>Google Analytics</code>）</li><li>默认特色图的集合。当文章没有设置特色图时，本主题会根据文章标题的<code>hashcode</code> 值取余，来选择展示对应的特色图</li></ul><p><strong>我认为个人博客应该都有自己的风格和特色</strong>。如果本主题中的诸多功能和主题色彩你不满意，可以在主题中自定义修改，很多更自由的功能和细节点的修改难以在主题的<code>_config.yml</code>中完成，需要修改源代码才来完成。以下列出了可能对你有用的地方：</p><h3 id="修改主题颜色">修改主题颜色</h3><p>在主题文件的 <code>/source/css/matery.css</code> 文件中，搜索<code>.bg-color</code> 来修改背景颜色：</p><figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre class=" language-hljs css"><span class="hljs-comment">/* 整体背景颜色，包括导航、移动端的导航、页尾、标签页等的背景颜色. */</span><br><span class="hljs-selector-class">.bg-color</span> {<br>    <span class="hljs-attribute">background-image</span>: <span class="hljs-built_in">linear-gradient</span>(to right, <span class="hljs-number">#4cbf30</span> <span class="hljs-number">0%</span>, <span class="hljs-number">#0f9d58</span> <span class="hljs-number">100%</span>);<br>}<br><br><span class="hljs-keyword">@-webkit-keyframes</span> rainbow {<br>   <span class="hljs-comment">/* 动态切换背景颜色. */</span><br>}<br><br><span class="hljs-keyword">@keyframes</span> rainbow {<br>    <span class="hljs-comment"><code class="language-hljs css"><span class="hljs-comment">/* 整体背景颜色，包括导航、移动端的导航、页尾、标签页等的背景颜色. */</span><br><span class="hljs-selector-class">.bg-color</span> {<br>    <span class="hljs-attribute">background-image</span>: <span class="hljs-built_in">linear-gradient</span>(to right, <span class="hljs-number">#4cbf30</span> <span class="hljs-number">0%</span>, <span class="hljs-number">#0f9d58</span> <span class="hljs-number">100%</span>);<br>}<br><br><span class="hljs-keyword">@-webkit-keyframes</span> rainbow {<br>   <span class="hljs-comment">/* 动态切换背景颜色. */</span><br>}<br><br><span class="hljs-keyword">@keyframes</span> rainbow {<br>    <span class="hljs-comment">/* 动态切换背景颜色. */</span><br>}<br></code></pre></td></tr></tbody></table></figure><h3 id="修改-banner-图和文章特色图">修改 banner 图和文章特色图</h3><p>你可以直接在 <code>/source/medias/banner</code> 文件夹中更换你喜欢的<code>banner</code> 图片，主题代码中是每天动态切换一张，只需<code>7</code> 张即可。如果你会 <code>JavaScript</code>代码，可以修改成你自己喜欢切换逻辑，如：随机切换等，<code>banner</code>切换的代码位置在 <code>/layout/_partial/bg-cover-content.ejs</code>文件的 <code>&lt;script&gt;&lt;/script&gt;</code> 代码中：</p><figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs javascript">$(<span class="hljs-string">'.bg-cover'</span>).<span class="hljs-title function_">css</span>(<span class="hljs-string">'background-image'</span>, <span class="hljs-string">'url(/medias/banner/'</span> + <span class="hljs-keyword">new</span> <span class="hljs-title class_">Date</span>().<span class="hljs-title function_">getDay</span>() + <span class="hljs-string"><code class="language-hljs javascript">$(<span class="hljs-string">'.bg-cover'</span>).<span class="hljs-title function_">css</span>(<span class="hljs-string">'background-image'</span>, <span class="hljs-string">'url(/medias/banner/'</span> + <span class="hljs-keyword">new</span> <span class="hljs-title class_">Date</span>().<span class="hljs-title function_">getDay</span>() + <span class="hljs-string">'.jpg)'</span>);<br></code></pre></td></tr></tbody></table></figure><p>在 <code>/source/medias/featureimages</code> 文件夹中默认有 24张特色图片，你可以再增加或者减少，并需要在 <code>_config.yml</code>做同步修改。</p><p>参考</p><p>[1] <a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">hexo-theme-matery中文文档</a></p>]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（四）自定义域名及百度谷歌收录</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-si-zi-ding-yi-yu-ming-ji-bai-du-gu-ge-shou-lu/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-si-zi-ding-yi-yu-ming-ji-bai-du-gu-ge-shou-lu/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex 多行公式等号对齐</title>
    <link href="/2023/06/01/latex-duo-xing-gong-shi-dui-qi/"/>
    <url>/2023/06/01/latex-duo-xing-gong-shi-dui-qi/</url>
    
    <content type="html"><![CDATA[<p>在 LaTeX中，使用“align”环境可以方便地实现等号对齐。使用“&amp;”符号可以在每行中分隔等号两边的内容，表示对齐位置，‘’\\‘’表示换行<span class="math inline">\(\lambda\)</span>：</p><figure class="highlight latex"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre class=" language-hljs latex"><span class="hljs-keyword">\begin</span>{aligned}<br>  2x + 3y <span class="hljs-built_in">&amp;</span>= 7 <span class="hljs-keyword">\\</span><br>  5x - 2y <span class="hljs-built_in">&amp;</span>= 1<br><span class="hljs-keyword"><code class="language-hljs latex"><span class="hljs-keyword">\begin</span>{aligned}<br>  2x + 3y <span class="hljs-built_in">&amp;</span>= 7 <span class="hljs-keyword">\\</span><br>  5x - 2y <span class="hljs-built_in">&amp;</span>= 1<br><span class="hljs-keyword">\end</span>{aligned}<br></code></pre></td></tr></tbody></table></figure><p>该代码将产生以下等式： <span class="math display">\[\begin{align}  2x + 3y &amp;= 7 \\\\  5x - 2y &amp;= 1\end{align}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github kex_exchange_identification Connection closed by remote host</title>
    <link href="/2023/05/31/github-kex-exchange/"/>
    <url>/2023/05/31/github-kex-exchange/</url>
    
    <content type="html"><![CDATA[<h2 id="问题描述">问题描述</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs bash">kex_exchange_identification: Connection closed by remote host<br>Connection closed by 20.205.243.166 port 22<br>fatal: Could not <span class="hljs-built_in"><code class="language-hljs bash">kex_exchange_identification: Connection closed by remote host<br>Connection closed by 20.205.243.166 port 22<br>fatal: Could not <span class="hljs-built_in">read</span> from remote repository.<br><br>Please make sure you have the correct access rights<br>and the repository exists.<br></code></pre></td></tr></tbody></table></figure><h2 id="解决方法">解决方法</h2><p><strong>本地生成.pub文件</strong></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class=" language-hljs bash">ssh-keygen -t ed25519 -C <span class="hljs-string"><code class="language-hljs bash">ssh-keygen -t ed25519 -C <span class="hljs-string">"your_mail@xxx.com"</span><br>ssh-agent bash<br></code></pre></td></tr></tbody></table></figure><p>会在<code>C:\Users\admin\.ssh</code>目录生成一个<code>id_ed25519.pub</code>文件，复制里面的内容。</p><p><strong>Github 新建 SSH key</strong></p><p>打开 Github，点击 Setting 如下图所示：</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531145549324.png" alt="Setting"><figcaption aria-hidden="true">Setting</figcaption></figure><p>在 Setting 页面中找到 SSH and GPG keys 选项，新建 SSH key。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531145936726.png" alt="SSH and GPG keys"><figcaption aria-hidden="true">SSH and GPG keys</figcaption></figure><p>Title 内容任意，Key 文本框内填入 <code>id_ed25519.pub</code>内的内容。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531150100824.png" alt="SSH keys"><figcaption aria-hidden="true">SSH keys</figcaption></figure><p>重新尝试上传。</p><p>如果仍然有上述错误提示可进行以下操作：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs shell"><code class="language-hljs shell">git init<br>git add .<br>git commit -m "init"<br>git branch -M main<br>git remote add origin git@github.com:xxx/xxx.git<br>git push -f -u origin main<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <categories>
      
      <category>Github</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Github</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（一）基础部署</title>
    <link href="/2023/05/31/github-pages-ge-ren-bo-ke-da-jian-yi/"/>
    <url>/2023/05/31/github-pages-ge-ren-bo-ke-da-jian-yi/</url>
    
    <content type="html"><![CDATA[<h2 id="本地创建环境">本地创建环境</h2><p>本人环境：</p><p><code>windows 10</code></p><h3 id="安装-node.js">安装 node.js</h3><p>建议使用 nvm （node version manager（node版本管理工具））安装node.js，</p><p><strong>下载地址</strong>：https://github.com/coreybutler/nvm-windows/releases</p><p><strong>安装</strong></p><p>（1）双击解压后的文件<code>nvm-setup.exe</code>； （2）选择 nvm安装路径（填坑警告：路径不能有空格！！！） （3）选择 node.js 路径；（4）确认安装； （5）检测：打开 cmd，输入<code>nvm</code>，显示当前 nvm版本以及 nvm 命令，成功！</p><p><strong>使用 nvm</strong></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><span class="hljs-comment"># 1.nvm list 命令 - 显示版本列表</span><br>nvm list // 显示已安装的版本（同 nvm list installed）<br>nvm list installed // 显示已安装的版本<br>nvm list available // 显示所有可以下载的版本<br><br><span class="hljs-comment"># 2.nvm install 命令 - 安装指定版本nodejs</span><br>nvm install 16.15.1// 安装16.15.1版本node<br>nvm install latest // 安装最新版本node<br><br><span class="hljs-comment"># 3.nvm use 命令 - 使用指定版本node</span><br>nvm use 16.15.1 // 使用16.15.1版本node<br><br><span class="hljs-comment"><code class="language-hljs bash"><span class="hljs-comment"># 1.nvm list 命令 - 显示版本列表</span><br>nvm list // 显示已安装的版本（同 nvm list installed）<br>nvm list installed // 显示已安装的版本<br>nvm list available // 显示所有可以下载的版本<br><br><span class="hljs-comment"># 2.nvm install 命令 - 安装指定版本nodejs</span><br>nvm install 16.15.1// 安装16.15.1版本node<br>nvm install latest // 安装最新版本node<br><br><span class="hljs-comment"># 3.nvm use 命令 - 使用指定版本node</span><br>nvm use 16.15.1 // 使用16.15.1版本node<br><br><span class="hljs-comment"># 4.nvm uninstall 命令 - 卸载指定版本 node</span><br>nvm uninstall 16.15.1 // 卸载16.15.1版本node<br></code></pre></td></tr></tbody></table></figure><p>填坑警告：nvm install 的时候，出现无权安装，需<code>以管理员身份</code>运行 cmd。！！！</p><p>本人目前安装 node.js 版本为16.15.1.</p><p><strong>设置nodejsprefix（全局）和cache（缓存）路径（非必须操作）</strong></p><p>在<code>nodejs</code>安装目录下新建两个文件夹，用于存放全局包和缓存，如下： 我的 node.js安装目录：<code>E:\Program\nvm</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class=" language-hljs bash">npm config <span class="hljs-built_in">set</span> prefix <span class="hljs-string">"E:\Program\nvm\node_gobal"</span> <br>npm config <span class="hljs-built_in">set</span> cache <span class="hljs-string"><code class="language-hljs bash">npm config <span class="hljs-built_in">set</span> prefix <span class="hljs-string">"E:\Program\nvm\node_gobal"</span> <br>npm config <span class="hljs-built_in">set</span> cache <span class="hljs-string">"E:\Program\nvm\node_cache"</span><br></code></pre></td></tr></tbody></table></figure><h3 id="安装-hexo">安装 Hexo</h3><p>命令行输入以下命令安装 Hexo：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">npm install -g hexo-cli<br></code></pre></td></tr></tbody></table></figure><p>输入以下命令验证是否安装成功：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">hexo -v<br></code></pre></td></tr></tbody></table></figure><h2 id="本地部署">本地部署</h2><p>选择一个准备放置博客网站的目录，然后使用以下命令来初始化一个项目：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre class=" language-hljs bash">hexo init LFD-byte.github.io<br><span class="hljs-built_in"><code class="language-hljs bash">hexo init LFD-byte.github.io<br><span class="hljs-built_in">cd</span> LFD-byte.github.io<br>npm install<br></code></pre></td></tr></tbody></table></figure><p>该命令将会在当前目录下，生成一个名为 <code>LFD-byte.github.io</code>的新目录，当然，你可以把这个名字换成任何你想要的名字，并将<code>hexo</code> 的初始化文件写入其中。</p><p>新建完成后，<code>LFD-byte.github.io</code> 文件夹的目录如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre class=" language-hljs bash">.<br>├── _config.yml <br>├── package.json <br>├── node_modules <br>├── scaffolds <br>├── <span class="hljs-built_in"><code class="language-hljs bash">.<br>├── _config.yml <br>├── package.json <br>├── node_modules <br>├── scaffolds <br>├── <span class="hljs-built_in">source</span> <br>| ├── _drafts <br>| └── _posts <br>└── themes<br></code></pre></td></tr></tbody></table></figure><p><code>_config.yml</code>是配置文件，里面有很多可以配置的数据，这里暂时不多介绍，后面的文章里会进行详细说明。</p><p><code>package.json</code> 是应用程序信息，通常不需要关心。</p><p><code>node_modules</code> 用来存放 <code>node</code>相关的模块，通常不需要关心。</p><p><code>scaffolds</code>里面是模版文件，也就是每次新建文章时，都会根据模版文件来创建对应的<code>md</code> 文件，这一点也会在后续的文章里进行详细介绍。</p><p><code>source</code> 是资源文件夹，用来存放用户资源的地方。除<code>_posts</code> 文件夹之外，开头命名为 _ (下划线)的文件 /文件夹和隐藏的文件将会被忽略。</p><p><code>theme</code>是主题文件夹，每个主题的配置都会有些不一样，需要根据具体主题情况来定，后续介绍主题的文章里会有说明。</p><p>在 <code>breeze-blog</code> 目录下使用以下命令来运行我们的博客：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">hexo server<br></code></pre></td></tr></tbody></table></figure><p>在默认情况下，服务会使用 <code>4000</code>端口，如果已经被占用，也可以添加 <code>-p</code>参数来换用其它端口：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">hexo server -p 8080<br></code></pre></td></tr></tbody></table></figure><p>打开 <code>http://localhost:4000</code>即可访问我们生成的网站了。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedntX31VrhMTjFozQ.jpg" alt="Hexo"><figcaption aria-hidden="true">Hexo</figcaption></figure><p>这样，我们的博客就搭建起来了。</p><h2 id="部署到-github-pages">部署到 Github Pages</h2><p>在你的 Github 账号创建一个新的仓库，仓库命名规范为<code>账号名.github.io</code>，公开仓库。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230604151148301.png" alt="create repository"><figcaption aria-hidden="true">create repository</figcaption></figure><p>在 Git bash 或 CMD 中<code>LFD-byte.github.io</code>博客目录下执行以下命令连接到 Github：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre class=" language-hljs bash">git init<br>git add .<br>git commit -m <span class="hljs-string"><code class="language-hljs bash">git init<br>git add .<br>git commit -m <span class="hljs-string">"first commit"</span><br>git branch -M main<br>git remote add origin git@github.com:xxx/xxx.git<br>git push -u origin main<br></code></pre></td></tr></tbody></table></figure><p>在 <code>LFD-byte.github.io</code> 仓库下 Settings 中开启 GithubPages：</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230604152021402.png" alt="open github pages"><figcaption aria-hidden="true">open github pages</figcaption></figure><p>然后我们修改一下本地的 <code>hexo</code>的配置文件(<code>_config.yml</code>)，我的在<code>LFD-byte.github.io</code>根目录下，找到对应的地方进行修改，指定我们的仓库信息，并修改<code>deploy</code> 信息。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre class=" language-hljs bash">deploy:<br><span class="hljs-built_in"><code class="language-hljs bash">deploy:<br><span class="hljs-built_in">type</span>: git<br>repo: git@github.com:xxx/blogxxx<br>branch: main<br></code></pre></td></tr></tbody></table></figure><p>把这里的 <code>repo</code> 地址修改为你的仓库地址即可。</p><p>安装 <code>hexo-deployer-git</code>：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><span class="hljs-built_in"><code class="language-hljs bash"><span class="hljs-built_in">cd</span> LFD-byte.github.io<br>npm install hexo-deployer-git --save<br></code></pre></td></tr></tbody></table></figure><p>进行部署</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">hexo clean && hexo generate<br>hexo deploy<br></code></pre></td></tr></tbody></table></figure><p>运行完成后，我们的博客文件就顺利部署到 <code>github pages</code>上了，现在我们打开下面网址来查看我们的博客效果： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">https://用户名.github.io<br></code></pre></td></tr></tbody></table></figure><p></p><p>之后每次我们添加或修改完本地文件后，使用：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">hexo clean && hexo g -d<br></code></pre></td></tr></tbody></table></figure><p>即可重新生成项目文件。</p><h2 id="部署到-vercel">部署到 Vercel</h2><p>注册 <a href="https://vercel.com/">Vercel</a>账号，建议谷歌邮箱注册。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedvercel-logo-freelogovectors.net.jpg" alt="vercel"><figcaption aria-hidden="true">vercel</figcaption></figure><p>使用 GitHub 账户登录 <a href="https://vercel.com/">Vercel</a>，授予Vercel repo 的 read 权限。</p><p>导入 GitHub 账户中的网站 repo，比如此处的 LFD-byte.github.io。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205007431.png" alt="Project import"><figcaption aria-hidden="true">Project import</figcaption></figure><p>在项目构建中，Framework Preset 选择 Other，其余不做改动。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205116417.png" alt="Build"><figcaption aria-hidden="true">Build</figcaption></figure><p>稍等片刻，部署成功，此时我们就可以直接通过部署完成后 Vercel提供的域名访问个人网站了，点击 Visit 进行访问。</p><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205341204.png" alt="Production Deployment"><figcaption aria-hidden="true">Production Deployment</figcaption></figure><figure><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205507057.png" alt="Visit Blog"><figcaption aria-hidden="true">Visit Blog</figcaption></figure><p>之后每次我们添加或修改完本地文件后，使用：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs bash"><code class="language-hljs bash">hexo clean && hexo g -d<br></code></pre></td></tr></tbody></table></figure><p>就可以生成静态文件同步部署到 Vercel 上了。</p><h2 id="注意">注意</h2><p>若 npm 不能下载包，可通过 nvm 更换 node.js 版本重新尝试下载。</p><h2 id="参考">参考</h2><p>[1] <a href="https://mfrank2016.github.io/breeze-blog/2020/05/02/hexo/hexo-start/">【Hexo】使用Hexo+githubpages+travis ci搭建好看的个人博客（一）</a></p><p>[2] <a href="https://blog.csdn.net/liangpingguo/article/details/125324362">Windows下使用nvm安装nodejs</a></p><p>[3] <a href="https://blog.csdn.net/weixin_40026797/article/details/126919662">建站过程中的踩坑记录：自定义域名、百度收录与备案</a></p>]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（五）图神经网络（GNN）</title>
    <link href="/2023/05/31/tu-shen-jing-wang-luo-xue-xi-ri-ji-wu/"/>
    <url>/2023/05/31/tu-shen-jing-wang-luo-xue-xi-ri-ji-wu/</url>
    
    <content type="html"><![CDATA[<p><strong>置换不变性和置换同变性</strong></p><p>任何将邻接矩阵<span class="math inline">\(A\)</span>作为输入的函数<span class="math inline">\(f\)</span>在理想状态下，都应满足下面两个条件之一：<span class="math display">\[f(PAP^T)=f(A)\text{(置换不变)} \\\\f(PAP^T)=Pf(A)\text{(置换同变)}\]</span></p><p>其中<span class="math inline">\(P\)</span>是置换矩阵。置换不变是指函数不依赖邻接矩阵中行/列的任意顺序，置换同变表示当置换邻接矩阵时<span class="math inline">\(f\)</span>的输出以一致的方式置换。</p><h2 id="神经消息传递">神经消息传递</h2><h3 id="gnn-框架">GNN 框架</h3><p>在 GNN 的每个消息传递迭代期，通过聚合每个节点<span class="math inline">\(u \in \mathcal{V}\)</span>的邻域<span class="math inline">\(\mathcal{N}(u)\)</span>的信息来更新其隐藏嵌入<span class="math inline">\(h_u^{(k)}\)</span>，过程如下式所示： <span class="math display">\[\begin{align}  h_u^{(k+1)} &amp; = UPDATE^{(k)}(h_u^{(k)},AGGREGATE^{(k)}(\{h_v^{(k)}, \forall v \in \mathcal{V}(u)\})) \\\\  &amp; = UPDATE^{(k)}(h_u^{(k)}, m_{\mathcal{N}(u)}^{(k)})\end{align}\]</span></p><p>其中 <span class="math inline">\(UPDATA\)</span> 和 <span class="math inline">\(AGGREGATE\)</span> 是任意可微函数，<span class="math inline">\(m_{\mathcal{N}(u)}\)</span> 是聚合节点 <span class="math inline">\(u\)</span>邻域消息的结果，上标表示消息迭代期的索引。</p><p>迭代最后一层的输出定义为每个节点的嵌入： <span class="math display">\[z_u = h_u^{(K)}, \forall u \in \mathcal{V}\]</span> 由于 <span class="math inline">\(AGGREGATE\)</span>函数将整个集合作为输入，这种方式定义的 GNN 是置换同变的。</p><p>节点嵌入编码了两种形式的信息。</p><p><strong>图的结构信息</strong></p><p><strong>基于节点特征的信息</strong></p><h3 id="gnn-实例">GNN 实例</h3><p>基本 GNN 的消息传递定义如下式： <span class="math display">\[h_u^{(k)} = \sigma(W_{self}^{(k)}h_u^{(k-1)} + W_{neigh}^{(k)}\sum_{v\in \mathcal{N}(u)} h_v^{(k-1)} + b^{(k)})\]</span> 其中，<span class="math inline">\(W_{self}^{(k)}\)</span> 和<span class="math inline">\(W_{neigh}^{(k)} \in \mathbb{R}^{d^{(k)}\times d^{(k-1)}}\)</span> 是可训练参数矩阵，<span class="math inline">\(\sigma\)</span> 表示逐元素的非线性函数。</p><p>通过定义更新和聚合函数等效地定义基本的 GNN： <span class="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)} h_v \\\\UPDATE(h_u, m_{\mathcal{N}(u)}) = \sigma (W_{self} h_u +W_{neigh}m_{\mathcal{N}(u)})\]</span> 下式可作为从节点 <span class="math inline">\(u\)</span>的图上邻域聚合消息的简写： <span class="math display">\[m_{\mathcal{N}(u)} = AGGREGATE^{(k)}(\{h_v^{(k)}, \forall v \in\mathcal{N}(u)\})\]</span> <strong>图级别 GNN 定义</strong> <span class="math display">\[H^{(k)} = \sigma(AH^{(k-1)}W_{neigh}^{(k)} + H^{(k-1)}W_{self}^{(k)})\]</span> 其中，<span class="math inline">\(H^{(k)} \in\mathbb{R}^{|\mathcal{V}| \times d}\)</span> 表示 GNN 中第 <span class="math inline">\(k\)</span>层的节点表示矩阵（每个节点对应矩阵的一行），<span class="math inline">\(A\)</span> 是邻接矩阵。</p><p><strong>自环消息传递</strong></p><p>添加自环并省略显示的更新步骤消息传递可定义如下： <span class="math display">\[h_u^{(k)} = AGGREGATE(\{ h_v^{(k-1)}, \forall v \in \mathcal{N}(n)\bigcup \{u\} \})\]</span> 其中，聚合在集合 <span class="math inline">\(\mathcal{N}(u)\bigcup \{u\}\)</span>上进行。这种消息传递方式可以缓解拟合问题，也因为无法区分节点和邻域的信息严重限制了GNN 的表达能力。</p><p>在基本 GNN 模型中，添加自环等效于在 <span class="math inline">\(W_{self}\)</span> 和 <span class="math inline">\(W_{neigh}\)</span>矩阵之间共享参数，<strong>图级别更新方式</strong>如下所示： <span class="math display">\[H^{(t)} = \sigma ((A+I)H^{(t-1)}W^{(t)})\]</span></p><h2 id="广义邻域聚合">广义邻域聚合</h2><h3 id="邻域归一化">邻域归一化</h3><p>最基本的邻域聚合函数仅取邻居嵌入的总和，这种方法有可能不稳定并且对节点度高度敏感。解决该问题方案之一是基于所涉及节点的度来归一化聚合操作。</p><p>均值代替求和，如下式所示： <span class="math display">\[m_{\mathcal{N}(u)} = \frac{\sum_{v \in\mathcal{N}(n)}h_v}{|\mathcal{N}(u)|}\]</span> 对称归一化，如下式所示： <span class="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)}\frac{h_v}{\sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}}\]</span> 图卷积神经网络（GCN）</p><p>采用对称归一化聚合及自环更新方法，GCN 消息传递函数如下式定义： <span class="math display">\[h_u^{(k)} = \sigma (W^{(k)} \sum_{v \in \mathcal{N}(u)} \bigcup \{u\}\frac{h_v}{|\mathcal{N}(u)||\mathcal{N}(v)|})\]</span> 是否归一化？</p><p>归一化可能导致信息丢失，在归一化后可能很难使用学习到的嵌入来区分不同度的节点，并且归一化会掩盖各种其他的图结构特征。</p><p>在通常情况下，在节点特征信息远比结构信息有用或由于节点度范围过于广泛导致优化过程可能不稳定的任务中，归一化最有用。</p><h3 id="集合聚合操作">集合聚合操作</h3><p><strong>集合池化</strong></p><p>一种定义聚合函数的原则是基于置换不变神经网络的理论，具有下式的聚合函数是通用的集合函数逼近器：<span class="math display">\[m_{\mathcal{N}(u)} = MLP_{\theta} (\sum_{v \in \mathcal{N}(u)}MLP_{\phi}(h_v))\]</span> 依照惯例用 <span class="math inline">\(MLP_{\theta}\)</span>表示可训练参数为 <span class="math inline">\(\theta\)</span>的任一深度多层感知器。将一组嵌入映射到一个嵌入的任何置换不变函数都可以基于上式的模型逼近到任意精度。</p><p><strong>Janossy 池化</strong></p><p>不使用置换不变的压缩方法（如求和或取均值），而是采用置换敏感的函数并对多种可能的置换取均值。具体操作为：令<span class="math inline">\(\pi_i \in \Pi\)</span> 表示将集合 <span class="math inline">\(\{h_v, \forall v \in \mathcal{N}(u)\}\)</span>映射到特定序列 <span class="math inline">\(((h_{v_1}, h_{v_2}, \cdots,h_{v_{| \mathcal{N}(u) |}})_{\pi_i})\)</span> 的置换函数。即 <span class="math inline">\(\pi_i\)</span>将无序的邻居嵌入集置于任意排列的序列中。然后通过 Janossy池化实现邻域聚合，如下式所示：</p><p><span class="math display">\[m_{\mathcal{N}(u)} = MLP_{\theta} (\frac{1}{|\Pi|} \sum_{\pi \in \Pi}\rho_{\phi} (h_{v_1}, h_{v_2}, \cdots,h_{v_{|\mathcal{N}(u)|}})_{\pi_i})\]</span></p><p>其中，<span class="math inline">\(\Pi\)</span> 表示一组置换函数，<span class="math inline">\(\rho_{\phi}\)</span>是置换敏感的函数（如应用于序列数据集的神经网络）。在实践中，通常将 <span class="math inline">\(\rho_{\phi}\)</span> 定义为 LSTM。</p><p>如果上式中的置换函数集合 <span class="math inline">\(\Pi\)</span>包含所有可能的置换函数，则上式中的聚合函数也是通用集合函数逼近器。但是对所有可能的置换求和很困难，在实践中通常采用如下两种方法进行Janossy 池化：</p><p>1在每次应用聚合函数时，对所有可能的置换采样出一个随机子集，并且对该随机自己进行求和。</p><p>2对邻域中的节点进行规范化排序，例如，根据节点度对节点进行降序排序，并随机断开一些关联关系。</p><h3 id="邻域注意力模型">邻域注意力模型</h3><p>基本思想是为每个邻域中的节点分配注意力权重，该权重用于在聚合步骤中权衡该节点的影响力。第一个引入注意力机制的GNN模型是图注意力网络GAT，该网络使用注意力权重来定义邻域的加权和，如下式所示： <span class="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)} \alpha_{u,v}h_v\]</span> 其中，<span class="math inline">\(\alpha_{u,v}\)</span>表示在节点 <span class="math inline">\(u\)</span>处聚合信息时，其邻域中的节点 <span class="math inline">\(v \in\mathcal{N}(u)\)</span> 的注意力权重。GAT 中注意力权重的定义如下式所示：<span class="math display">\[\alpha_{u,v} = \frac{exp([Wh_v \bigoplus Wh_v])}{\sum_{v' \in\mathcal{N}(u)} exp(a^T[Wh_v \bigoplus Wh_{v'}])}\]</span> 其中，<span class="math inline">\(a\)</span>是可训练的注意力向量，<span class="math inline">\(W\)</span>是可训练的矩阵，<span class="math inline">\(\bigoplus\)</span>表示拼接操作。</p><p>注意力机制变体： <span class="math display">\[\alpha_{u,v} = \frac{exp(h_u^TWh_v)}{\sum_{v' \in \mathcal{N}(u)}exp(h_u^TWh_{v'})}\]</span> MLP 注意力层的变体： <span class="math display">\[\alpha_{u,v} = \frac{exp(MLP(h_u,h_v))}{\sum_{v' \in \mathcal{N}(u)}exp(MLP(h_u,h_{v'}))}\]</span> 上式限定 MLP 输出为标量。</p><p>添加多注意力头，使用彼此独立的 <span class="math inline">\(K\)</span>个注意力层计算 <span class="math inline">\(K\)</span> 个不同的注意力权重<span class="math inline">\(\alpha_{u,v,k}\)</span>，然后使用不同的注意力权重聚合的消息会在聚合步骤中进行转换和合并，通常是先进性线性映射，再进行拼接操作，如下式所示：<span class="math display">\[\begin{aligned}m_{\mathcal{N}(u)} &amp;= [a_1 \bigoplus a_2 \bigoplus \cdots \bigoplusa_K] \\\\a_k &amp;= W_i \sum_{v \in \mathcal{N}(u)} \alpha_{u,v,k}h_v\end{aligned}\]</span> 其中，<span class="math inline">\(K\)</span>个注意力头中的每一个注意力权重 <span class="math inline">\(\alpha_{u,v,k}\)</span>可以使用上述任何一种注意力机制进行计算。</p><h2 id="广义更新方法">广义更新方法</h2><p><strong>过度平滑和邻域影响</strong></p><p>GNN 的一个常见问题是过度平滑。过度平滑的基本原理是：经过多次 GNN消息传递后，图中所有节点的表示可能变得非常相似。过度平滑导致无法建立更深的GNN 模型以利用图上的长期依赖关系，因为这些深层的 GNN模型往往会生成过度平滑的嵌入。</p><p>可以通过定义每个节点的输入特征 <span class="math inline">\(h_u^{(0)}=x_u\)</span>对图上其它节点的最终层输出的嵌入（<span class="math inline">\(h_v^{(K)},\forall v \in\mathcal{V}\)</span>）的影响来形式化定义 GNN中的过度平滑问题。对于任意一对节点 <span class="math inline">\(u\)</span> 和 <span class="math inline">\(v\)</span>，可以通过检查相应的雅可比矩阵的大小来量化 GNN 中节点 <span class="math inline">\(u\)</span> 对节点 <span class="math inline">\(v\)</span> 的影响，如下式所示： <span class="math display">\[I_{K(u,v)} = 1^T(\frac{\partial h_v^{(K)}}{\partial h_u^{(0)}})\]</span> 其中，<span class="math inline">\(1\)</span> 是元素全为 1的向量。<span class="math inline">\(I_{K(u,v)}\)</span> 是雅可比矩阵<span class="math inline">\(\frac{\partial h_v^{(K)}}{\partialh_u^{(0)}}\)</span> 中的元素之和。用来衡量 GNN 中节点 <span class="math inline">\(u\)</span> 的初始嵌入对节点 <span class="math inline">\(v\)</span> 的最终嵌入的影响程度。</p><p>定理：对于任何使用自环更新方法和用下式表示聚合函数的 GNN 模型 <span class="math display">\[AGGRGATE(\{h_v, \forall v \in \mathcal{N}(u) \bigcup \{u\}\}) =\frac{1}{f_n(|\mathcal{N}(u) \bigcup \{u\}|)} \sum_{v \in \mathcal{N}(u)\bigcup \{u\}} h_v\]</span> 其中，<span class="math inline">\(f:\mathbb{R}^+ \rightarrow\mathbb{R}^+\)</span> 是任意可微的归一化函数。</p><p>可得出下式结论： <span class="math display">\[I_K (u,v) \propto p_{\mathcal{G},K}(u|v)\]</span> 其中，<span class="math inline">\(p_{\mathcal{G},K}(u|v)\)</span> 表示从节点 <span class="math inline">\(u\)</span> 开始的 <span class="math inline">\(K\)</span> 步随机游走过程中访问节点 <span class="math inline">\(v\)</span> 的概率。</p><p>当使用 <span class="math inline">\(K\)</span> 层 GCN 型魔性时，节点<span class="math inline">\(u\)</span> 对节点 <span class="math inline">\(v\)</span> 的影响从节点 <span class="math inline">\(u\)</span> 开始经过 <span class="math inline">\(K\)</span> 步随机游走到达节点 <span class="math inline">\(v\)</span> 的概率成正比。但是，随着 <span class="math inline">\(K \rightarrow\infty\)</span>，每个节点的影响都接近图上随机游走的平稳分布，这意味着本地邻域信息会丢失。</p><p>上述定理直接适用于使用自环更新方法的模型，但是只要任意层 <span class="math inline">\(k\)</span> 满足 <span class="math inline">\(\|W_{self}^{(k)} \| &lt; \| W_{neigh}^{(k)}\|\)</span>，其结果也可以渐近地扩展基本 GNN 的更新。因此当使用简单 GNN模型时，构建更深的模型实际上会损害模型性能。随着更多层的加入，模型将丢失更多关于本地邻域结构的信息，并且学习的嵌入会变得过于平滑，接近几乎均匀的分布。</p><h3 id="拼接和跳跃连接">拼接和跳跃连接</h3><p>最简单更新跳跃连接的方式之一是使用拼接操作再消息传递期间保留更多节点级别信息，如下式所示：<span class="math display">\[UPDATE_{concat}(h_u, m_{\mathcal{N}(u)}) =[UPDATE_{base}(h_u,m_{\mathcal{N}(u)}) \bigoplus h_u]\]</span>其中，直接将基本更新函数的输出与节点的上一层表示拼接，鼓励模型再消息传递过程中解耦信息，将来自邻域的信息（<span class="math inline">\(m_{\mathcal{N}(u)}\)</span>）与每个节点当前的表示（<span class="math inline">\(h_u\)</span>）分开。</p><p>线性插值法跳跃连接，如下式所示： <span class="math display">\[UPDATE_{interpolate}(h_u,m_{\mathcal{N}(u)}) = \alpha_1 \circUPDATE_{base}(h_u,m_{\mathcal{N}(u)}) + \alpha_2 \bigodot h_u\]</span> 其中，<span class="math inline">\(\alpha_1,\alpha_2 \in[0,1]^d\)</span> 是满足 <span class="math inline">\(\alpha_2 = 1 -\alpha_1\)</span> 的门控向量，<span class="math inline">\(\circ\)</span>表示逐元素相乘。</p><p>最终更新的表示是先前表示与基于邻域信息进行更新的表示之间的线性插值。</p><p>在通常情况下，拼接和跳跃连接有助于缓解 GNN中过渡平滑问题，同时可以提高优化数值的稳定性。</p><h3 id="门控更新函数">门控更新函数</h3><p>一种解读 GNN消息传递算法的观点是：聚合函数从邻域接收观察结果，然后将其用于更新每个节点的隐状态。基于这一观点可以根据观察结果直接使用更新RNN 框架的隐状态的方法，最早的 GNN 架构之一定义更新函数如下式所示：<span class="math display">\[h_u^{(k)} = GRU(h_u^{(k-1)},m_{\mathcal{N}(u)}^{(k)})\]</span> 其中，GRU 表示 GRU 单元的更新函数。</p><p>门控更新方法在提高 GNN框架的模型深度（超过10层）和防止过度平滑问题方面非常有效。</p><h3 id="跳跃知识连接">跳跃知识连接</h3><p>提高最终的节点表示质量的一种补充策略是利用消息传递的每一层输出的表示，叫做加入跳跃知识，如下式所示：<span class="math display">\[z_u = f_{JK}(h_u^{(0)} \bigoplus h_u^{(1)} \bigoplus \cdots \bigoplush_u^{(K)})\]</span> 其中，<span class="math inline">\(f_{JK}\)</span>是任意微分函数。</p><h2 id="边特征和多元关系-gnn">边特征和多元关系 GNN</h2><p>下面介绍 GNN 在多元关系图或其它异构图中的应用。</p><h3 id="关系-gnn">关系 GNN</h3><p>关系图卷积网络（RGCN），通过为每种关系类型指定一个单独的变化矩阵来增强聚合函数处理多种关系的能力，如下式所示：<span class="math display">\[m_{\mathcal{N}(u)} = \sum_{\tau \in \mathcal{R}} \sum_{v \in\mathcal{N}_{\tau}(u)} \frac{W_{\tau}h_v}{f_n(\mathcal{N}(u),\mathcal{N}(v))}\]</span> 其中， <span class="math inline">\(f_n\)</span>是一个归一化函数，它的值取决于节点 <span class="math inline">\(u\)</span> 的邻域以及被聚合的节点 <span class="math inline">\(v\)</span> 的邻域。RGCN中的多元关系聚合类似具有归一化函数的基本GNN，但根据边的类型不同分别聚合信息。</p><p><strong>参数共享</strong></p><p>朴素 RGCN方法的一个缺点是由于每一种关系类型都需要一个可训练的矩阵导致参数量急剧增加，这种参数量的激增可能导致过拟合和训练缓慢的问题。</p><p>通过与基矩阵共享参数的方法来解决此问题，如下式： <span class="math display">\[W_{\tau} = \sum_{i=1}^b \alpha_{i,\tau}B_i\]</span> 该方法中，所有关系矩阵都定义为 <span class="math inline">\(b\)</span> 个基矩阵（<span class="math inline">\(B1,\cdots,B_b\)</span>）的线性组合；唯一的关于关系的参数是每种关系<span class="math inline">\(\tau\)</span> 的 <span class="math inline">\(b\)</span> 个组合权重 <span class="math inline">\(\alpha_{1,\tau}, \cdots,\alpha_{b,\tau}\)</span>。在这种基本共享方法中，看可以将完整聚合函数表示为下式：<span class="math display">\[m_{\mathcal{N}(u)} = \sum_{\tau \in \mathcal{R}} \sum_{v \in\mathcal{N}_{\tau}(u)} \frac{\alpha_{\tau} \times_{1} B \times_{2}h_v}{f_n(\mathcal{N}(u), \mathcal{N}(v))}\]</span> 其中，<span class="math inline">\(B = (B_1, \cdots,B_b)\)</span> 是一个由基矩阵堆叠构成的张量， <span class="math inline">\(\alpha_{\tau} = \alpha_{1,\tau}, \cdots,\alpha_{b,\tau}\)</span> 是一个关于关系 <span class="math inline">\(\tau\)</span> 的包含基矩阵组合权重的向量，<span class="math inline">\(\times_{i}\)</span> 表示沿着模 <span class="math inline">\(i\)</span> 的张量积。另一种理解参数共享 RGCN方法的过程是：学习每个关系的嵌入及所有关系之间共享的张量。</p><h3 id="注意力机制和特征拼接">注意力机制和特征拼接</h3><p>为适应更一般形式的边特征的情况，可以在消息传递过程中基于注意力机制或将这些信息与邻域嵌入拼接来充分利用这些特征。在给定任意基本聚合方法<span class="math inline">\(AGGREGATE_{base}\)</span>的情况下，利用边特征的一种简单策略是如下式定义新的聚合函数： <span class="math display">\[m_{\mathcal{N}(u)} = AGGREGATE_{base}(\{h_v \bigoplus e_{(u,\tau,v)},\forall v \in \mathcal{N}(u) \})\]</span> 其中，<span class="math inline">\(e_{(u,\tau,v)}\)</span>表示边 <span class="math inline">\((u,\tau,v)\)</span> 的特征。</p><h2 id="图池化">图池化</h2><p><strong>集合池化方法</strong></p><p>与 AGGREGATE操作类似，图池化任务可以看作是解决集合上的问题。要设计一个池化函数 <span class="math inline">\(f_p\)</span> 将一组节点嵌入 <span class="math inline">\(\{ z_1, \cdots, z_{|V|} \}\)</span>映射为表示整张图的嵌入 <span class="math inline">\(z_{\mathcal{G}}\)</span>。</p><p>第一种常用方法是对节点嵌入求和（或取均值），如下式所示： <span class="math display">\[z_{\mathcal{G}} = \frac{\sum_{v \in \mathcal{V}}z_c}{f_n(|\mathcal{V}|)}\]</span> 其中，<span class="math inline">\(f_n\)</span>是归一化函数（如恒等函数）。适用于小规模图。</p><p>第二种常用方法基于集合的方法，结合了 LSTM和注意力机制来池化节点嵌入。这种池化方法迭代 <span class="math inline">\(t=1,\cdots,T\)</span>步基于注意力机制的聚合操作，如下式所示： <span class="math display">\[\begin{aligned}q_i &amp;= LSTM(o_{t-1}, q_{t-1}) \\\\e_{v,t} &amp;= f_a(z_v,q_t),\forall v \in \mathcal{V} \\\\a_{v,t} &amp;= \frac{exp(e_{v,i})}{\sum_{u \in \mathcal{V}} e_{u,t}},\forall v \in \mathcal{V} \\\\o_t &amp;= \sum_{v \in \mathcal{V}} a_{v,t} z_v\end{aligned}\]</span></p><p>其中，<span class="math inline">\(q_t\)</span> 表示每次迭代 <span class="math inline">\(t\)</span>次注意力机制中的查询向量。查询向量用于使用注意力函数 <span class="math inline">\(f_a: \mathbb{R}^d \times \mathbb{R} \rightarrow\mathbb{R}\)</span>（如点积）为每个节点计算注意力分数，然后将该注意力分数进行归一化，最后根据注意力权重计算节点嵌入的加权和，并基于该加权和采用LSTM 更新来更新查询向量。通常情况下，用全零向量初始化 <span class="math inline">\(q_0\)</span> 和 <span class="math inline">\(o_0\)</span> ，进行了 <span class="math inline">\(T\)</span> 次迭代后计算整张图的嵌入如下式所示：<span class="math display">\[z_{\mathcal{G}} = o_1 \bigoplus o_2 \bigoplus \cdots \bigoplus o_T\]</span> <strong>图粗糙化方法</strong></p><p>集合池化方法的局限性在于不能利用图的结构信息。在池化阶段利用图的拓扑信息可以进一步提供增益，实现次目的的一种流形策略是用图聚类或粗糙化作为池化节点表示的一种方法。</p><p>假设有聚类函数如下式所示： <span class="math display">\[f_c \rightarrow \mathcal{G} \times \mathbb{R}^{|V| \times d} \rightarrow\mathbb{R}^{+|V| \times c}\]</span> 聚类函数将图上的所有节点分为 <span class="math inline">\(c\)</span> 个簇。假定该函数输出一个分配矩阵 <span class="math inline">\(S = f_c(\mathcal{G}, Z)\)</span>，其中，<span class="math inline">\(S[u,i] \in \mathbb{R}\)</span> 表示节点 <span class="math inline">\(u\)</span> 和簇 <span class="math inline">\(i\)</span> 之间的关联强度。</p><p>图粗糙化方法的关键思想是使用聚类分配矩阵来粗糙化图。这里使用分配矩阵<span class="math inline">\(S\)</span>来计算新的粗糙化邻接矩阵和一个新的节点特征集合，如下式所示： <span class="math display">\[\begin{aligned}A^{new} &amp;= S^TAS \in \mathbb{R}^{+c \times c} \\\\X^{new} &amp;= S^TX \in \mathbb{R}^{c \times d}\end{aligned}\]</span>这一新的邻接矩阵表示图中的簇之间的关联强度（边），而新的特征矩阵表示聚合分配给每个簇的所有节点嵌入的结果。在该粗糙化的图上运行GNN，并在每次迭代的过程中重复粗糙化过程，图在每一步都会减小，最后在足够粗糙化的图上对节点嵌入执行集合池化可以获得图的最终表示。</p><h2 id="通用消息传递方法">通用消息传递方法</h2><p>GNN消息传递方法可以泛化为在消息传递的每个阶段利用边和图级别的信息。</p><p>更为通用的消息传递方法如下式： <span class="math display">\[\begin{aligned}h_{(u,v)}^{(k)} &amp;= UPDATE_{edge}(h_{(u,v)}^{(k-1)}, h_u^{(k-1)},h_v^{(k-1)}, h_{\mathcal{G}}^{(k-1)}) \\\\m_{\mathcal{N}(u)} &amp;= AGGREGATE_{node}(\{ h_{(u,v)}^{(k-1)}, \forallv \in \mathcal{N}(u) \}) \\\\h_u^{(k)} &amp;= UPDATE_{node}(h_u^{(k-1)}, m_{\mathcal{N}(u)},h_{\mathcal{G}}^{(k-1)}) \\\\h_{\mathcal{G}}^{(k)} &amp;= UPDATE_{graph}(h_{\mathcal{G}}^{(k-1)}, \{h_u^{(k-1)}, \forall u \in \mathcal{V}, \{ h_{(u,v)}^{(k)}, \forall(u,v) \in \varepsilon \} \})\end{aligned}\]</span> 通用消息传递框架中，在消息传递过程中为图上的每条边生成嵌入<span class="math inline">\(h_(u,v)^{(k)}\)</span>，并为整张图生成相应的嵌入<span class="math inline">\(h_{\mathcal{G}}^{(k)}\)</span>，这使得消息传递模型可以聚合边和图级别的特征。</p><p>在通用消息传递框架中进行消息传递时，首先根据便关联的节点的嵌入来更新边的嵌入。接下来，通过聚合节点关联所有边的嵌入来更新节点嵌入。图嵌入被用于节点和边表示的更新函数中，并且图级别的嵌入本身通过在每次迭代结束时对所有节点和边的嵌入进行聚合来更新。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python程序运行时间查看方法</title>
    <link href="/2023/05/30/python-cheng-xu-yun-xing-shi-jian-cha-kan-fang-fa/"/>
    <url>/2023/05/30/python-cheng-xu-yun-xing-shi-jian-cha-kan-fang-fa/</url>
    
    <content type="html"><![CDATA[<p>time包查看程序运行时间</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-keyword">import</span> time<br><br>start_time = time.time()  <span class="hljs-comment"># 记录程序开始运行时间</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000000</span>):<br>    <span class="hljs-keyword">pass</span><br>end_time = time.time()  <span class="hljs-comment"># 记录程序结束运行时间</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string"><code class="language-hljs python"><span class="hljs-keyword">import</span> time<br><br>start_time = time.time()  <span class="hljs-comment"># 记录程序开始运行时间</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000000</span>):<br>    <span class="hljs-keyword">pass</span><br>end_time = time.time()  <span class="hljs-comment"># 记录程序结束运行时间</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">'cost %f s'</span> % (end_time - start_time))<br></code></pre></td></tr></tbody></table></figure><p>输出</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre class=" language-hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>cost <span class="hljs-number"><code class="language-hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>cost <span class="hljs-number">0.179781</span> s<br></code></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（三）节点嵌入</title>
    <link href="/2023/05/29/tu-shen-jing-wang-luo-xue-xi-ri-ji-san/"/>
    <url>/2023/05/29/tu-shen-jing-wang-luo-xue-xi-ri-ji-san/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（二）</title>
    <link href="/2023/05/28/song-ci-er/"/>
    <url>/2023/05/28/song-ci-er/</url>
    
    <content type="html"><![CDATA[<center>蝶恋花</center><center>宋 柳永</center><center>伫倚危楼风细细，望极春愁，黯黯生天际。草色烟光残照里，无言谁会凭栏意？</center><center>拟把疏狂图一醉，对酒当歌，强乐还无味。衣带渐宽终不悔，为伊消得人憔悴。</center><p><br></p><center>木兰花</center><center>钱惟演</center><center>城上风光莺语乱，城下烟波春拍岸。绿杨芳草几时休，泪眼愁肠先已断。</center><center>情怀渐觉成衰晚，鸾镜朱颜惊暗换。昔年多病厌芳尊，今日芳尊惟恐浅。</center><p><br></p><center>踏莎行</center><center>寇准</center><center>春色将阑，莺声渐老，红英落尽青梅小。画堂人静雨蒙蒙，屏山半掩余香袅。</center><center>密约沉沉，离情杳杳，菱花尘满慵将照。倚楼无语欲销魂，长空暗淡逢芳草。</center><p><br></p><center>曲玉管</center><center>柳永</center><center>陇首云飞，江边日晚，烟波满目凭栏久。一望关河萧索，千里清秋，忍凝眸。</center><center>杳杳神京，盈盈仙子，别来锦字终难偶。断雁无凭，冉冉飞下汀州，思悠悠。</center><center>暗想当初，有多少，幽欢佳会；岂知聚散难期，翻成雨恨云愁。</center><center>阻追游，每登山临水，惹起平生心事，一场消黯，永日无言，却下层楼。</center>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（一）</title>
    <link href="/2023/05/28/song-ci-yi/"/>
    <url>/2023/05/28/song-ci-yi/</url>
    
    <content type="html"><![CDATA[<center>木兰花 春景</center><center>宋祁</center><center>东城渐觉风光好，縠皱波纹迎客掉。绿杨烟外晓云轻，红杏枝头春意闹。</center><center>浮生长恨欢娱少，肯爱千金轻一笑？为君持酒劝斜阳，且向花间留晚照。</center>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（二）传统机器学习方法</title>
    <link href="/2023/05/28/tu-shen-jing-wang-luo-xue-xi-ri-ji-er/"/>
    <url>/2023/05/28/tu-shen-jing-wang-luo-xue-xi-ri-ji-er/</url>
    
    <content type="html"><![CDATA[<p><strong>标准机器学习范式</strong></p><p>首先，基于启发式函数或领域知识提取一些统计特征；然后将其作为标准机器学习分类器（如逻辑回归）的输入。</p><h2 id="图统计特征">图统计特征</h2><h3 id="节点层面的统计特征">节点层面的统计特征</h3><p><strong>节点的度</strong></p><p>节点<span class="math inline">\(u\)</span>的度反映与这个节点相连接的边的数目，可由下式表示<span class="math display">\[d_u = \sum_{v \in \mathcal{V}} A[u,v]\]</span> 对于有向图和加权图，度分为入度和出度。对矩阵<span class="math inline">\(A\)</span>中的节点<span class="math inline">\(u\)</span>的行求和可以得到出度，对节点<span class="math inline">\(u\)</span>的列求和可以得到入度。</p><p><strong>节点的中心性</strong></p><p>角度一：特征向量中心性度量不仅考虑邻居节点个数的度，还考虑了邻居节点的重要性。<span class="math display">\[e_u = \frac{1}{\lambda}\sum_{v\in \mathcal{V}}A[u,v]e_v, \forall u \in\mathcal{V}\]</span> 将向量表示<span class="math inline">\(e\)</span>代入上述等式取代节点中心性向量可得到邻接矩阵的标准特征向量方程：<span class="math display">\[\lambda e = Ae\]</span>角度二：特征向量中心性衡量了一个节点在路径无限长的情况下在随机游走时被访问的概率。这种理解方式连接了节点重要性、随机游走和谱三个重要概念。<span class="math inline">\(\lambda\)</span>是<span class="math inline">\(A\)</span>的主要特征向量，可通过幂次迭代法则计算<span class="math inline">\(e\)</span>如下所示： <span class="math display">\[e^{(t+1)} = Ae^{(t)}\]</span> 从向量<span class="math inline">\(e^{(0)}=(1,1,\cdots,1)^T\)</span>开始，依据幂次迭代法则，第一次迭代可以得到每个节点的度，在第<span class="math inline">\(t\)</span>次迭代<span class="math inline">\((t\geq 1)\)</span>时，<span class="math inline">\(e^{(t)}\)</span>包括了尝试到达每个节点且长度为<span class="math inline">\(t\)</span>的路线的长度的数量，无限重复下去可得到路径无限长时每个节点被访问的次数。</p><p><strong>聚类系数</strong></p><p>聚类系数通过一个节点的局部邻域中闭合三角形的比例度量节点的邻居节点聚类的紧密程度。聚类系数计算方法Local Variant 方法如下式所示： <span class="math display">\[c_u = \frac{|(v_1, v_2)\in \varepsilon: v_1, v_2 \in\mathcal{N}(u)|}{\binom{d_u}{2}}\]</span> 其中<span class="math inline">\(\mathcal{N}(u)=\{v\in\mathcal{V}: (u,v)\in \varepsilon\}\)</span>表示节点<span class="math inline">\(u\)</span>的邻居节点。</p><p><strong>闭合三角形、自我中心图、Motifs</strong></p><p><strong>闭合三角形</strong></p><h3 id="图层面的统计特征">图层面的统计特征</h3><p>节点袋</p><p>Weisfeiler-Lehman 核</p><p>Graphlets 和基于路径的方法</p><h2 id="邻域重叠检测">邻域重叠检测</h2><h2 id="图的拉普拉斯矩阵和图的谱方法">图的拉普拉斯矩阵和图的谱方法</h2>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex 字母类符号</title>
    <link href="/2023/05/24/latex-zi-mu-lei-fu-hao/"/>
    <url>/2023/05/24/latex-zi-mu-lei-fu-hao/</url>
    
    <content type="html"><![CDATA[<p>Caligraphic letters: <code>$\mathcal{A}$</code> etc.: <span class="math inline">\(\mathcal{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Mathbb letters: <code>$\mathbb{A}$</code>etc.: <span class="math inline">\(\mathbb{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Mathfrak letters: <code>$\mathfrak{A}$</code>etc.: <span class="math inline">\(\mathfrak{A B C D E F G H I J K L M N O P Q I S TU V W X Y Z}\)</span></p><p>Math Sans serif letters: <code>$\mathsf{A}$</code>etc.: <span class="math inline">\(\mathsf{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Math bold letters: <code>$\mathbf{A}$</code>etc.: <span class="math inline">\(\mathbf{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Math bold italic letters: define<code>\def\mathbi#1{\textbf{\em #1}}</code> then use<code>$\mathbi{A}$</code></p>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（一）图基础知识</title>
    <link href="/2023/05/24/tu-shen-jing-wang-luo-xue-xi-ri-ji-yi/"/>
    <url>/2023/05/24/tu-shen-jing-wang-luo-xue-xi-ri-ji-yi/</url>
    
    <content type="html"><![CDATA[<h2 id="图的定义">图的定义</h2><p>图由节点集合<span class="math inline">\(\mathcal{V}\)</span>和边集合<span class="math inline">\(\varepsilon\)</span>组成，记作<span class="math inline">\(\mathcal{G}=(\mathcal{V},\varepsilon)\)</span>，节点<span class="math inline">\(u\in \mathcal{V}\)</span>到节点<span class="math inline">\(v\in \mathcal{V}\)</span>的边表示为<span class="math inline">\((u,v)\in\varepsilon\)</span>.</p><h2 id="图的表示">图的表示</h2><p>图可以由邻接矩阵<span class="math inline">\(A\in\mathbb{R}^{|\mathcal{V}|\times|\mathcal{V}|}\)</span>表示，矩阵的行和列代表节点索引，矩阵元素<span class="math inline">\(A[u,v]\)</span>表示节点<span class="math inline">\(u\)</span>和节点<span class="math inline">\(v\)</span>的连接情况，如果<span class="math inline">\((u,v)\in \varepsilon\)</span>，则<span class="math inline">\(A[u,v]=1\)</span>，否则<span class="math inline">\(A[u,v]=0\)</span>。</p><h2 id="多关系图的分类">多关系图的分类</h2><p><strong>异构图</strong></p><p>异构图可以通过节点类型将节点划分为不相交的子集，即<span class="math inline">\(\mathcal{V} = \mathcal{V}_1 \bigcup \mathcal{V}_2\bigcup \cdots \bigcup \mathcal{V}_k\)</span>，其中<span class="math inline">\(\mathcal{V}_i \bigcap \mathcal{V}_j = \emptyset,\forall \neq j\)</span>。</p><p><strong>多重图</strong></p><p>多重图中的边只能连接不同类型的节点，即<span class="math inline">\((u,\tau_i,v)\in \varepsilon \rightarrow u \in\mathcal{V}_j,v \in \mathcal{V}_k \bigwedge j \neqk\)</span>。在多重图中，通常假设图可以被分解为<span class="math inline">\(k\)</span>个层级，每个节点可以属于一层或多层，每层代表唯一特定关系，表示本层内边的类型。</p><h2 id="图机器学习任务">图机器学习任务</h2><h3 id="节点预测">节点预测</h3><p>提供训练集真实的标签<span class="math inline">\(\mathcal{V}_{train}\subset \mathcal{V}\)</span>时，通过所有的节点<span class="math inline">\(u \in \mathcal{V}\)</span>预测标签<span class="math inline">\(y_u\)</span>应该属于哪种类型、类别或属性。</p><p>节点预测可通过显式利用节点之间的连接进行分类，如下几种节点之间的连接性质：</p><p><strong>同质性</strong></p><p>图中的节点与其邻居节点的属性相似，即节点有与邻居节点共享属性的趋势。</p><p><strong>异质性</strong></p><p>假定节点将优先连接到具有不同标签的节点。</p><p><strong>结构等价性</strong></p><p>具有相似局部结构的节点将具有相似的标签。</p><p><strong>节点预测是有监督还是半监督任务？</strong></p><p><strong>非标准的半监督任务。</strong></p><p>在半监督学习中，模型训练过程同时使用有标签数据和无标签数据，标准的半监督学习以独立同分布假设为前提，标准的监督学习在训练过程中不使用所有无标签的测试数据。节点分类任务中，图中节点全部都被使用，包括无标签节点，故节点分类任务是半监督学习，同时节点分类任务对一组相互连接的节点进行建模，打破了独立同分布假设，故节点分类是非标准的半监督任务。</p><h3 id="关系预测">关系预测</h3><p>给定一组节点<span class="math inline">\(\mathcal{V}\)</span>和部分边的集合<span class="math inline">\(\varepsilon_{train}\)</span>（<span class="math inline">\(\varepsilon_{train} \subset \varepsilon,\varepsilon\)</span>表示全体边的集合），利用这些给定信息推断缺失边的集合<span class="math inline">\(\varepsilon \ \varepsilon_{train}\)</span>。</p><h3 id="社区发现">社区发现</h3><p>通过输入一张图<span class="math inline">\(\mathcal{G}=(\mathcal{V},\varepsilon)\)</span>推断出潜在的社区结构。</p><p>社区发现常被类比为图领域的无监督学习中的聚类任务。</p><h3 id="图预测">图预测</h3><p>图预测包括对整张图进行分类、回归与聚类。</p><p>图分类或图回归任务中，数据集由多张不同图构成，图机器学习算法针对每张图进行独立预测，而不是预测图的组成部分。在图聚类任务中，目标是学习一个无监督的测量图与图之间相似性的策略。</p><p>图分类与图回归任务属于标准监督学习范畴。</p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
