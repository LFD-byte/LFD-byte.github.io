<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2023 论文砖 Prototypical Networks for Few shot Learning</title>
    <link href="/2023/12/13/2023-lun-wen-zhuan-prototypical-networks-for-few-shot-learning/"/>
    <url>/2023/12/13/2023-lun-wen-zhuan-prototypical-networks-for-few-shot-learning/</url>
    
    <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><p>我们针对小样本分类问题提出了原型网络（PrototypicalNetworks），其中分类器必须泛化到训练集中未见过的新类，每个新类的仅有少量样本。原型网络学习一个度量空间，在该空间中可以通过计算到每个类的原型表示的距离来执行分类。与最近的小样本学习方法相比，它们反映了更简单的归纳偏差，这在这种有限数据的情况下是有益的，并取得了优异的结果。我们提供的分析表明，一些简单的设计决策可以比涉及复杂架构选择和元学习的最新方法产生重大改进。我们进一步将原型网络扩展到零样本学习，并在CU-Birds 数据集上取得了最先进的结果。</p><h2 id="介绍">介绍</h2><p>小样本分类是一项任务，其中分类器必须适应训练中未见过的新类别，每个类别仅给出几个样本。简单的方法，例如根据新数据重新训练模型，会严重过度拟合。虽然这个问题相当困难，但事实证明，人类甚至有能力执行一次性分类，即仅给出每个新类别的一个样本，并且具有很高的准确度。</p><p>最近的两种方法在小样本学习方面取得了重大进展。Vinyals 等人提出了Matching Networks，它在标记样本集（supportset）的学习嵌入上使用注意机制来预测未标记点（query set）的类别。MatchingNetworks可以解释为在嵌入空间中应用的加权最近邻分类器。值得注意的是，该模型在训练期间使用称为“episode” 的采样小批量，其中每个 episode都旨在通过对类和数据点进行二次采样来模拟小样本任务。episode的使用使训练问题更加忠实于测试环境，从而提高泛化能力。 Ravi 和Larochelle 进一步采用 episode训练思想，提出了一种用于小样本学习的元学习方法。他们的方法涉及训练LSTM，以在给定的情况下生成分类器的更新，以便它能够很好地推广到测试集。在这里，LSTM元学习器不是在多个 episode 上训练单个模型，而是学习为每个 episode训练一个自定义模型。</p><p>我们通过解决过度拟合的关键问题来解决小样本学习的问题。由于数据严重有限，我们假设分类器应该具有非常简单的归纳偏差。我们的方法“原型网络”基于这样的想法：存在一种嵌入，其中点聚集在每个类的单个原型表示周围。为了做到这一点，我们使用神经网络学习输入到嵌入空间的非线性映射，并将类的原型作为其在嵌入空间中支持集的平均值。然后通过简单地查找最近的类原型来对嵌入查询点执行分类。我们遵循相同的方法来解决零样本学习，这里，每个类都带有元数据（meta-data），提供类的高级描述，而不是少量标记的示例。因此，我们学习将元数据嵌入到共享空间中，作为每个类的原型。与在少数场景中一样，通过查找嵌入查询点的最近的类原型来执行分类。</p><p>在本文中，我们为小样本和零样本设置制定了原型网络。我们在 one-shot设置中建立与 Matching Networks的连接，并分析模型中使用的基础距离函数。特别是，我们将原型网络与聚类联系起来，以便在使用Bregman散度（例如平方欧几里得距离）计算距离时证明使用类均值作为原型的合理性。我们根据经验发现距离的选择至关重要，因为欧几里得距离远远优于更常用的余弦相似度。在多项基准测试任务中，我们实现了最先进的性能。原型网络比最近的元学习算法更简单、更高效，这使得它们成为小样本和零样本学习的一种有吸引力的方法。</p><h2 id="原型网络">原型网络</h2><h3 id="notion">Notion</h3><p>在小样本分类中，我们得到了一个由 N 个标记样本组成的小 support set<span class="math inline">\(S={(x_i,y_i), \dots,(x_N,y_N)}\)</span>，其中每个 <span class="math inline">\(x_i \in\mathbb{R}^D\)</span> 是一个样本的 D 维向量特征，<spanclass="math inline">\(y_i \in {1,\dots,K}\)</span> 是相应的标签。<spanclass="math inline">\(S_k\)</span> 表示被标记为类 k 的样本集合。</p><h3 id="模型">模型</h3><p>原型网络通过一个可学习参数 $$ 计算一个 M 维表示 <spanclass="math inline">\(c_k \in \mathbb{R}^M\)</span>，</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Few-shot Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 AI CCF-A Differential Privacy 一窥（持续更新中）</title>
    <link href="/2023/12/07/2023-ai-ccf-a-differential-privacy-yi-kui-chi-xu-geng-xin-zhong/"/>
    <url>/2023/12/07/2023-ai-ccf-a-differential-privacy-yi-kui-chi-xu-geng-xin-zhong/</url>
    
    <content type="html"><![CDATA[<p>2023年度的 AI 顶会和顶刊已经出炉，在此把关于 Differential Privacy的部分筛选出来，以供后续学习，整理内容已同步在 <ahref="https://github.com/LFD-byte/awesome-differential-privacy-papers/blob/main/2023_CCF-A_top_differential-privacy.md">Github</a>。</p><h2 id="人工智能">人工智能</h2><h3 id="aaai">AAAI</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26452">DifferentiallyPrivate Nonlinear Causal Discovery from Numerical Data</a></td><td>Hao Zhang</td><td><a href="https://github.com/Causality-Inference/PCD">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26770">DPAUC:Differentially Private AUC Computation in Federated Learning</a></td><td>Jiankai Sun</td><td><ahref="https://github.com/bytedance/fedlearner/tree/master/example/privacy/DPAUC">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26401">XRand:Differentially Private Defense against Explanation-GuidedAttacks</a></td><td>Truc Nguyen</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26242">DifferentiallyPrivate Learning with Per-Sample Adaptive Clipping</a></td><td>Tianyu Xia</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25933">DifferentiallyPrivate Heatmaps</a></td><td>Badih Ghazi</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25895">IntegerSubspace Differential Privacy</a></td><td>Prathamesh Dharangutte</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2303.00228">Two Views of ConstrainedDifferential Privacy: Belief Revision and Update</a></td><td>Likang Liu</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25721">DifferentiallyPrivate Fair Division</a></td><td>Pasin Manurangsi</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25714">DifferentiallyPrivate Condorcet Voting</a></td><td>Zhechen Li</td><td>none</td></tr></tbody></table><h3 id="neurlps">NeurlPS</h3><p>官方暂未发布。</p><h3 id="acl">ACL</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2210.14348">Synthetic Text Generationwith Differential Privacy: A Simple and Practical Recipe</a></td><td>Xiang Yue</td><td><a href="https://github.com/microsoft/dp-transformers">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.355/">ACustomized Text Sanitization Mechanism with DifferentialPrivacy</a></td><td>Sai Chen</td><td><a href="https://github.com/sai4july/CusText">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.07636">DP-BART for PrivatizedText Rewriting under Local Differential Privacy</a></td><td>Timour Igamberdiev</td><td><ahref="https://github.com/trusthlt/dp-bart-private-rewriting">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.18465">Federated Learning ofGboard Language Models with Differential Privacy</a></td><td>Zheng Xu</td><td>none</td></tr></tbody></table><h3 id="cvpr">CVPR</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Learning_To_Generate_Image_Embeddings_With_User-Level_Differential_Privacy_CVPR_2023_paper.html">Learningto Generate Image Embeddings with User-Level DifferentialPrivacy</a></td><td>Zheng Xu</td><td><ahref="https://github.com/google-research/federated/tree/master/dp_visual_embeddings">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Chen_Federated_Learning_in_Non-IID_Settings_Aided_by_Differentially_Private_Synthetic_CVPRW_2023_paper.html">FederatedLearning in Non-IID Settings Aided by Differentially Private SyntheticData</a></td><td>Huancheng Chen</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Shi_Make_Landscape_Flatter_in_Differentially_Private_Federated_Learning_CVPR_2023_paper.html">MakeLandscape Flatter in Differentially Private Federated Learning</a></td><td>Yifan Shi</td><td>none</td></tr></tbody></table><h3 id="icml">ICML</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/alghamdi23a.html">TheSaddle-Point Method in Differential Privacy</a></td><td>Wael Alghamdi</td><td><ahref="https://github.com/Felipe-Gomez/saddlepoint_accountant">url</a></td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/alparslan23a.html">DifferentiallyPrivate Distributed Bayesian Linear Regression with MCMC</a></td><td>Baris Alparslan</td><td><ahref="https://github.com/sinanyildirim/Bayesian_DP_dist_LR">url</a></td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/bu23a.html">DifferentiallyPrivate Optimization on Large Model at Small Cost</a></td><td>Zhiqi Bu</td><td><ahref="https://github.com/awslabs/fast-differential-privacy">url</a></td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/chaturvedi23a.html">StreamingSubmodular Maximization with Differential Privacy</a></td><td>Anamay Chaturvedi</td><td><ahref="https://github.com/thydnguyen/PrivSubmodularOpt">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.00037">Differentially PrivateHierarchical Clustering with Provable Approximation Guarantees</a></td><td>Jacob Imola</td><td><ahref="https://bitbucket.org/jjimola/dphc/src/master/">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/kazan23a.html">The Testof Tests: A Framework for Differentially Private HypothesisTesting</a></td><td>Zeki Kazan</td><td><a href="https://github.com/diff-priv-ht/test-of-tests">url</a></td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/krichene23a.html">Multi-TaskDifferential Privacy Under Distribution Skew</a></td><td>Walid Krichene</td><td><ahref="https://github.com/google-research/google-research/tree/master/">url</a></td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/mangold23a.html">DifferentialPrivacy has Bounded Impact on Fairness in Classification</a></td><td>Paul Mangold</td><td><a href="https://github.com/pmangold/fairness-privacy">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.05651">Differentially PrivateSharpness-Aware Training</a></td><td>Jinseong Park</td><td><a href="https://github.com/jinseongP/DPSAT">url</a></td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/rust23a.html">DifferentialPrivacy, Linguistic Fairness, and Training Data Influence: Impossibilityand Possibility Theorems for Multilingual Language Models</a></td><td>Phillip Rust</td><td><ahref="https://github.com/xplip/multilingual-lm-objectives">url</a></td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/zanella-beguelin23a.html">BayesianEstimation of Differential Privacy</a></td><td>Santiago Zanella-Beguelin</td><td><ahref="https://github.com/microsoft/responsible-ai-toolbox-privacy">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.01121">Differentially PrivateEpisodic Reinforcement Learning with Heavy-tailed Rewards</a></td><td>Yulian Wu</td><td>none</td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/whitehouse23a.html">Fully-AdaptiveComposition in Differential Privacy</a></td><td>Justin Whitehouse</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2301.12535">Concurrent ShuffleDifferential Privacy Under Continual Observation</a></td><td>Jay Tenenbaum</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/song23i.html">SketchingMeets Differential Privacy: Fast Algorithm for Dynamic KroneckerProjection Maintenance</a></td><td>Zhao Song</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.03884">DIFF2: DifferentialPrivate Optimization via Gradient Differences for Nonconvex DistributedLearning</a></td><td>Tomoya Murata</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/liu23q.html">OnlineLocal Differential Private Quantile Inference viaSelf-normalization</a></td><td>Yi Liu</td><td>none</td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/jain23b.html">The Priceof Differential Privacy under Continual Observation</a></td><td>Palak Jain</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/huang23q.html">FederatedLinear Contextual Bandits with User-level Differential Privacy</a></td><td>Ruiquan Huang</td><td>none</td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/fichtenberger23a.html">ConstantMatters: Fine-grained Error Bound on Differentially Private ContinualObservation</a></td><td>Hendrik Fichtenberger</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/das23c.html">BeyondUniform Lipschitz Condition in Differentially PrivateOptimization</a></td><td>Rudrajit Das</td><td>none</td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/chen23d.html">DifferentiallyPrivate Stochastic Convex Optimization under a Quantile LossFunction</a></td><td>Du Chen</td><td>none</td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/busa-fekete23a.html">Labeldifferential privacy and private training data release</a></td><td>Robert Istvan Busa-Fekete</td><td>none</td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/arora23a.html">FasterRates of Convergence to Stationary Points in Differentially PrivateOptimization</a></td><td>Raman Arora</td><td>none</td></tr></tbody></table><h3 id="ijcai">IJCAI</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://www.ijcai.org/proceedings/2023/656">Fast andDifferentially Private Fair Clustering</a></td><td>Junyoung Byun</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2207.10240">Differentially PrivatePartial Set Cover with Applications to Facility Location</a></td><td>George Z. Li</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2308.09902">DPMAC: DifferentiallyPrivate Communication for Cooperative Multi-Agent ReinforcementLearning</a></td><td>Canzhe Zhao</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2304.12528">Model Conversion viaDifferentially Private Data-Free Distillation</a></td><td>Bochao Liu</td><td>none</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Differential Privacy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 AI CCF-A Federated Learning 一窥（持续更新中）</title>
    <link href="/2023/12/06/2023-ai-ccf-a-federated-learning-yi-kui-chi-xu-geng-xin-zhong/"/>
    <url>/2023/12/06/2023-ai-ccf-a-federated-learning-yi-kui-chi-xu-geng-xin-zhong/</url>
    
    <content type="html"><![CDATA[<p>2023年度的 AI 顶会和顶刊已经出炉，在此把关于 Federated Learning的部分筛选出来，以供后续学习，整理内容已同步在 <ahref="https://github.com/LFD-byte/awesome-federated-learning-papers/blob/main/2023_CCF-A_top_federated-learning.md">Github</a>。</p><h2 id="人工智能">人工智能</h2><h3 id="会议">会议</h3><h4 id="aaai">AAAI</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25611">UntargetedAttack against Federated Recommendation Systems via Poisonous ItemEmbeddings and the Defense</a></td><td>Yang Yu</td><td><a href="https://github.com/yflyl613/FedRec">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25955">FederatedRobustness Propagation: Sharing Adversarial Robustness in HeterogeneousFederated Learning</a></td><td>Junyuan Hong</td><td><a href="https://github.com/illidanlab/FedRBN">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26083">Poisoningwith Cerberus: Stealthy and Colluded Backdoor Attack against FederatedLearning</a></td><td>Xiaoting Lyu</td><td><a href="https://github.com/xtlyu/CerP">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26122">FedMDFG:Federated Learning with Multi-Gradient Descent and FairGuidance</a></td><td>Zibin Pan</td><td><a href="https://github.com/zibinpan/FedMDFG">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26187">FederatedLearning on Non-IID Graphs via Structural Knowledge Sharing</a></td><td>Yue Tan</td><td><a href="https://github.com/yuetan031/FedStar">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26223">FedGS:Federated Graph-Based Sampling with Arbitrary ClientAvailability</a></td><td>Zheng Wang</td><td><a href="https://github.com/WwZzz/FedGS">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26237">FedNP:Towards Non-IID Federated Learning via Federated NeuralPropagation</a></td><td>Xueyang Wu</td><td><a href="https://github.com/CodePothunter/fednp">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26245">BayesianFederated Neural Matching That Completes Full Information</a></td><td>Peng Xiao</td><td><a href="https://github.com/XiaoPeng24/NAFI">url;</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26246">CDMA: APractical Cross-Device Federated Learning Algorithm for General MinimaxProblems</a></td><td>Jiahao Xie</td><td><ahref="https://github.com/xjiajiahao/federated-minimax">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26330">FedALA:Adaptive Local Aggregation for Personalized Federated Learning</a></td><td>Jianqing Zhang</td><td><a href="https://github.com/TsingZ0/FedALA">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26770">DPAUC:Differentially Private AUC Computation in Federated Learning</a></td><td>Jiankai Sun</td><td><ahref="https://github.com/bytedance/fedlearner/tree/master/example/privacy/DPAUC">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27049">ClusteredFederated Learning for Heterogeneous Data (Student Abstract)</a></td><td>Xue Yu</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26995">MGIA:Mutual Gradient Inversion Attack in Multi-Modal Federated Learning(Student Abstract)</a></td><td>Xuan Liu</td><td>none</td></tr><tr class="even"><td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26984">AFederated Learning Monitoring Tool for Self-Driving Car Simulation(Student Abstract)</a></td><td>Taejoon Lee</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26847">Industry-ScaleOrchestrated Federated Learning for Drug Discovery</a></td><td>Martijn Oldenhof</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.11485">Efficient Training ofLarge-Scale Industrial Fault Diagnostic Models through FederatedOpportunistic Block Dropout</a></td><td>Yuanyuan Chen</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2304.05516">Echo of Neighbors:Privacy Amplification for Personalized Private Federated Learning withShuffle Model</a></td><td>Yixuan Liu</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2301.08170">On the Vulnerability ofBackdoor Defenses for Federated Learning</a></td><td>Pei Fang</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.09479">Delving into theAdversarial Robustness of Federated Learning</a></td><td>Jie Zhang</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26271">DeFL:Defending against Model Poisoning Attacks in Federated Learning viaCritical Learning Periods Awareness</a></td><td>Gang Yan</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26252">FederatedGenerative Model on Multi-Source Heterogeneous Data in IoT</a></td><td>Zuobin Xiong</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26235">FasterAdaptive Federated Learning</a></td><td>Xidong Wu</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26212">BeyondADMM: A Unified Client-Variance-Reduced Adaptive Federated LearningFramework</a></td><td>Shuai Wang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.07450">FedABC: Targeting FairCompetition in Personalized Federated Learning</a></td><td>Dui Wang</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26197">EfficientDistribution Similarity Identification in Clustered Federated Learningvia Principal Angles between Client Data Subspaces</a></td><td>Saeed Vahidian</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26177">SecuringSecure Aggregation: Mitigating Multi-Round Privacy Leakage in FederatedLearning</a></td><td>Jinhyun So</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26023">Layer-WiseAdaptive Model Aggregation for Scalable Federated Learning</a></td><td>Sunwoo Lee</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2208.09215">Almost Cost-FreeCommunication in Federated Best Arm Identification</a></td><td>Kota Srinivas Reddy</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2303.06237">ComplementSparsification: Low-Overhead Model Pruning for FederatedLearning</a></td><td>Xiaopeng Jiang</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25911">FairFed:Enabling Group Fairness in Federated Learning</a></td><td>Yahya H. Ezzeldin</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25891">TacklingData Heterogeneity in Federated Learning with Class Prototypes</a></td><td>Yutong Dai</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25744">Incentive-BoostedFederated Crowdsourcing</a></td><td>Xiangping Kang</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25531">Win-Win: APrivacy-Preserving Federated Framework for Dual-Target Cross-DomainRecommendation</a></td><td>Gaode Chen</td><td>none</td></tr></tbody></table><h4 id="neurlps">NeurlPS</h4><p>官方暂未发布。</p><h4 id="acl">ACL</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.193/">FEDLEGAL: TheFirst Real-World Federated Learning Benchmark for Legal NLP</a></td><td>Zhuo Zhang</td><td><a href="https://github.com/SMILELab-FL/FedLegal">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17221">Federated Learning forSemantic Parsing: Task Formulation, Evaluation Setup, NewAlgorithms</a></td><td>Tianshu Zhang</td><td><ahref="https://github.com/OSU-NLP-Group/FL4SemanticParsing">url</a></td></tr><tr class="odd"><td><ahref="https://aclanthology.org/2023.findings-acl.75/">Client-CustomizedAdaptation for Parameter-Efficient Federated Learning</a></td><td>Yeachan Kim</td><td><a href="https://github.com/yeachan-kr/c2a">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.12449">Communication EfficientFederated Learning for Multilingual Neural Machine Translation withAdapter</a></td><td>Yi Liu</td><td><a href="https://github.com/lancopku/FedMNMT">url</a></td></tr><tr class="odd"><td><ahref="https://aclanthology.org/2023.findings-acl.632/">FedPETuning: WhenFederated Learning Meets the Parameter-Efficient Tuning Methods ofPre-trained Language Models</a></td><td>Zhuo Zhang</td><td><a href="https://github.com/SMILELab-FL/FedPETuning">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.18465">Federated Learning ofGboard Language Models with Differential Privacy</a></td><td>Zheng Xu</td><td>none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.470/">FederatedDomain Adaptation for Named Entity Recognition via Distilling withHeterogeneous Tag Sets</a></td><td>Rui Wang</td><td>none</td></tr></tbody></table><h4 id="cvpr">CVPR</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Luo_GradMA_A_Gradient-Memory-Based_Accelerated_Federated_Learning_With_Alleviated_Catastrophic_Forgetting_CVPR_2023_paper.html">GradMA:A Gradient-Memory-based Accelerated Federated Learning with AlleviatedCatastrophic Forgetting</a></td><td>Kangyang Luo</td><td><a href="https://github.com/lkyddd/GradMA">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.html">FederatedIncremental Semantic Segmentation</a></td><td>Jiahua Dong</td><td><a href="https://github.com/JiahuaDong/FISS">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Re-Thinking_Federated_Active_Learning_Based_on_Inter-Class_Diversity_CVPR_2023_paper.html">Re-ThinkingFederated Active Learning Based on Inter-Class Diversity</a></td><td>SangMook Kim</td><td><a href="https://github.com/raymin0223/LoGo">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.html">FederatedDomain Generalization with Generalization Adjustment</a></td><td>Ruipeng Zhang</td><td><a href="https://github.com/MediaBrain-SJTU/FedDG-GA">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Li_On_the_Effectiveness_of_Partial_Variance_Reduction_in_Federated_Learning_CVPR_2023_paper.html">Onthe Effectiveness of Partial Variance Reduction in Federated Learningwith Heterogeneous Data</a></td><td>Bo Li</td><td><a href="https://github.com/lyn1874/fedpvr">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Learning_Federated_Visual_Prompt_in_Null_Space_for_MRI_Reconstruction_CVPR_2023_paper.html">LearningFederated Visual Prompt in Null Space for MRI Reconstruction</a></td><td>Chun-Mei Feng</td><td><a href="https://github.com/chunmeifeng/FedPR">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Duan_Federated_Learning_With_Data-Agnostic_Distribution_Fusion_CVPR_2023_paper.html">FederatedLearning with Data-Agnostic Distribution Fusion</a></td><td>Jian-hui Duan</td><td><a href="https://github.com/LiruichenSpace/FedFusion">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Class_Balanced_Adaptive_Pseudo_Labeling_for_Federated_Semi-Supervised_Learning_CVPR_2023_paper.html">ClassBalanced Adaptive Pseudo Labeling for Federated Semi-SupervisedLearning</a></td><td>Ming Li</td><td><a href="https://github.com/minglllli/CBAFed">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_Fair_Federated_Medical_Image_Segmentation_via_Client_Contribution_Estimation_CVPR_2023_paper.html">FairFederated Medical Image Segmentation via Client ContributionEstimation</a></td><td>Meirui Jiang</td><td><ahref="https://github.com/NVIDIA/NVFlare/tree/main/research/fed-ce">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/10203389">RethinkingFederated Learning with Domain Shift: A Prototype View</a></td><td>Wenke Huang</td><td><a href="https://github.com/WenkeHuang/RethinkFL">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Chow_STDLens_Model_Hijacking-Resilient_Federated_Learning_for_Object_Detection_CVPR_2023_paper.html">STDLens:Model Hijacking-Resilient Federated Learning for ObjectDetection</a></td><td>Ka-Ho Chow</td><td><a href="https://github.com/git-disl/STDLens">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Ilhan_ScaleFL_Resource-Adaptive_Federated_Learning_With_Heterogeneous_Clients_CVPR_2023_paper.html">ScaleFL:Resource-Adaptive Federated Learning with Heterogeneous Clients</a></td><td>Fatih Ilhan</td><td><a href="https://github.com/git-disl/scale-fl">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Chen_OpenFed_A_Comprehensive_and_Versatile_Open-Source_Federated_Learning_Framework_CVPRW_2023_paper.html">OpenFed:A Comprehensive and Versatile Open-Source Federated LearningFramework</a></td><td>Dengsheng Chen</td><td><a href="https://github.com/FederalLab/OpenFed">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Cai_Many-Task_Federated_Learning_A_New_Problem_Setting_and_a_Simple_CVPRW_2023_paper.html">Many-TaskFederated Learning: A New Problem Setting and A Simple Baseline</a></td><td>Ruisi Cai</td><td><a href="https://github.com/VITA-Group/MaT-FL">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Ovi_Mixed_Quantization_Enabled_Federated_Learning_To_Tackle_Gradient_Inversion_Attacks_CVPRW_2023_paper.html">MixedQuantization Enabled Federated Learning to Tackle Gradient InversionAttacks</a></td><td>Pretom Roy Ovi</td><td><ahref="https://github.com/PretomRoy/Defense-against-grad-inversion-attacks">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Shenaj_Asynchronous_Federated_Continual_Learning_CVPRW_2023_paper.html">AsynchronousFederated Continual Learning</a></td><td>Donald Shenaj</td><td><a href="https://github.com/LTTM/FedSpace">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Yuan_Peer-to-Peer_Federated_Continual_Learning_for_Naturalistic_Driving_Action_Recognition_CVPRW_2023_paper.html">Peer-to-PeerFederated Continual Learning for Naturalistic Driving ActionRecognition</a></td><td>Liangqi Yuan</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Zhang_TimelyFL_Heterogeneity-Aware_Asynchronous_Federated_Learning_With_Adaptive_Partial_Training_CVPRW_2023_paper.html">TimelyFL:Heterogeneity-aware Asynchronous Federated Learning with AdaptivePartial Training</a></td><td>Tuo Zhang</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/FedVision/html/Chen_Federated_Learning_in_Non-IID_Settings_Aided_by_Differentially_Private_Synthetic_CVPRW_2023_paper.html">FederatedLearning in Non-IID Settings Aided by Differentially Private SyntheticData</a></td><td>Huancheng Chen</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Shi_Make_Landscape_Flatter_in_Differentially_Private_Federated_Learning_CVPR_2023_paper.html">MakeLandscape Flatter in Differentially Private Federated Learning</a></td><td>Yifan Shi</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Confidence-Aware_Personalized_Federated_Learning_via_Variational_Expectation_Maximization_CVPR_2023_paper.html">Confidence-AwarePersonalized Federated Learning via Variational ExpectationMaximization</a></td><td>Junyi Zhu</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Bias-Eliminating_Augmentation_Learning_for_Debiased_Federated_Learning_CVPR_2023_paper.html">Bias-EliminatingAugmentation Learning for Debiased Federated Learning</a></td><td>Yuan-Yi Xu</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liao_Adaptive_Channel_Sparsity_for_Federated_Learning_Under_System_Heterogeneity_CVPR_2023_paper.html">AdaptiveChannel Sparsity for Federated Learning under SystemHeterogeneity</a></td><td>Dongping Liao</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_Reliable_and_Interpretable_Personalized_Federated_Learning_CVPR_2023_paper.html">Reliableand Interpretable Personalized Federated Learning</a></td><td>Zixuan Qin</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_DaFKD_Domain-Aware_Federated_Knowledge_Distillation_CVPR_2023_paper.html">DaFKD:Domain-aware Federated Knowledge Distillation</a></td><td>Haozhao Wang</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xiong_FedDM_Iterative_Distribution_Matching_for_Communication-Efficient_Federated_Learning_CVPR_2023_paper.html">FedDM:Iterative Distribution Matching for Communication-Efficient FederatedLearning</a></td><td>Yuanhao Xiong</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Takahashi_Breaching_FedMD_Image_Recovery_via_Paired-Logits_Inversion_Attack_CVPR_2023_paper.html">BreachingFedMD: Image Recovery via Paired-Logits Inversion Attack</a></td><td>Hideaki Takahashi</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Elastic_Aggregation_for_Federated_Optimization_CVPR_2023_paper.html">ElasticAggregation for Federated Optimization</a></td><td>Dengsheng Chen</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Qu_How_To_Prevent_the_Poor_Performance_Clients_for_Personalized_Federated_CVPR_2023_paper.html">Howto Prevent the Poor Performance Clients for Personalized FederatedLearning?</a></td><td>Zhe Qu</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Miao_FedSeg_Class-Heterogeneous_Federated_Learning_for_Semantic_Segmentation_CVPR_2023_paper.html">FedSeg:Class-Heterogeneous Federated Learning for SemanticSegmentation</a></td><td>Jiaxu Miao</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_The_Resource_Problem_of_Using_Linear_Layer_Leakage_Attack_in_CVPR_2023_paper.html">TheResource Problem of Using Linear Layer Leakage Attack in FederatedLearning</a></td><td>Joshua C. Zhao</td><td>none</td></tr></tbody></table><h4 id="icml">ICML</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/baek23a.html">PersonalizedSubgraph Federated Learning</a></td><td>Jinheon Baek</td><td><a href="https://github.com/JinheonBaek/FED-PUB">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.06508">Optimizing theCollaboration Structure in Cross-Silo Federated Learning</a></td><td>Wenxuan Bao</td><td><a href="https://github.com/baowenxuan/FedCollab">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.02776">Efficient PersonalizedFederated Learning via Sparse Model-Adaptation</a></td><td>Daoyuan Chen</td><td><ahref="https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.12559">From Noisy Fixed-PointIterations to Private ADMM for Centralized and FederatedLearning</a></td><td>Edwige Cyffers</td><td><a href="https://github.com/totilas/padadmm">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2304.12961">Chameleon: Adapting toPeer Images for Planting Durable Backdoors in FederatedLearning</a></td><td>Yanbo Dai</td><td><ahref="https://github.com/ybdai7/Chameleon-durable-backdoor">url</a></td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/gascon23a.html">Federated HeavyHitter Recovery under Linear Sketching</a></td><td>Adria Gascon</td><td><a href="https://github.com/google-research/federated">url</a></td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/guo23b.html">Out-of-DistributionGeneralization of Federated Learning via Implicit InvariantRelationships</a></td><td>Yaming Guo</td><td><a href="https://github.com/YamingGuo98/FedIIR">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/guo23c.html">FeDXL:Provable Federated Learning for Deep X-Risk Optimization</a></td><td>Zhishuai Guo</td><td><ahref="https://github.com/Optimization-AI/ICML2023_FeDXL">url</a></td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/guo23g.html">FedBR:Improving Federated Learning on Heterogeneous Data via Local LearningBias Reduction</a></td><td>Yongxin Guo</td><td><a href="https://github.com/lins-lab/fedbr">url</a></td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/humbert23a.html">One-ShotFederated Conformal Prediction</a></td><td>Pierre Humbert</td><td><a href="https://github.com/pierreHmbt/FedCP-QQ">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.10911">Revisiting WeightedAggregation in Federated Learning with Neural Networks</a></td><td>Zexi Li</td><td><a href="https://github.com/ZexiLee/ICML-2023-FedLAW">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/lu23i.html">FederatedConformal Predictors for Distributed Uncertainty Quantification</a></td><td>Charles Lu</td><td><a href="https://github.com/clu5/federated-conformal">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2303.05786">Vertical Federated GraphNeural Network for Recommender System</a></td><td>Peihua Mai</td><td><a href="https://github.com/maiph123/VerticalGNN">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.07644">SRATTA: SampleRe-ATTribution Attack of Secure Aggregation in FederatedLearning</a></td><td>Tanguy Marchand</td><td><a href="https://github.com/owkin/SRATTA">url</a></td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/pang23a.html">SecureFederated Correlation Test and Entropy Estimation</a></td><td>Qi Pang</td><td><ahref="https://github.com/Qi-Pang/Federated-Correlation-Test">url</a></td></tr><tr class="even"><td><a href="https://openreview.net/forum?id=mRiDy4qGwB">TabLeak:Tabular Data Leakage in Federated Learning</a></td><td>Mark Vero</td><td><a href="https://github.com/eth-sri/tableak">url</a></td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/wang23n.html">FedHPO-Bench: ABenchmark Suite for Federated Hyperparameter Optimization</a></td><td>Zhen Wang</td><td><ahref="https://github.com/alibaba/FederatedScope/tree/master/benchmark/FedHPOBench">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/wu23e.html">AnchorSampling for Federated Learning with Partial ClientParticipation</a></td><td>Feijie Wu</td><td><a href="https://github.com/HarliWu/FedAMD">url</a></td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/ye23b.html">PersonalizedFederated Learning with Inferred Collaboration Graphs</a></td><td>Rui Ye</td><td><a href="https://github.com/MediaBrain-SJTU/pFedGraph">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.19229">FedDisco: FederatedLearning with Discrepancy-Aware Collaboration</a></td><td>Rui Ye</td><td><a href="https://github.com/MediaBrain-SJTU/FedDisco">url</a></td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/yi23a.html">DoublyAdversarial Federated Bandits</a></td><td>Jialin Yi</td><td><ahref="https://github.com/jialinyi94/doubly-stochastic-federataed-bandit">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/zhang23w.html">FedCR:Personalized Federated Learning Based on Across-Client CommonRepresentation with Conditional Mutual InformationRegularization</a></td><td>Hao Zhang</td><td><a href="https://github.com/haozzh/FedCR">url</a></td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/zhang23aa.html">No OneIdles: Efficient Heterogeneous Federated Learning with Parallel Edge andServer Computation</a></td><td>Feilong Zhang</td><td><a href="https://github.com/Hypervoyager/PFL">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/zhu23j.html">LeadFL:Client Self-Defense against Model Poisoning in FederatedLearning</a></td><td>Chaoyi Zhu</td><td><a href="https://github.com/chaoyitud/LeadFL">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.00127">Surrogate Model Extension(SME): A Fast and Accurate Weight Update Attack on FederatedLearning</a></td><td>Junyi Zhu</td><td><ahref="https://github.com/JunyiZhu-AI/surrogate_model_extension">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.00771">Towards Unbiased Trainingin Federated Open-world Semi-supervised Learning</a></td><td>Jie Zhang</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/zhang23y.html">Fed-CBS:A Heterogeneity-Aware Client Sampling Mechanism for Federated Learningvia Class-Imbalance Reduction</a></td><td>Jianyi Zhang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.04969">Communication-EfficientFederated Hypergradient Computation via Aggregated IterativeDifferentiation</a></td><td>Peiyao Xiao</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.01068">Personalized FederatedLearning under Mixture of Distributions</a></td><td>Yue Wu</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.10697">The Blessing ofHeterogeneity in Federated Q-Learning: Linear Speedup andBeyond</a></td><td>Jiin Woo</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/ullah23b.html">PrivateFederated Learning with Autotuned Compression</a></td><td>Enayat Ullah</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.11584">Dynamic RegularizedSharpness Aware Minimization in Federated Learning: Approaching GlobalConsistency and Smooth Landscape</a></td><td>Yan Sun</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/song23e.html">FedAvgConverges to Zero Training Loss Linearly for OverparameterizedMulti-Layer Neural Networks</a></td><td>Bingqing Song</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.04083">Improving the ModelConsistency of Decentralized Federated Learning</a></td><td>Yifan Shi</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.05131">Conformal Prediction forFederated Uncertainty Quantification Under Label Shift</a></td><td>Vincent Plassier</td><td>none</td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/patel23a.html">FederatedOnline and Bandit Convex Optimization</a></td><td>Kumar Kshitij Patel</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/park23e.html">TowardsUnderstanding Ensemble Distillation in Federated Learning</a></td><td>Sejun Park</td><td>none</td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/panchal23a.html">Flash:Concept Drift Adaptation in Federated Learning</a></td><td>Kunjal Panchal</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2304.13407">FedVS:Straggler-Resilient and Privacy-Preserving Vertical Federated Learningfor Split Models</a></td><td>Songze Li</td><td>none</td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/li23z.html">FederatedAdversarial Learning: A Framework with Convergence Analysis</a></td><td>Xiaoxiao Li</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/li23o.html">Analysis ofError Feedback in Federated Non-Convex Optimization with BiasedCompression: Fast Convergence and Partial Participation</a></td><td>Xiaoyun Li</td><td>none</td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/kariyappa23a.html">CocktailParty Attack: Breaking Aggregation-Based Privacy in Federated LearningUsing Independent Component Analysis</a></td><td>Sanjay Kariyappa</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/huang23q.html">FederatedLinear Contextual Bandits with User-level Differential Privacy</a></td><td>Ruiquan Huang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.05412">Achieving Linear Speedupin Non-IID Federated Bilevel Learning</a></td><td>Minhui Huang</td><td>none</td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/guo23a.html">Privacy-AwareCompression for Federated Learning Through Numerical MechanismDesign</a></td><td>Chuan Guo</td><td>none</td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/dorfman23a.html">DoCoFL:Downlink Compression for Cross-Device Federated Learning</a></td><td>Ron Dorfman</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.03109">On the Convergence ofFederated Averaging with Cyclic Client Participation</a></td><td>Yae Jee Cho</td><td>none</td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/chen23j.html">GuardHFL:Privacy Guardian for Heterogeneous Federated Learning</a></td><td>Hanxiao Chen</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/che23b.html">FastFederated Machine Unlearning with Nonlinear Functional Theory</a></td><td>Tianshi Che</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.02219">LESS-VFL:Communication-Efficient Feature Selection for Vertical FederatedLearning</a></td><td>Timothy Castiglia</td><td>none</td></tr></tbody></table><h4 id="ijcai">IJCAI</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2301.09152">Prompt Federated Learningfor Weather Forecasting: Toward Foundation Models on MeteorologicalData</a></td><td>Shengchao Chen</td><td><a href="https://github.com/shengchaochen82/MetePFL">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2208.05174">FedOBD: OpportunisticBlock Dropout for Efficiently Training Large-scale Neural Networksthrough Federated Learning</a></td><td>Yuanyuan Chen</td><td><ahref="https://github.com/cyyever/distributed_learning_simulator">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.09729">FedHGN: A FederatedFramework for Heterogeneous Graph Neural Networks</a></td><td>Xinyu Fu</td><td><a href="https://github.com/cynricfu/FedHGN">url</a></td></tr><tr class="even"><td><a href="https://www.ijcai.org/proceedings/2023/419">GloballyConsistent Federated Graph Autoencoder for Non-IID Graphs</a></td><td>Kun Guo</td><td><a href="https://github.com/gcfgae/GCFGAE">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.14245">FedSampling: A BetterSampling Strategy for Federated Learning</a></td><td>Tao Qi</td><td><a href="https://github.com/taoqi98/FedSampling">url</a></td></tr><tr class="even"><td><a href="https://www.ijcai.org/proceedings/2023/483">FedBFPT: AnEfficient Federated Learning Framework for Bert FurtherPre-training</a></td><td>Xin'ao Wang</td><td><a href="https://github.com/Hanzhouu/FedBFPT">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.05230">FedNoRo: TowardsNoise-Robust Federated Learning by Addressing Class Imbalance and LabelNoise Heterogeneity</a></td><td>Nannan Wu</td><td><a href="https://github.com/wnn2000/FedNoRo">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2301.08143">Dual Personalization onFederated Recommendation</a></td><td>Chunxu Zhang</td><td><a href="https://github.com/Zhangcx19/IJCAI-23-PFedRec">url</a></td></tr><tr class="odd"><td><a href="https://www.ijcai.org/proceedings/2023/772">SAMBA: AGeneric Framework for Secure Federated Multi-Armed Bandits (ExtendedAbstract)</a></td><td>Radu Ciucanu</td><td><a href="https://github.com/gamarcad/paper-samba-code">url</a></td></tr><tr class="even"><td><a href="https://www.ijcai.org/proceedings/2023/838">Fedstellar: APlatform for Training Models in a Privacy-preserving and DecentralizedFashion</a></td><td>Enrique Tomás Martínez Beltrán</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.08070">A Survey of FederatedEvaluation in Federated Learning</a></td><td>Behnaz Soltani</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2304.10783">Denial-of-Service orFine-Grained Control: Towards Flexible Model Poisoning Attacks onFederated Learning</a></td><td>Hangtao Zhang</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.05221">BARA: Efficient IncentiveMechanism with Online Reward Budget Allocation in Cross-Silo FederatedLearning</a></td><td>Yunchao Yang</td><td>none</td></tr><tr class="even"><td><ahref="https://www.ijcai.org/proceedings/2023/474">Competitive-CooperativeMulti-Agent Reinforcement Learning for Auction-based FederatedLearning</a></td><td>Xiaoli Tang</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.06124">FedDWA: PersonalizedFederated Learning with Dynamic Weight Adjustment</a></td><td>Jiahao Liu</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.15347">FedET: ACommunication-Efficient Federated Class-Incremental Learning FrameworkBased on Enhanced Transformer</a></td><td>Chenghao Liu</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2307.14384">HyperFed: HyperbolicPrototypes Exploration with Consistent Aggregation for Non-IID Data inFederated Learning</a></td><td>Xinting Liao</td><td>none</td></tr><tr class="even"><td><a href="https://www.ijcai.org/proceedings/2023/0426.pdf">FederatedGraph Semantic and Structural Learning</a></td><td>Wenke Huang</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2301.12623">FedPass:Privacy-Preserving Vertical Federated Deep Learning with AdaptiveObfuscation</a></td><td>Hanlin Gu</td><td>none</td></tr><tr class="even"><td><a href="https://www.ijcai.org/proceedings/2023/245">FederatedProbabilistic Preference Distribution Modelling with CompactnessCo-Clustering for Privacy-Preserving Multi-DomainRecommendation</a></td><td>Weiming Liu</td><td>none</td></tr></tbody></table><h3 id="期刊">期刊</h3><h4 id="ai">AI</h4><h4 id="tpami">TPAMI</h4><h4 id="ijcv">IJCV</h4><h4 id="jmlr">JMLR</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://jmlr.org/papers/v24/22-0014.html">Attacks againstFederated Learning Defense Systems and their Mitigation</a></td><td>Cody Lewis</td><td><a href="https://github.com/codymlewis/viceroy">url</a></td></tr><tr class="even"><td><a href="https://jmlr.org/papers/v24/22-0440.html">FedLab: AFlexible Federated Learning Framework</a></td><td>Dun Zeng</td><td><a href="https://github.com/SMILELab-FL/FedLab">url</a></td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/22-0689.html">A General Theoryfor Federated Optimization with Asynchronous and Heterogeneous ClientsUpdates</a></td><td>Yann Fraboni</td><td><ahref="https://github.com/Accenture/Labs-Federated-Learning/tree/asynchronous_FL">url</a></td></tr><tr class="even"><td><a href="https://jmlr.org/papers/v24/21-1301.html">Memory-BasedOptimization Methods for Model-Agnostic Meta-Learning and PersonalizedFederated Learning</a></td><td>Bokun Wang</td><td><a href="https://github.com/bokun-wang/moml">url</a></td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/21-0445.html">A First Look intothe Carbon Footprint of Federated Learning</a></td><td>Xinchi Qiu</td><td>none</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Federated Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 AI CCF-A Meta-Learning 一窥（持续更新中）</title>
    <link href="/2023/12/05/2023-ai-ccf-a-meta-learning-yi-kui/"/>
    <url>/2023/12/05/2023-ai-ccf-a-meta-learning-yi-kui/</url>
    
    <content type="html"><![CDATA[<p>2023年度的 AI 顶会和顶刊已经出炉，在此把关于 Meta-Learning的部分筛选出来，以供后续学习，整理内容已同步在 <ahref="https://github.com/LFD-byte/awesome-meta-learning-papers/blob/main/2023_CCF-A_top_meta-learning.md">Github</a>。</p><h2 id="人工智能">人工智能</h2><h3 id="aaai">AAAI</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25629">BETA-CD: ABayesian Meta-Learned Cognitive Diagnosis Framework for PersonalizedLearning</a></td><td>Haoyang Bi</td><td><a href="https://github.com/AyiStar/pyat">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25654">MetaTPTrans:A Meta Learning Approach for Multilingual Code RepresentationLearning</a></td><td>Weiguo Pian</td><td><a href="https://github.com/weiguoPian/MetaTPTrans">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.04418">Learning to SelectPivotal Samples for Meta Re-weighting</a></td><td>Yinjun Wu</td><td><ahref="https://github.com/thuwuyinjun/meta_sample_selections">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25846">Meta-Sketch:A Neural Data Structure for Estimating Item Frequencies of DataStreams</a></td><td>Yukun Cao</td><td><a href="https://github.com/FFY0/meta-sketch/tree/main">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25976">Spatio-TemporalMeta-Graph Learning for Traffic Forecasting</a></td><td>Renhe Jiang</td><td><a href="https://github.com/deepkashiwa20/MegaCRN">url</a></td></tr><tr class="even"><td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26012">TheEffect of Diversity in Meta-Learning</a></td><td>Ramnath Kumar</td><td><ahref="https://github.com/RamnathKumar181/Task-Diversity-meta-learning">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.09731">CMVAE: Causal Meta VAEfor Unsupervised Meta-Learning</a></td><td>Guodong Qi</td><td><a href="https://github.com/GuodongQi/CMVAE">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2303.17768">Scalable BayesianMeta-Learning through Generalized Implicit Gradients</a></td><td>Yilang Zhang</td><td><a href="https://github.com/zhangyilang/iBaML">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26621">BalancedMeta Learning and Diverse Sampling for Lifelong Task-Oriented DialogueSystems</a></td><td>Qiancheng Xu</td><td><a href="https://github.com/travis-xu/MetaLTDS">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26680">Task-AdaptiveMeta-Learning Framework for Advancing Spatial Generalizability</a></td><td>Zhexiong Liu</td><td><ahref="https://github.com/ZhexiongLiu/Task-Adaptive-Meta-Learning">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27040">EnhancingDynamic GCN for Node Attribute Forecasting with Meta Spatial-TemporalLearning (Student Abstract)</a></td><td>Bo Wu</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.01020">Meta Learning inDecentralized Neural Networks: Towards More General AI</a></td><td>Yuwei Sun</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26776">RobustGraph Meta-Learning via Manifold Calibration with ProxySubgraphs</a></td><td>Zhenzhong Wang</td><td>none</td></tr><tr class="even"><td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26626">ADomain-Transfer Meta Task Design Paradigm for Few-Shot SlotTagging</a></td><td>Fengyi Yang</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2303.10966">Towards Reliable NeuralMachine Translation with Consistency-Aware Meta-Learning</a></td><td>Rongxiang Weng</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26313">QuantumMulti-Agent Meta Reinforcement Learning</a></td><td>Won Joon Yun</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26238">MetaZSCIL:A Meta-Learning Approach for Generalized Zero-Shot Class IncrementalLearning</a></td><td>Yanan Wu</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26210">Meta-ReinforcementLearning Based on Self-Supervised Task Representation Learning</a></td><td>Mingyang Wang</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26139">TrainingMeta-Surrogate Model for Transferable Adversarial Attack</a></td><td>Yunxiao Qin</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25823">Meta-Learningfor Simple Regret Minimization</a></td><td>Javad Azizi</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2304.06411">Meta-Auxiliary Learningfor Adaptive Human Pose Prediction</a></td><td>Qiongjie Cui</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25498">SRoUDA:Meta Self-Training for Robust Unsupervised Domain Adaptation</a></td><td>Wanqing Zhu</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25569">Multi-DomainGeneralized Graph Meta Learning</a></td><td>Mingkai Lin</td><td>none</td></tr></tbody></table><h3 id="neurlps">NeurlPS</h3><p>官方暂未发布。</p><h3 id="acl">ACL</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2305.12692">MetaAdapt: DomainAdaptive Few-Shot Misinformation Detection via Meta Learning</a></td><td>Zhenrui Yue</td><td><a href="https://github.com/Yueeeeeeee/MetaAdapt">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17373">Zero- and Few-Shot EventDetection via Prompt-Based Meta Learning</a></td><td>Zhenrui Yue</td><td><a href="https://github.com/Yueeeeeeee/MetaEvent">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2212.10559">Why Can GPT LearnIn-Context? Language Models Implicitly Perform Gradient Descent asMeta-Optimizers</a></td><td>Damai Dai</td><td><ahref="https://github.com/microsoft/LMOps/tree/main/understand_icl">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.10444">Universal InformationExtraction with Meta-Pretrained Self-Retrieval</a></td><td>Xin Cong. Bowen Yu</td><td><ahref="https://github.com/AlibabaResearch/DAMO-ConvAI">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2205.12453">Know Where You're Going:Meta-Learning for Parameter-Efficient Fine-Tuning</a></td><td>Mozhdeh Gheini</td><td>none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.727/">GeneratingLabeled Data for Relation Extraction: A Meta Learning Approach withJoint GPT-2 Training</a></td><td>Amir Pouran Ben Veyseh</td><td>none</td></tr><tr class="odd"><td><ahref="https://aclanthology.org/2023.findings-acl.699/">ContrastiveToken-Wise Meta-Learning for Unseen Performer Visual Temporal-AlignedTranslation</a></td><td>Linjun Li</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.00119">Meta-training withDemonstration Retrieval for Efficient Few-shot Learning</a></td><td>Aaron Mueller</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.01311">MetaVL: TransferringIn-Context Learning Ability From Language Models to Vision-LanguageModels</a></td><td>Masoud Monajatipoor</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.08143">Learning to Initialize:Can Meta Learning Improve Cross-task Generalization in PromptTuning?</a></td><td>Chengwei Qin</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2308.02753">DaMSTF: DomainAdversarial Learning Enhanced Meta Self-Training for DomainAdaptation</a></td><td>Menglong Lu</td><td>none</td></tr></tbody></table><h3 id="cvpr">CVPR</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Han_Noisy_Correspondence_Learning_With_Meta_Similarity_Correction_CVPR_2023_paper.html">NoisyCorrespondence Learning with Meta Similarity Correction</a></td><td>Haochen Han</td><td><a href="https://github.com/hhc1997/MSCN">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Bohdal_Meta_Omnium_A_Benchmark_for_General-Purpose_Learning-To-Learn_CVPR_2023_paper.html">MetaOmnium: A Benchmark for General-Purpose Learning-to-Learn</a></td><td>Ondrej Bohdal</td><td><ahref="https://github.com/edi-meta-learning/meta-omnium">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.html">Diversity-AwareMeta Visual Prompting</a></td><td>Qidong Huang</td><td><a href="https://github.com/shikiw/DAM-VP">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_MetaViewer_Towards_a_Unified_Multi-View_Representation_CVPR_2023_paper.html">MetaViewer:Towards A Unified Multi-View Representation</a></td><td>Ren Wang</td><td><a href="https://github.com/xxLifeLover/MetaViewer">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_MetaFusion_Infrared_and_Visible_Image_Fusion_via_Meta-Feature_Embedding_From_CVPR_2023_paper.html">MetaFusion:Infrared and Visible Image Fusion via Meta-Feature Embedding From ObjectDetection</a></td><td>Wenda Zhao</td><td><a href="https://github.com/wdzhao123/MetaFusion">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Meta-Learning_With_a_Geometry-Adaptive_Preconditioner_CVPR_2023_paper.html">Meta-Learningwith a Geometry-Adaptive Preconditioner</a></td><td>Suhyun Kang</td><td><a href="https://github.com/Suhyun777/CVPR23-GAP">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yeh_Meta-Personalizing_Vision-Language_Models_To_Find_Named_Instances_in_Video_CVPR_2023_paper.html">Meta-PersonalizingVision-Language Models to Find Named Instances in Video</a></td><td>Chun-Hsiao Yeh</td><td><a href="https://github.com/danielchyeh/this-is-my">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Tu_Learning_From_Noisy_Labels_With_Decoupled_Meta_Label_Purifier_CVPR_2023_paper.html">Learningfrom Noisy Labels with Decoupled Meta Label Purifier</a></td><td>Yuanpeng Tu</td><td><a href="https://github.com/yuanpengtu/DMLP">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Ko_MELTR_Meta_Loss_Transformer_for_Learning_To_Fine-Tune_Video_Foundation_CVPR_2023_paper.html">MELTR:Meta Loss Transformer for Learning to Fine-tune Video FoundationModels</a></td><td>Dohwan Ko</td><td><a href="https://github.com/mlvlab/MELTR">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_MetaPortrait_Identity-Preserving_Talking_Head_Generation_With_Fast_Personalized_Adaptation_CVPR_2023_paper.html">MetaPortrait:Identity-Preserving Talking Head Generation with Fast PersonalizedAdaptation</a></td><td>Bowen Zhang</td><td><a href="https://github.com/Meta-Portrait/MetaPortrait">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.09309">StyleAdv: Meta StyleAdversarial Training for Cross-Domain Few-Shot Learning</a></td><td>Yuqian Fu</td><td><a href="https://github.com/lovelyqian/StyleAdv-CDFSL">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Lee_XDNet_A_Few-Shot_Meta-Learning_Approach_for_Cross-Domain_Visual_Inspection_CVPRW_2023_paper.html">XDNet:A Few-Shot Meta-Learning Approach for Cross-Domain VisualInspection</a></td><td>Xian Yeow Lee</td><td><a href="https://github.com/xylhal/XDNet-Dataset">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Li_Compound_Expression_Recognition_In-the-Wild_With_AU-Assisted_Meta_Multi-Task_Learning_CVPRW_2023_paper.html">CompoundExpression Recognition In-the-wild with AU-assisted Meta Multi-taskLearning</a></td><td>Ximan Li</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/PBVS/html/Kamenou_A_Meta-Learning_Approach_for_Domain_Generalisation_Across_Visual_Modalities_in_CVPRW_2023_paper.html">AMeta-learning Approach for Domain Generalisation across VisualModalities in Vehicle Re-identification</a></td><td>Eleni Kamenou</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Ragonesi_Learning_Unbiased_Classifiers_From_Biased_Data_With_Meta-Learning_CVPRW_2023_paper.html">Learningunbiased classifiers from biased data with meta-learning</a></td><td>Ruggero Ragonesi</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_MetaMix_Towards_Corruption-Robust_Continual_Learning_With_Temporally_Self-Adaptive_Data_Transformation_CVPR_2023_paper.html">MetaMix:Towards Corruption-Robust Continual Learning With TemporallySelf-Adaptive Data Transformation</a></td><td>Zhenyi Wang</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Che_Image_Quality-Aware_Diagnosis_via_Meta-Knowledge_Co-Embedding_CVPR_2023_paper.html">ImageQuality-aware Diagnosis via Meta-knowledge Co-embedding</a></td><td>Haoxuan Che</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Meta_Compositional_Referring_Expression_Segmentation_CVPR_2023_paper.html">MetaCompositional Referring Expression Segmentation</a></td><td>Li Xu</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Tang_Master_Meta_Style_Transformer_for_Controllable_Zero-Shot_and_Few-Shot_Artistic_CVPR_2023_paper.html">Master:Meta Style Transformer for Controllable Zero-Shot and Few-Shot ArtisticStyle Transfer</a></td><td>Hao Tang</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_Bi-Level_Meta-Learning_for_Few-Shot_Domain_Generalization_CVPR_2023_paper.html">Bi-LevelMeta-Learning for Few-Shot Domain Generalization</a></td><td>Xiaorong Qin</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_Ground-Truth_Free_Meta-Learning_for_Deep_Compressive_Sampling_CVPR_2023_paper.html">Ground-TruthFree Meta-Learning for Deep Compressive Sampling</a></td><td>Xinran Qin</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Improving_Generalization_of_Meta-Learning_With_Inverted_Regularization_at_Inner-Level_CVPR_2023_paper.html">ImprovingGeneralization of Meta-Learning with Inverted Regularization atInner-Level</a></td><td>Lianzhe Wang</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Architecture_Dataset_and_Model-Scale_Agnostic_Data-Free_Meta-Learning_CVPR_2023_paper.html">Architecture,Dataset and Model-Scale Agnostic Data-free Meta-Learning</a></td><td>Zixuan Hu</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Meta-Causal_Learning_for_Single_Domain_Generalization_CVPR_2023_paper.html">Meta-CausalLearning for Single Domain Generalization</a></td><td>Jin Chen</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.html">Meta-TuningLoss Functions and Data Augmentation for Few-Shot ObjectDetection</a></td><td>Berkan Demirel</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Jain_A_Meta-Learning_Approach_to_Predicting_Performance_and_Data_Requirements_CVPR_2023_paper.html">AMeta-Learning Approach to Predicting Performance and DataRequirements</a></td><td>Achin Jain</td><td>none</td></tr></tbody></table><h3 id="icml">ICML</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2206.05262">Meta OptimalTransport</a></td><td>Brandon Amos</td><td><a href="https://github.com/facebookresearch/meta-ot">url</a></td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/dorrell23a.html">Meta-Learningthe Inductive Bias of Simple Neural Circuits</a></td><td>Will Dorrell</td><td><ahref="https://github.com/WilburDoz/Meta_Learning_Inductive_Bias">url</a></td></tr><tr class="odd"><td><a href="https://openreview.net/forum?id=Rg5CRU2M4Z">Meta-learningParameterized Skills</a></td><td>Haotian Fu</td><td><ahref="https://github.com/Minusadd/Meta-learning-parameterized-skills">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.03067">Memory-BasedMeta-Learning on Non-Stationary Distributions</a></td><td>Tim Genewein</td><td><ahref="https://github.com/google-deepmind/nonstationary_mbml">url</a></td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/miconi23a.html">Learningto acquire novel cognitive tasks with evolution, plasticity andmeta-meta-learning</a></td><td>Thomas Miconi</td><td><ahref="https://github.com/ThomasMiconi/LearningToLearnCogTasks">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.19923">MetaDiffuser: DiffusionModel as Conditional Planner for Offline Meta-RL</a></td><td>Fei Ni</td><td><a href="https://metadiffuser.github.io/">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.02688">Meta-SAGE: ScaleMeta-Learning Scheduled Adaptation with Guided Exploration forMitigating Scale Shift on Combinatorial Optimization</a></td><td>Jiwoo Son</td><td><a href="https://github.com/kaist-silab/meta-sage">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.10309">MetaModulation: LearningVariational Feature Hierarchies for Few-Shot Learning with FewerTasks</a></td><td>Wenfang Sun</td><td><a href="https://github.com/lmsdss/MetaModulation">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.19529">Offline MetaReinforcement Learning with In-Distribution Online Adaptation</a></td><td>Jianhao Wang</td><td><a href="https://github.com/NagisaZj/IDAQ_Public">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/wu23d.html">AdaptiveCompositional Continual Meta-Learning</a></td><td>Bin Wu</td><td><a href="https://github.com/BinWu-Cs/AC-CML">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.08400">Simple Embodied LanguageLearning as a Byproduct of Meta-Reinforcement Learning</a></td><td>Evan Zheran Liu</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2210.12669">Meta Learning ofInterface Conditions for Multi-Domain Physics-Informed NeuralNetworks</a></td><td>Shibo Li</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/jiang23k.html">EffectiveStructured Prompting by Meta-Learning and RepresentativeVerbalizer</a></td><td>Weisen Jiang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.18413">Learning to Learn fromAPIs: Black-Box Data-Free Meta-Learning</a></td><td>Zixuan Hu</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.01520">Multiple ThinkingAchieving Meta-Ability Decoupling for Object Navigation</a></td><td>Ronghao Dang</td><td>none</td></tr></tbody></table><h3 id="ijcai">IJCAI</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2212.06751">Speeding UpMulti-Objective Hyperparameter Optimization by Task Similarity-BasedMeta-Learning for the Tree-Structured Parzen Estimator</a></td><td>Shuhei Watanabe</td><td><ahref="https://github.com/nabenabe0928/meta-learn-tpe">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.16780">Graph Sampling-basedMeta-Learning for Molecular Property Prediction</a></td><td>Xiang Zhuang</td><td><a href="https://github.com/HICAI-ZJU/GS-Meta">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2308.02746">Meta-Tsallis-EntropyMinimization: A New Self-Training Approach for Domain Adaptation on TextClassification</a></td><td>Menglong Lu</td><td>none</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Meta-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 论文砖 Evolving Semantic Prototype Improves Generative Zero-Shot Learning</title>
    <link href="/2023/12/02/2023-lun-wen-zhuan-evolving-semantic-prototype-improves-generative-zero-shot-learning/"/>
    <url>/2023/12/02/2023-lun-wen-zhuan-evolving-semantic-prototype-improves-generative-zero-shot-learning/</url>
    
    <content type="html"><![CDATA[<p>Zero-shot learning（ZSL）就是希望我们的模型能够对其从没见过的类别进行分类，让机器具有推理能力，实现真正的智能。其中零次（Zero-shot）是指对于要分类的类别对象，一次也不学习。</p><h2 id="摘要">摘要</h2><p>在零样本学习（ZSL）中，生成方法基于预定义的语义原型来合成与类相关的样本特征。他们通过合成未见过的类样本特征来提高ZSL性能，以更好地训练分类器。我们观察到每个类的预定义语义原型（也称为语义嵌入或条件）与其真实语义原型并不准确匹配。因此，合成的视觉样本特征不能忠实地代表真实样本特征，限制了分类器的训练和现有的ZSL性能。在本文中，我们将这种不匹配现象表述为<strong>视觉语义域转移问题</strong>。我们提出了一种动态语义原型演化（DSP）方法，以将经验预定义的语义原型与真实原型相结合，以进行与类相关的特征合成。通过在统一的框架中细化样本特征和语义原型并使合成的视觉样本特征接近真实样本特征来学习对齐。对齐后，未见过类的合成样本特征更接近真实样本特征，有利于DSP 在标准 CUB、SUN AWA2 数据集上将现有生成 ZSL 方法改进 8.5%、8.0% 和9.7%，显着的性能提升表明：不断发展的语义原型探索了 ZSL 的处女地。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Few-shot Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 论文砖 Leveraging Label Non-Uniformity for Node Classification in Graph Neural Networks</title>
    <link href="/2023/12/01/2023-lun-wen-zhuan-leveraging-label-non-uniformity-for-node-classification-in-graph-neural-networks/"/>
    <url>/2023/12/01/2023-lun-wen-zhuan-leveraging-label-non-uniformity-for-node-classification-in-graph-neural-networks/</url>
    
    <content type="html"><![CDATA[<p>符号约定</p><p><span class="math inline">\(\mathcal{G}=(\mathcal{V},\mathcal{E})\)</span></p><p>无向图，点集为 <spanclass="math inline">\(\mathcal{V}\)</span>，边集为 <spanclass="math inline">\(\mathcal{E}\)</span></p><p><span class="math inline">\(\mathbb{S}\)</span>为类标签集合，每一个点 <span class="math inline">\(v \in\mathcal{V}\)</span> 有一个标签 ，对于每类标签 <spanclass="math inline">\(s \in \mathbb{S}\)</span>，<spanclass="math inline">\(\mathcal{V}_s\)</span> 表示标签为 <spanclass="math inline">\(s\)</span> 的点集。</p><p>训练集表示为 <spanclass="math inline">\(\mathfrak{R}\)</span>，测试集表示为 <spanclass="math inline">\(\mathfrak{T}\)</span>。</p><p><span class="math inline">\((f_1(v),f_2(v))\)</span></p><p><span class="math inline">\(f_1(v) = min_{v&#39; \in \mathfrak{R}}d_{\mathcal{G}}(v,v&#39;)\)</span></p><p><span class="math inline">\(\text{arg} min_{v&#39; \in \mathfrak{R}}d_{\mathcal{G}}(v,v&#39;)\)</span></p><p><span class="math inline">\(g(v)\)</span></p><p><span class="math inline">\(f_2 = 1 - g(v)\)</span></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 AI CCF-A Drug 一窥（持续更新中）</title>
    <link href="/2023/11/29/2023-ai-ccf-a-drug-yi-kui/"/>
    <url>/2023/11/29/2023-ai-ccf-a-drug-yi-kui/</url>
    
    <content type="html"><![CDATA[<p>2023年度的 AI 顶会已经出炉，在此把关于 Drug的部分筛选出来，以供后续学习。</p><hr /><h2 id="人工智能">人工智能</h2><h3 id="aaai">AAAI</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25665">Multi-RelationalContrastive Learning Graph Neural Network for Drug-Drug InteractionEvent Prediction</a></td><td>Zhankun Xiong</td><td><a href="https://github.com/Zhankun-Xiong/MRCGNN">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25970">DrugOOD:Out-of-Distribution Dataset Curator and Benchmark for AI-Aided DrugDiscovery - a Focus on Affinity Prediction Problems with NoiseAnnotations</a></td><td>Yuanfeng Ji</td><td><a href="https://github.com/tencent-ailab/DrugOOD">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26679">InterpretableChirality-Aware Graph Neural Network for Quantitative Structure ActivityRelationship Modeling in Drug Discovery</a></td><td>Yunchao (Lance) Liu</td><td><a href="https://github.com/meilerlab/MolKGNN">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26847">Industry-ScaleOrchestrated Federated Learning for Drug Discovery</a></td><td>Martijn Oldenhof</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26692">DetectingAnomalous Networks of Opioid Prescribers and Dispensers in PrescriptionDrug Data</a></td><td>Katie Rosman</td><td>none</td></tr><tr class="even"><td><a href="Sumana%20Basu">On the Challenges of Using ReinforcementLearning in Precision Drug Dosing: Delay and Prolongedness of ActionEffects</a></td><td>Sumana Basu</td><td>none</td></tr></tbody></table><h3 id="neurlps">NeurlPS</h3><p>官方未发布。</p><h3 id="acl">ACL</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2212.09867">Detecting ContradictoryCOVID-19 Drug Efficacy Claims from Biomedical Literature</a></td><td>Daniel N. Sosa</td><td><ahref="https://github.com/%20dnsosa/covid_lit_contra_claims">url</a></td></tr></tbody></table><h3 id="cvpr">CVPR</h3><p>无相关文章。</p><h3 id="icml">ICML</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://openreview.net/forum?id=9qy9DizMlr">DecompDiff:Diffusion Models with Decomposed Priors for Structure-Based DrugDesign</a></td><td>Jiaqi Guan</td><td><a href="https://github.%20com/bytedance/DecompDiff">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/klarner23a.html">DrugDiscovery under Covariate Shift with Domain-Informed Prior Distributionsover Functions</a></td><td>Leo Klarner</td><td><a href="https://github.com/leojklarner/Q-SAVI">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2303.03363">Enhancing ActivityPrediction Models in Drug Discovery with the Ability to Understand HumanLanguage</a></td><td>Philipp Seidl</td><td><a href="https://github.com/ml-jku/clamp">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.13997">Learning SubpocketPrototypes for Generalizable Structure-based Drug Design</a></td><td>Zaixi Zhang</td><td><a href="https://github.com/zaixizhang/FLAG">url</a></td></tr></tbody></table><h3 id="ijcai">IJCAI</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://www.ijcai.org/proceedings/2023/537">Multi-viewContrastive Learning Hypergraph Neural Network for Drug-Microbe-DiseaseAssociation Prediction</a></td><td>Luotao Liu</td><td><a href="https://github.com/Liuluotao/MCHNN">url</a></td></tr></tbody></table><hr /><h2 id="数据库数据挖掘内容检索">数据库／数据挖掘／内容检索</h2><h3 id="sigmod">SIGMOD</h3><p>无相关文章</p><h3 id="sigkdd">SIGKDD</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://dl.acm.org/doi/abs/10.1145/3580305.3599563">Knowledge-augmentedGraph Machine Learning for Drug Discovery: From Precision toInterpretability</a></td><td>Zhiqiang Zhong</td><td><ahref="https://github.com/zhiqiangzhongddu/Awesome-Knowledge-augmented-GML-forDrug-Discovery">url</a></td></tr><tr class="even"><td><a href="https://dl.acm.org/doi/abs/10.1145/3580305.3599559">Graphand Geometry Generative Modeling for Drug Discovery</a></td><td>Minkai Xu</td><td>none</td></tr></tbody></table><h3 id="icde">ICDE</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2303.02405">Decision Support Systemfor Chronic Diseases Based on Drug-Drug Interactions</a></td><td>Tian Bian</td><td><a href="https://github.com/TianBian95/DSSDDI">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9023472">DGDFS:Dependence Guided Discriminative Feature Selection for PredictingAdverse Drug-Drug Interaction : Extended Abstract</a></td><td>Jiajing Zhu</td><td><a href="https://github.com/AdverseDDI/DGDFS">url</a></td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/10184559">HyGNN:Drug-Drug Interaction Prediction via Hypergraph Neural Network</a></td><td>Khaled Mohammed Saifuddin</td><td>none</td></tr></tbody></table><h3 id="sigir">SIGIR</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="">BioSift: A Dataset for Filtering Biomedical Abstracts forDrug Repurposing and Clinical Meta-Analysis</a></td><td>David Kartchner</td><td><ahref="https://github.com/pat%20hology-dynamics/biosift/">url</a></td></tr></tbody></table><h3 id="vldb">VLDB</h3><p>无相关文章。</p><hr /><h2 id="交叉综合新兴">交叉/综合/新兴</h2><p>无法下载 PDF 文件查看是否含有代码的条目，暂时置空。</p><h3 id="www">WWW</h3><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://dl.acm.org/doi/abs/10.1145/3543507.3583872">MoleRec:Combinatorial Drug Recommendation with Substructure-Aware MolecularRepresentation Learning</a></td><td>Nianzu Yang</td><td></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 AI CCF-A Few-Shot 一窥（持续更新中）</title>
    <link href="/2023/11/29/2023-ai-ccf-a-few-shot-learning-yi-kui/"/>
    <url>/2023/11/29/2023-ai-ccf-a-few-shot-learning-yi-kui/</url>
    
    <content type="html"><![CDATA[<p>2023年度的 AI 顶会已经出炉，在此把关于 Few-Shot Learning, Zero-shotLearning 的部分筛选出来，以供后续学习，整理内容已同步在 <ahref="https://github.com/LFD-byte/awesome-shot-learning-papers/blob/main/2023_CCF-A_top_shot-learning.md">Github</a>。</p><h2 id="人工智能">人工智能</h2><h3 id="会议">会议</h3><h4 id="aaai">AAAI</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25080">Zero-ShotLinear Combinations of Grounded Social Interactions with Linear SocialMDPs</a></td><td>Ravi Tejwani</td><td><ahref="https://github.com/Linear-Social-MDP/linear-social-mdp-framework">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25102">DeconstructedGeneration-Based Zero-Shot Model</a></td><td>Dubing Chen</td><td><a href="https://github.com/cdb342/DGZ">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25114">DUET:Cross-Modal Semantic Grounding for Contrastive Zero-ShotLearning</a></td><td>Zhuo Chen</td><td><a href="https://github.com/zjukg/DUET">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25129">Incremental-DETR:Incremental Few-Shot Object Detection via Self-SupervisedLearning</a></td><td>Na Dong</td><td><ahref="https://github.com/dongnana777/Incremental-DETR">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25132">Few-ShotDefect Image Generation via Defect-Aware Feature Manipulation</a></td><td>Yuxuan Duan</td><td><a href="https://github.com/Ldhlwh/DFMGAN">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25150">RankDNN:Learning to Rank for Few-Shot Learning</a></td><td>Qianyu Guo</td><td><a href="https://github.com/guoqianyu-alberta/RankDNN">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25152">CALIP:Zero-Shot Enhancement of CLIP with Parameter-Free Attention</a></td><td>Ziyu Guo</td><td><a href="https://github.com/ZiyuGuo99/CALIP">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2301.13411">Few-Shot Object Detectionvia Variational Feature Aggregation</a></td><td>Jiaming Han</td><td><a href="https://github.com/csuhan/VFA">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25168">LeveragingSub-class Discimination for Compositional Zero-Shot Learning</a></td><td>Xiaoming Hu</td><td><a href="https://github.com/hxm97/SCD-CZSL">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25216">Disentangleand Remerge: Interventional Knowledge Distillation for Few-Shot ObjectDetection from a Conditional Causal Perspective</a></td><td>Jiangmeng Li</td><td><a href="https://github.com/ZYN-1101/DandR">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25274">BreakingImmutable: Information-Coupled Prototype Elaboration for Few-Shot ObjectDetection</a></td><td>Xiaonan Lu</td><td><a href="https://github.com/lxn96/ICPE">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25276">RobustOne-Shot Segmentation of Brain Tissues via Image-Aligned StyleTransformation</a></td><td>Jinxin Lv</td><td><ahref="https://github.com/JinxLv/One-shot-segmentation-via-IST">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2301.01081">StyleTalk: One-ShotTalking Head Generation with Controllable Speaking Styles</a></td><td>Yifeng Ma</td><td><a href="https://github.com/FuxiVirtualHuman/styletalk">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25303">FoPro:Few-Shot Guided Robust Webly-Supervised Prototypical Learning</a></td><td>Yulei Qin</td><td><a href="https://github.com/yuleiqin/fopro">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25344">Doodle toObject: Practical Zero-Shot Sketch-Based 3D Shape Retrieval</a></td><td>Bingrui Wang</td><td><a href="https://github.com/yigohw/doodle2object">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25383">Bi-directionalFeature Reconstruction Network for Fine-Grained Few-Shot ImageClassification</a></td><td>Jijie Wu</td><td><a href="https://github.com/PRIS-CV/Bi-FRN">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25385">End-to-EndZero-Shot HOI Detection via Vision and Language KnowledgeDistillation</a></td><td>Mingrui Wu</td><td><a href="https://github.com/mrwu-mac/EoID">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25394">FEditNet:Few-Shot Editing of Latent Semantics in GAN Spaces</a></td><td>Mengfei Xia</td><td><a href="https://github.com/THU-LYJ-Lab/FEditNet">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2303.15654">Few-Shot 3D Point CloudSemantic Segmentation via Stratified Class-Specific Attention BasedTransformer Network</a></td><td>Canyu Zhang</td><td><a href="https://github.com/czzhang179/SCAT">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25651">Zero-ShotRumor Detection with Propagation Structure via Prompt Learning</a></td><td>Hongzhan Lin</td><td><a href="https://github.com/PengyaoYi/zeroRumor_AAAI">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25668">Multi-LabelFew-Shot ICD Coding as Autoregressive Generation with Prompt</a></td><td>Zhichao Yang</td><td><a href="https://github.com/whaleloops/KEPT">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25758">MaximumEntropy Population-Based Training for Zero-Shot Human-AICoordination</a></td><td>Rui Zhao</td><td><ahref="https://github.com/ruizhaogit/maximum_entropy_population_based_training">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25945">Dream toGeneralize: Zero-Shot Model-Based Reinforcement Learning for UnseenVisual Distractions</a></td><td>Jeongsoo Ha</td><td><a href="https://github.com/JeongsooHa/DrG">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25999">BetterGeneralized Few-Shot Learning Even without Base Data</a></td><td>Seong-Woong Kim</td><td><ahref="https://github.com/bigdata-inha/Zero-Base-GFSL">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.06584">CompositionalPrototypical Networks for Few-Shot Classification</a></td><td>Qiang Lyu</td><td><a href="https://github.com/fikry102/CPN">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26146">Hypernetworksfor Zero-Shot Transfer in Reinforcement Learning</a></td><td>Sahand Rezaei-Shoshtari</td><td><a href="https://github.com/SAIC-MONTREAL/hyperzero">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2304.13287">ESPT: A Self-SupervisedEpisodic Spatial Pretext Task for Improving Few-Shot Learning</a></td><td>Yi Rong</td><td><a href="https://github.com/Whut-YiRong/ESPT">url</a></td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26355">BayesianCross-Modal Alignment Learning for Few-Shot Out-of-DistributionGeneralization</a></td><td>Lin Zhu</td><td><a href="https://github.com/LinLLLL/BayesCAL">url</a></td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26365">Zero-ShotAssistance in Sequential Decision Problems</a></td><td>Sebastiaan De Peuter</td><td><ahref="https://github.com/AaltoPML/Zero-Shot-Assistance-in-Sequential-Decision-Problems">url</a></td></tr><tr class="even"><td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26668">LowEmission Building Control with Zero-Shot Reinforcement Learning</a></td><td>Scott Jeen</td><td><a href="https://github.com/enjeeneer/PEARL">url</a></td></tr><tr class="odd"><td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/27019">CanYou Answer This? - Exploring Zero-Shot QA Generalization Capabilities inLarge Language Models (Student Abstract)</a></td><td>Saptarshi Sengupta</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27037">Quantifythe Political Bias in News Edits: Experiments with Few-Shot Learners(Student Abstract)</a></td><td>Preetika Verma</td><td>none</td></tr><tr class="odd"><td><a href="https://ojs.aaai.org/index.php/AAAI/article/view/26626">ADomain-Transfer Meta Task Design Paradigm for Few-Shot SlotTagging</a></td><td>Fengyi Yang</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26607">Zero-ShotFace-Based Voice Conversion: Bottleneck-Free Speech Disentanglement inthe Real-World Scenario</a></td><td>Shao-En Weng</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26566">Zero-ShotSlot Filling with Slot-Prefix Prompting and Attention RelationshipDescriptor</a></td><td>Qiaoyang Luo</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2303.16764">Boosting Few-Shot TextClassification via Distribution Estimation</a></td><td>Han Liu</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26495">Prompt-AugmentedLinear Probing: Scaling beyond the Limit of Few-Shot In-ContextLearners</a></td><td>Hyunsoo Cho</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26482">Zero-ShotCross-Lingual Event Argument Extraction with Language-OrientedPrefix-Tuning</a></td><td>Pengfei Cao</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2301.01956">High-Level SemanticFeature Matters Few-Shot Unsupervised Domain Adaptation</a></td><td>Lei Yu</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26238">MetaZSCIL:A Meta-Learning Approach for Generalized Zero-Shot Class IncrementalLearning</a></td><td>Yanan Wu</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26228">FeatureDistribution Fitting with Direction-Driven Weighting for Few-Shot ImagesClassification</a></td><td>Xin Wei</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2303.09281">SpatialFormer: Semanticand Target Aware Attentions for Few-Shot Learning</a></td><td>Jinxiang Lai</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25942">GraphKnows Unknowns: Reformulate Zero-Shot Learning as Sample-Level GraphRecognition</a></td><td>Jingcai Guo</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25931">Semi-transductiveLearning for Generalized Zero-Shot Sketch-Based Image Retrieval</a></td><td>Ce Ge</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25863">SupervisedContrastive Few-Shot Learning for High-Frequency Time Series</a></td><td>Xi Chen</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25615">Cross-DomainFew-Shot Graph Classification with a Reinforced TaskCoordinator</a></td><td>Qiannan Zhang</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25597">Few-ShotComposition Learning for Image Retrieval with Prompt Tuning</a></td><td>Junda Wu</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25417">One-ShotReplay: Boosting Incremental Object Detection via Retrospecting OneObject</a></td><td>Dongbao Yang</td><td>none</td></tr><tr class="odd"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25403">Revisitingthe Spatial and Temporal Modeling for Few-Shot ActionRecognition</a></td><td>Jiazheng Xing</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25283">ProgressiveFew-Shot Adaptation of Generative Model with Align-Free SpatialCorrelation</a></td><td>Jongbo Moon</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2301.01871">Hypotheses Tree Buildingfor One-Shot Temporal Sentence Localization</a></td><td>Daizong Liu</td><td>none</td></tr><tr class="even"><td><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25128">ExploringTuning Characteristics of Ventral Stream's Neurons for Few-Shot ImageClassification</a></td><td>Lintao Dong</td><td>none</td></tr></tbody></table><h4 id="neurlps">NeurlPS</h4><p>官方暂未发布。</p><h4 id="acl">ACL</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2212.10496">Precise Zero-Shot DenseRetrieval without Relevance Labels</a></td><td>Luyu Gao</td><td><a href="https://github.com/texttron/hyde">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2208.01009">Few-shot Adaptation Workswith UnpredicTable Data</a></td><td>Jun Shern Chan</td><td><ahref="https://github.com/JunShern/few-shot-adaptation">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.00434">Divide, Conquer, andCombine: Mixture of Semantic-Independent Experts for Zero-Shot DialogueState Tracking</a></td><td>Qingyue Wang</td><td><a href="https://github.com/qingyue2014/MoE4DST">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.128/">What MakesPre-trained Language Models Better Zero-shot Learners?</a></td><td>Jinghui Lu</td><td><ahref="https://github.com/GeorgeLuImmortal/Perplection_ACL2023">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2212.09865">Z-ICL: Zero-ShotIn-Context Learning with Pseudo-Demonstrations</a></td><td>Xinxi Lyu</td><td><a href="https://github.com/alrope123/z-icl">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.141/">Cold-StartData Selection for Better Few-shot Language Model Fine-tuning: APrompt-based Uncertainty Propagation Approach</a></td><td>Yue Yu</td><td><a href="https://github.com/yueyu1030/Patron">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.04091">Plan-and-Solve Prompting:Improving Zero-Shot Chain-of-Thought Reasoning by Large LanguageModels</a></td><td>Lei Wang</td><td><ahref="https://github.com/AGIEdgerunners/Plan-and-Solve-Prompting">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.16885">Hierarchical Verbalizerfor Few-Shot Hierarchical Text Classification</a></td><td>Ke Ji</td><td><a href="https://github.com/1KE-JI/HierVerb">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.202/">Code4Struct:Code Generation for Few-Shot Event Structure Prediction</a></td><td>Xingyao Wang</td><td><a href="https://github.com/xingyaoww/code4struct">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.234/">MANNER: AVariational Memory-Augmented Model for Cross Domain Few-Shot NamedEntity Recognition</a></td><td>Jinyuan Fang</td><td><a href="https://github.com/Alibaba-NLP/MANNER">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2212.08061">On Second Thought, Let'sNot Think Step by Step! Bias and Toxicity in Zero-ShotReasoning</a></td><td>Omar Shaikh</td><td><ahref="https://github.com/SALT-NLP/chain-of-thought-bias">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.04724">Prompter: Zero-shotAdaptive Prefixes for Dialogue State Tracking Domain Adaptation</a></td><td>Taha Aksu</td><td><a href="https://github.com/cuthalionn/Prompter">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.12692">MetaAdapt: DomainAdaptive Few-Shot Misinformation Detection via Meta Learning</a></td><td>Zhenrui Yue</td><td><a href="https://github.com/Yueeeeeeee/MetaAdapt">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.07982">Zero-shot FaithfulFactual Error Correction</a></td><td>Kung-Hsiang Huang</td><td><a href="https://github.com/khuangaf/ZeroFEC">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.15689">Zero-shot Approach toOvercome Perturbation Sensitivity of Prompts</a></td><td>Mohna Chakraborty</td><td><a href="https://github.com/Mohna0310/ZSSC">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2211.04928">miCSE: Mutual InformationContrastive Learning for Low-shot Sentence Embeddings</a></td><td>Tassilo Klein</td><td><a href="https://github.com/SAP-samples/acl2023-micse/">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.04954">RE-Matching: AFine-Grained Semantic Matching Method for Zero-Shot RelationExtraction</a></td><td>Jun Zhao</td><td><a href="https://github.com/zweny/RE-Matching">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.01750">Few-shot In-contextLearning on Knowledge Base Question Answering</a></td><td>Tianle Li</td><td><a href="https://github.com/ltl3A87/KB-BINDER">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.409/">ConsistentPrototype Learning for Few-Shot Continual Relation Extraction</a></td><td>Xiudi Chen</td><td><a href="https://github.com/XiudiChen/ConPL">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.03823">Linguisticrepresentations for fewer-shot relation extraction acrossdomains</a></td><td>Sireesh Gururaja</td><td><a href="https://github.com/ShoRit/flow_graphs">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.417/">TowardsZero-Shot Multilingual Transfer for Code-Switched Responses</a></td><td>Ting-Wei Wu</td><td><ahref="https://github.com/waynewu6250/zero-shot-multilingual-transfer-ACL-2023">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17373">Zero- and Few-Shot EventDetection via Prompt-Based Meta Learning</a></td><td>Zhenrui Yue</td><td><a href="https://github.com/Yueeeeeeee/MetaEvent">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2209.02203">Few-Shot Document-LevelEvent Argument Extraction</a></td><td>Xianjun Yang</td><td><a href="https://github.com/Xianjun-Yang/FewDocAE">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.19597">What does the Failure toReason with "Respectively" in Zero/Few-Shot Settings Tell Us aboutLanguage Models?</a></td><td>Ruixiang Cui</td><td><ahref="https://github.com/ruixiangcui/WikiResNLI_NatResNLI">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.544/">PAED:Zero-Shot Persona Attribute Extraction in Dialogues</a></td><td>Luyao Zhu</td><td><a href="https://github.com/SenticNet/PAED">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2211.08099">A Universal Discriminatorfor Zero-Shot Generalization</a></td><td>Haike Xu</td><td><a href="https://github.com/Rafa-zy/UD">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.02175">TART: Improved Few-shotText Classification Using Task-Adaptive ReferenceTransformation</a></td><td>Shuo Lei</td><td><a href="https://github.com/slei109/TART">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.625/">MultitaskPre-training of Modular Prompt for Chinese Few-Shot Learning</a></td><td>Tianxiang Sun</td><td><a href="https://github.com/Hzfinfdu/MPMP">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.01901">Few-shot Event Detection:An Empirical Study and a Unified View</a></td><td>Yubo Ma</td><td><a href="https://github.com/mayubo2333/fewshot_ED">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.631/">HINT:Hypernetwork Instruction Tuning for Efficient Zero- and Few-ShotGeneralisation</a></td><td>Hamish Ivison</td><td><ahref="https://github.com/allenai/hyper-task-descriptions">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2212.10773">MultiInstruct: ImprovingMulti-Modal Zero-Shot Learning via Instruction Tuning</a></td><td>Zhiyang Xu</td><td><a href="https://github.com/VT-NLP/MultiInstruct">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2212.09535">BLOOM+1: Adding LanguageSupport to BLOOM for Zero-Shot Prompting</a></td><td>Zheng-Xin Yong</td><td><ahref="https://github.com/bigscience-workshop/multilingual-modeling">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2308.13218">MultiCapCLIP:Auto-Encoding Prompts for Zero-Shot Multilingual VisualCaptioning</a></td><td>Bang Yang</td><td><a href="https://github.com/yangbang18/MultiCapCLIP">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.718/">UniSumm andSummZoo: Unified Model and Diverse Benchmark for Few-ShotSummarization</a></td><td>Yulong Chen</td><td><a href="https://github.com/microsoft/UniSumm">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.12567">Model-GeneratedPretraining Signals Improves Zero-Shot Generalization of Text-to-TextTransformers</a></td><td>Linyuan Gong</td><td><a href="https://github.com/gonglinyuan/metro_t0">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.747/">C-STANCE: ALarge Dataset for Chinese Zero-Shot Stance Detection</a></td><td>Chenye Zhao</td><td><a href="https://github.com/chenyez/C-STANCE">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.794/">GeneratingStructured Pseudo Labels for Noise-resistant Zero-shot Video SentenceLocalization</a></td><td>Minghang Zheng</td><td><a href="https://github.com/minghangz/SPL">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.14963">PESCO: Prompt-enhancedSelf Contrastive Learning for Zero-shot Text Classification</a></td><td>Yau-Shian Wang</td><td><a href="https://github.com/princeton-nlp/SimCSE">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.05711">CodeIE: Large CodeGeneration Models are Better Few-Shot Information Extractors</a></td><td>Peng Li</td><td><a href="https://github.com/dasepli/CodeIE">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2212.06950">Pre-trained LanguageModels Can be Fully Zero-Shot Learners</a></td><td>Xuandong Zhao</td><td><a href="https://github.com/XuandongZhao/NPPrompt">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.885/">Few-shotReranking for Multi-hop QA via Language Model Prompting</a></td><td>Muhammad Khalifa</td><td><a href="https://github.com/mukhal/PromptRank">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.04597">Language Models Get aGender Makeover: Mitigating Gender Bias with Few-Shot DataInterventions</a></td><td>Himanshu Thakur</td><td><ahref="https://github.com/joelparkerhenderson/inclusive-language">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-short.142/">EvaluatingZero-Shot Event Structures: Recommendations for Automatic ContentExtraction (ACE) Annotations</a></td><td>Erica Cai</td><td><a href="https://github.com/ec769/ZS-evalanalysis-ACE">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2210.05901">Zero-Shot Prompting forImplicit Intent Prediction and Recommendation with CommonsenseReasoning</a></td><td>Hui-Chi Kuo</td><td><a href="http://github.com/MiuLab/ImplicitBot">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.47/">EnhancingFew-shot Cross-lingual Transfer with Target Language PeculiarExamples</a></td><td>Hwichan Kim</td><td><ahref="https://github.com/hwichan0720/fewshot_transfer_with_peculiarity">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.00862">UniFine: A Unified andFine-grained Approach for Zero-shot Vision-LanguageUnderstanding</a></td><td>Rui Sun</td><td><a href="https://github.com/ThreeSR/UniFine">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.11159">Aligning InstructionTasks Unlocks Large Language Models as Zero-Shot RelationExtractors</a></td><td>Kai Zhang</td><td><a href="https://github.com/OSUNLP-Group/QA4RE">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.02830">Generative Zero-ShotPrompt Learning for Cross-Domain Slot Filling with InversePrompting</a></td><td>Xuefeng Li</td><td><a href="https://github.com/LiXuefeng2020ai/GZPL">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.13729">Discrete PromptOptimization via Constrained Generation for Zero-shot Re-ranker</a></td><td>Sukmin Cho</td><td><a href="https://github.com/zomss/Co-Prompt">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.16521">Label AgnosticPre-training for Zero-shot Text Classification</a></td><td>Christopher Clarke</td><td><ahref="https://github.com/ChrisIsKing/zero-shot-text-classification">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.69/">CLIPText: ANew Paradigm for Zero-shot Text Classification</a></td><td>Libo Qin</td><td><a href="https://github.com/LightChen233/CLIPText">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.19821">LMCap: Few-shotMultilingual Image Captioning by Retrieval Augmented Language ModelPrompting</a></td><td>Rita Ramos</td><td><a href="https://github.com/RitaRamo/lmcap">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.11442">Zero-Shot TextClassification via Self-Supervised Tuning</a></td><td>Chaoqun Liu</td><td><a href="https://github.com/DAMO-NLP-SG/SSTuning">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.11066">Adversarial Robustness ofPrompt-based Few-Shot Learning for Natural LanguageUnderstanding</a></td><td>Venkata Prabhakara Sarath Nookala</td><td><ahref="https://github.com/claws-lab/few-shot-adversarial-robustness">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.17674">X-RiSAWOZ: High-QualityEnd-to-End Multilingual Dialogue Datasets and Few-shot Agents</a></td><td>Mehrad Moradshahi</td><td><a href="https://github.com/stanford-oval/dialogues">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.05295">Boosting Zero-shotCross-lingual Retrieval by Training on Artificially Code-SwitchedData</a></td><td>Robert Litschko</td><td><a href="https://github.com/MaiNLP/CodeSwitchCLIR">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.200/">ZeroAE:Pre-trained Language Model based Autoencoder for Transductive Zero-shotText Classification</a></td><td>Kaihao Guo</td><td><ahref="https://github.com/ant-research/plm_based_autoencoder_zero_shot_text_classification">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.242/">A SimpleYet Strong Domain-Agnostic De-bias Method for Zero-Shot SentimentClassification</a></td><td>Yang Zhao</td><td><a href="https://github.com/repo4nlp/">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2210.00185">Zemi: Learning Zero-ShotSemi-Parametric Language Models from Multiple Tasks</a></td><td>Zhenhailong Wang</td><td><a href="https://github.com/MikeWangWZHL/Zemi">url</a></td></tr><tr class="even"><td><ahref="https://aclanthology.org/2023.findings-acl.253/">Coarse-to-fineFew-shot Learning for Named Entity Recognition</a></td><td>Ruotian Ma</td><td><a href="https://github.com/rtmaww/C2FNER">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.02569">Prompt to be Consistentis Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verificationwith Pre-trained Language Models</a></td><td>Fengzhu Zeng</td><td><a href="https://github.com/znhy1024/ProToCo">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.339/">DSP:Discriminative Soft Prompts for Zero-Shot Entity and RelationExtraction</a></td><td>Bo Lv</td><td><a href="https://github.com/declare-lab/RelationPrompt">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2307.00119">Meta-training withDemonstration Retrieval for Efficient Few-shot Learning</a></td><td>Aaron Mueller</td><td><ahref="https://github.com/facebookresearch/metatrained-demRAG">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2211.04337">Prompt-Based MetricLearning for Few-Shot NER</a></td><td>Yanru Chen</td><td><a href="https://github.com/AChen-qaq/ProML">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.484/">Word-levelPrefix/Suffix Sense Detection: A Case Study on Negation Sense withFew-shot Learning</a></td><td>Yameng Li</td><td><ahref="https://github.com/mengmeng233/Word-level-Prefix-Suffix-Sense-Detection">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.15904">MTCue: Learning Zero-ShotControl of Extra-Textual Attributes by Leveraging Unstructured Contextin Neural Machine Translation</a></td><td>Sebastian Vincent</td><td><a href="https://github.com/st-vincent1/MTCue">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2211.03252">Zero-Shot Classificationby Logical Reasoning on Natural Language Explanations</a></td><td>Chi Han</td><td><a href="https://github.com/Glaciohound/CLORE">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17006">Zero-shot Visual QuestionAnswering with Language Model Feedback</a></td><td>Yifan Du</td><td><a href="https://github.com/RUCAIBox/LAMOC">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.10930">On the Off-Target Problemof Zero-Shot Multilingual Neural Machine Translation</a></td><td>Liang Chen</td><td><ahref="https://github.com/PKUnlpicler/Off-Target-MNMT">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17812">Tab-CoT: Zero-shotTabular Chain of Thought</a></td><td>Ziqi Jin</td><td><a href="https://github.com/Xalp/Tab-CoT">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2212.10505">DePlot: One-shot visuallanguage reasoning by plot-to-table translation</a></td><td>Fangyu Liu</td><td><ahref="https://github.com/google-research/google-research/tree/master/deplot">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.18170">Leveraging Training Datain Few-Shot Prompting for Numerical Reasoning</a></td><td>Zhanming Jie</td><td><a href="https://github.com/allanj/dynamic-pal">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.05278">Revisit Few-shot IntentClassification with PLMs: Direct Fine-tuning vs. ContinualPre-training</a></td><td>Haode Zhang</td><td><a href="https://github.com/hdzhang-code/DFTPlus">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.10169">Few-shot Joint MultimodalAspect-Sentiment Analysis Based on Generative Multimodal Prompt</a></td><td>Xiaocui Yang</td><td><a href="https://github.com/YangXiaocui1215/GMP">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.10703">ReGen: Zero-Shot TextClassification via Training Data Generation with Progressive DenseRetrieval</a></td><td>Yue Yu</td><td><a href="https://github.com/yueyu1030/ReGen">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2211.05750">Nano: NestedHuman-in-the-Loop Reward Learning for Few-shot Language ModelControl</a></td><td>Xiang Fan</td><td><a href="https://github.com/sfanxiang/Nano">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.07310">Improving Zero-shotMultilingual Neural Machine Translation by Leveraging Cross-lingualConsistency Regularization</a></td><td>Pengzhi Gao</td><td><a href="https://github.com/gpengzhi/CrossConSTMT">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.16938">Few-shot Fine-tuning vs.In-context Learning: A Fair Comparison and Evaluation</a></td><td>Marius Mosbach</td><td><a href="https://github.com/uds-lsv/llmft">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.06616">Serial ContrastiveKnowledge Distillation for Continual Few-shot RelationExtraction</a></td><td>Xinyi Wang</td><td><a href="https://github.com/nju-websoft/SCKD">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.842/">WithPrejudice to None: A Few-Shot, Multilingual Transfer Learning Approachto Detect Social Bias in Low Resource Languages</a></td><td>Nihar Sahoo</td><td><ahref="https://github.com/sahoonihar/Hindi_Social_Bias">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.853/">MakingPre-trained Language Models Better Learn Few-Shot Spoken LanguageUnderstanding in More Practical Scenarios</a></td><td>Yufan Wang</td><td><a href="https://github.com/wyf401/Few-ID-BERT-Prompt">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.13497">Zshot: An Open-sourceFramework for Zero-Shot Named Entity Recognition and RelationExtraction</a></td><td>Gabriele Picco</td><td><a href="https://youtu.be/Mhc1zJXKEJQ">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.07157">Exploring Zero andFew-shot Techniques for Intent Classification</a></td><td>Soham Parikh</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.03507">"A Little is Enough":Few-Shot Quality Estimation based Corpus Filtering improves MachineTranslation</a></td><td>Akshay Batheja</td><td>none</td></tr><tr class="odd"><td><ahref="https://aclanthology.org/2023.findings-acl.852/">AdversarialKnowledge Stimulated Contrastive Prompting for Few-shot LanguageLearners</a></td><td>Kai Zheng</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17325">Why Does Zero-ShotCross-Lingual Generation Fail? An Explanation and a Solution</a></td><td>Tianjian Li</td><td>none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.784/">DSPM-NLG:A Dual Supervised Pre-trained Model for Few-shot Natural LanguageGeneration in Task-oriented Dialogue System</a></td><td>Yufan Wang</td><td>none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.633/">MixPAVE:Mix-Prompt Tuning for Few-shot Product Attribute ValueExtraction</a></td><td>Li Yang</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2210.05613">Contrastive TrainingImproves Zero-Shot Classification of Semi-structured Documents</a></td><td>Muhammad Khalifa</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2212.09104">LaSQuE: ImprovedZero-Shot Classification from Explanations Through Quantifier Modelingand Curriculum Learning</a></td><td>Sayan Ghosh</td><td>none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.455/">Few-shotLow-resource Knowledge Graph Completion with Reinforced TaskGeneration</a></td><td>Shichao Pei</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.03315">Few Shot RationaleGeneration using Self-Training with Dual Teachers</a></td><td>Aditya Srikanth Veerubhotla</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2211.07730">QueryForm: A SimpleZero-shot Form Entity Query Framework</a></td><td>Zifeng Wang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.14106">Better Zero-ShotReasoning with Self-Adaptive Prompting</a></td><td>Xingchen Wan</td><td>none</td></tr><tr class="odd"><td><ahref="https://aclanthology.org/2023.findings-acl.203/">Task-adaptiveLabel Dependency Transfer for Few-shot Named Entity Recognition</a></td><td>Shan Zhang</td><td>none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.164/">Focusing,Bridging and Prompting for Few-shot Nested Named EntityRecognition</a></td><td>Yuanyuan Xu</td><td>none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.90/">TowardsZero-Shot Persona Dialogue Generation with In-Context Learning</a></td><td>Xinchao Xu</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2211.05319">Few-shot Classificationwith Hypersphere Modeling of Prototypes</a></td><td>Ning Ding</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.17369">Modularized Zero-shot VQAwith Pre-trained Models</a></td><td>Rui Cao</td><td>none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-srw.4.pdf">Prompt-basedZero-shot Text Classification with Conceptual Knowledge</a></td><td>Yuki Wang</td><td>none</td></tr><tr class="odd"><td><ahref="https://www.amazon.science/publications/few-shot-data-to-text-generation-via-unified-representation-and-multi-source-learning">Few-ShotData-to-Text Generation via Unified Representation and Multi-SourceLearning</a></td><td>Alexander Hanbo Li</td><td>none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.787/">AmbiguousLearning from Retrieval: Towards Zero-shot Semantic Parsing</a></td><td>Shan Wu</td><td>none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.500/">Synthesize,Prompt and Transfer: Zero-shot Conversational Question Generation withPre-trained Language Model</a></td><td>Hongwei Zeng</td><td>none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.480/">Dual ClassKnowledge Propagation Network for Multi-label Few-shot IntentDetection</a></td><td>Feng Zhang</td><td>none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.391/">UniEvent:Unified Generative Model with Multi-Dimensional Prefix for Zero-ShotEvent-Relational Reasoning</a></td><td>Zhengwei Tao</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.08042">FLamE: Few-shot Learningfrom Natural Language Explanations</a></td><td>Yangqiaoyu Zhou</td><td>none</td></tr></tbody></table><h4 id="cvpr">CVPR</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_CF-Font_Content_Fusion_for_Few-Shot_Font_Generation_CVPR_2023_paper.html">CF-Font:Content Fusion for Few-Shot Font Generation</a></td><td>Chi Wang</td><td><a href="https://github.com/wangchi95/CF-Font">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_Two-Shot_Video_Object_Segmentation_CVPR_2023_paper.html">Two-shotVideo Object Segmentation</a></td><td>Kun Yan</td><td><ahref="https://github.com/ykpku/Two-shot-Video-Object-Segmentation">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Kumar_Few-Shot_Referring_Relationships_in_Videos_CVPR_2023_paper.html">Few-ShotReferring Relationships in Videos</a></td><td>Yogesh Kumar</td><td><a href="https://github.com/vl2g/RefRelations">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Sain_CLIP_for_All_Things_Zero-Shot_Sketch-Based_Image_Retrieval_Fine-Grained_or_CVPR_2023_paper.html">CLIPfor All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained orNot</a></td><td>Aneeshan Sain</td><td><a href="https://aneeshan95.github.io/SketchLVM/">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Delving_Into_Shape-Aware_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.html">Delvinginto Shape-aware Zero-shot Semantic Segmentation</a></td><td>Xinyu Liu</td><td><a href="https://github.com/Liuxinyv/SAZS">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Ma_DiGeo_Discriminative_Geometry-Aware_Learning_for_Generalized_Few-Shot_Object_Detection_CVPR_2023_paper.html">DiGeo:Discriminative Geometry-Aware Learning for Generalized Few-Shot ObjectDetection</a></td><td>Jiawei Ma</td><td><a href="https://github.com/Phoenix-V/DiGeo">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.html">AsyFOD:An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive ObjectDetection</a></td><td>Yipeng Gao</td><td><a href="https://github.com/Hlings/AsyFOD">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_Where_Is_My_Spot_Few-Shot_Image_Generation_via_Latent_Subspace_CVPR_2023_paper.html">Whereis My Spot? Few-shot Image Generation via Latent SubspaceOptimization</a></td><td>Chenxi Zheng</td><td><a href="https://github.com/chansey0529/LSO">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/An_ZBS_Zero-Shot_Background_Subtraction_via_Instance-Level_Background_Modeling_and_Foreground_CVPR_2023_paper.html">ZBS:Zero-Shot Background Subtraction via Instance-Level Background Modelingand Foreground Selection</a></td><td>Yongqi An</td><td><a href="https://github.com/CASIA-IVA-Lab/ZBS">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_MIANet_Aggregating_Unbiased_Instance_and_General_Information_for_Few-Shot_Semantic_CVPR_2023_paper.html">MIANet:Aggregating Unbiased Instance and General Information for Few-ShotSemantic Segmentation</a></td><td>Yong Yang</td><td><a href="https://github.com/Aldrich2y/MIANet">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Exploring_Incompatible_Knowledge_Transfer_in_Few-Shot_Image_Generation_CVPR_2023_paper.html">ExploringIncompatible Knowledge Transfer in Few-shot Image Generation</a></td><td>Yunqing Zhao</td><td><a href="https://github.com/yunqing-me/RICK">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Glocal_Energy-Based_Learning_for_Few-Shot_Open-Set_Recognition_CVPR_2023_paper.html">GlocalEnergy-based Learning for Few-Shot Open-Set Recognition</a></td><td>Haoyu Wang</td><td><a href="https://github.com/00why00/Glocal">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Trosten_Hubs_and_Hyperspheres_Reducing_Hubness_and_Improving_Transductive_Few-Shot_Learning_CVPR_2023_paper.html">Hubsand Hyperspheres: Reducing Hubness and Improving Transductive Few-ShotLearning with Hyperspherical Embeddings</a></td><td>Daniel J</td><td><a href="https://github.com/uitml/noHub">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.html">FreeNeRF:Improving Few-Shot Neural Rendering with Free FrequencyRegularization</a></td><td>Jiawei Yang</td><td><a href="https://github.com/Jiawei-Yang/FreeNeRF">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.html">StyleRF:Zero-Shot 3D Style Transfer of Neural Radiance Fields</a></td><td>Kunhao Liu</td><td><a href="https://github.com/Kunhao-Liu/StyleRF">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yin_NeRFInvertor_High_Fidelity_NeRF-GAN_Inversion_for_Single-Shot_Real_Image_Animation_CVPR_2023_paper.html">NeRFInvertor:High Fidelity NeRF-GAN Inversion for Single-Shot Real ImageAnimation</a></td><td>Yu Yin</td><td><a href="https://github.com/YuYin1/NeRFInvertor">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Zero-Shot_Pose_Transfer_for_Unrigged_Stylized_3D_Characters_CVPR_2023_paper.html">Zero-shotPose Transfer for Unrigged Stylized 3D Characters</a></td><td>Jiashun Wang</td><td><a href="https://jiashunwang.github.io/ZPT/">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Zero-Shot_Dual-Lens_Super-Resolution_CVPR_2023_paper.html">Zero-ShotDual-Lens Super-Resolution</a></td><td>Ruikang Xu</td><td><a href="https://github.com/XrKang/ZeDuSR">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2023_paper.html">FromImages to Textual Prompts: Zero-shot Visual Question Answering withFrozen Large Language Models</a></td><td>Jiaxian Guo</td><td><a href="https://github.com/salesforce/LAVIS/">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.html">ImprovingZero-shot Generalization and Robustness of Multi-Modal Models</a></td><td>Yunhao Ge</td><td><a href="https://github.com/gyhandy/Hierarchy-CLIP">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.html">Hint-Aug:Drawing Hints from Foundation Vision Transformers towards BoostedFew-shot Parameter-Efficient Tuning</a></td><td>Zhongzhi Yu</td><td><a href="https://github.com/GATECHEIC/Hint-Aug">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.html">ZegCLIP:Towards Adapting CLIP for Zero-shot Semantic Segmentation</a></td><td>Ziqin Zhou</td><td><a href="https://github.com/ZiqinZhou66/ZegCLIP">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Learning_Conditional_Attributes_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.html">LearningConditional Attributes for Compositional Zero-Shot Learning</a></td><td>Qingsheng Wang</td><td><a href="https://github.com/wqshmzh/CANet-CZSL">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/He_Primitive_Generation_and_Semantic-Related_Alignment_for_Universal_Zero-Shot_Segmentation_CVPR_2023_paper.html">PrimitiveGeneration and Semantic-Related Alignment for Universal Zero-ShotSegmentation</a></td><td>Shuting He</td><td><a href="https://github.com/heshuting555/PADing">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Hajimiri_A_Strong_Baseline_for_Generalized_Few-Shot_Semantic_Segmentation_CVPR_2023_paper.html">AStrong Baseline for Generalized Few-Shot Semantic Segmentation</a></td><td>Sina Hajimiri</td><td><a href="https://github.com/sinahmr/DIaM">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Guo_Zero-Shot_Generative_Model_Adaptation_via_Image-Specific_Prompt_Learning_CVPR_2023_paper.html">Zero-ShotGenerative Model Adaptation via Image-Specific Prompt Learning</a></td><td>Jiayi Guo</td><td><ahref="https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Few-Shot_Class-Incremental_Learning_via_Class-Aware_Bilateral_Distillation_CVPR_2023_paper.html">Few-ShotClass-Incremental Learning via Class-Aware BilateralDistillation</a></td><td>Linglan Zhao</td><td><a href="https://github.com/LinglanZhao/BiDistFSCIL">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Mansour_Zero-Shot_Noise2Noise_Efficient_Image_Denoising_Without_Any_Data_CVPR_2023_paper.html">Zero-ShotNoise2Noise: Efficient Image Denoising without any Data</a></td><td>Youssef Mansour</td><td><ahref="https://colab.research.google.com/drive/1i82nyizTdszyHkaHBuKPbWnTzao8HF9b">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Naeem_I2MVFormer_Large_Language_Model_Generated_Multi-View_Document_Supervision_for_Zero-Shot_CVPR_2023_paper.html">I2MVFormer:Large Language Model Generated Multi-View Document Supervision forZero-Shot Image Classification</a></td><td>Muhammad Ferjad Naeem</td><td><a href="https://github.com/ferjad/I2DFormer">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.html">Prompt,Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-ShotLearners</a></td><td>Renrui Zhang</td><td><a href="https://github.com/ZrrSkywalker/CaFo">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Hao_Learning_Attention_As_Disentangler_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.html">LearningAttention as Disentangler for Compositional Zero-Shot Learning</a></td><td>Shaozhe Hao</td><td><a href="https://github.com/haoosz/ade-czsl">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Progressive_Semantic-Visual_Mutual_Adaption_for_Generalized_Zero-Shot_Learning_CVPR_2023_paper.html">ProgressiveSemantic-Visual Mutual Adaption for Generalized Zero-ShotLearning</a></td><td>Man Liu</td><td><a href="https://github.com/ManLiuCoder/PSVMA">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Zero-Shot_Object_Counting_CVPR_2023_paper.html">Zero-ShotObject Counting</a></td><td>Jingyi Xu</td><td><ahref="https://github.com/cvlab-stonybrook/zero-shot-counting">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Ma_OTAvatar_One-Shot_Talking_Face_Avatar_With_Controllable_Tri-Plane_Rendering_CVPR_2023_paper.html">OTAvatar:One-Shot Talking Face Avatar with Controllable Tri-PlaneRendering</a></td><td>Zhiyuan Ma</td><td><a href="https://github.com/theEricMa/OTAvatar">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Li_One-Shot_High-Fidelity_Talking-Head_Synthesis_With_Deformable_Neural_Radiance_Field_CVPR_2023_paper.html">One-ShotHigh-Fidelity Talking-Head Synthesis With Deformable Neural RadianceField</a></td><td>Weichuang Li</td><td><a href="https://www.waytron.net/hidenerf/">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_MoLo_Motion-Augmented_Long-Short_Contrastive_Learning_for_Few-Shot_Action_Recognition_CVPR_2023_paper.html">MoLo:Motion-Augmented Long-Short Contrastive Learning for Few-Shot ActionRecognition</a></td><td>Xiang Wang</td><td><a href="https://github.com/alibaba-mmai-research/MoLo">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.html">Pic2Word:Mapping Pictures to Words for Zero-shot Composed ImageRetrieval</a></td><td>Kuniaki Saito</td><td><ahref="https://github.com/google-research/composed_image_retrieval">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.html">MultimodalityHelps Unimodality: Cross-Modal Few-Shot Learning with MultimodalModels</a></td><td>Zhiqiu Lin</td><td><ahref="https://github.com/linzhiqiu/cross_modal_adaptation">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Goyal_Finetune_Like_You_Pretrain_Improved_Finetuning_of_Zero-Shot_Vision_Models_CVPR_2023_paper.html">FinetuneLike You Pretrain: Improved Finetuning of Zero-Shot VisionModels</a></td><td>Sachin Goyal</td><td><a href="https://github.com/locuslab/FLYP">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Zero-Shot_Referring_Image_Segmentation_With_Global-Local_Context_Features_CVPR_2023_paper.html">Zero-shotReferring Image Segmentation with Global-Local Context Features</a></td><td>Seonghoon Yu</td><td><a href="https://github.com/Seonghoon-Yu/Zero-shot-RIS">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/He_Semantic-Promoted_Debiasing_and_Background_Disambiguation_for_Zero-Shot_Instance_Segmentation_CVPR_2023_paper.html">Semantic-PromotedDebiasing and Background Disambiguation for Zero-Shot InstanceSegmentation</a></td><td>Shuting He</td><td><a href="https://github.com/heshuting555/D2Zero">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Supervised_Masked_Knowledge_Distillation_for_Few-Shot_Transformers_CVPR_2023_paper.html">SupervisedMasked Knowledge Distillation for Few-Shot Transformers</a></td><td>Han Lin</td><td><a href="https://github.com/HL-hanlin/SMKD">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.html">VisualLanguage Pretrained Multiple Instance Zero-Shot Transfer forHistopathology Images</a></td><td>Ming Y. Lu</td><td><a href="https://github.com/mahmoodlab/MI-Zero">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Bi-Directional_Distribution_Alignment_for_Transductive_Zero-Shot_Learning_CVPR_2023_paper.html">Bi-DirectionalDistribution Alignment for Transductive Zero-Shot Learning</a></td><td>Zhicai Wang</td><td><a href="https://github.com/Zhicaiwww/Bi-VAEGAN">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Revisiting_Prototypical_Network_for_Cross_Domain_Few-Shot_Learning_CVPR_2023_paper.html">RevisitingPrototypical Network for Cross Domain Few-Shot Learning</a></td><td>Fei Zhou</td><td><a href="https://github.com/NWPUZhoufei/LDP-Net">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.html">Dream3D:Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-ImageDiffusion Models</a></td><td>Jiale Xu</td><td><a href="https://bluestyle97.github.io/dream3d/">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Zero-Shot_Everything_Sketch-Based_Image_Retrieval_and_in_Explainable_Style_CVPR_2023_paper.html">Zero-ShotEverything Sketch-Based Image Retrieval, and in ExplainableStyle</a></td><td>Fengyin Lin</td><td><a href="https://github.com/buptLinfy/ZSE-SBIR">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.html">Few-ShotLearning with Visual Distribution Calibration and Cross-ModalDistribution Alignment</a></td><td>Runqi Wang</td><td><ahref="https://gitee.com/mindspore/models/tree/master/research/cv">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_ConZIC_Controllable_Zero-Shot_Image_Captioning_by_Sampling-Based_Polishing_CVPR_2023_paper.html">ConZIC:Controllable Zero-shot Image Captioning by Sampling-BasedPolishing</a></td><td>Zequn Zeng</td><td><a href="https://github.com/joeyz0z/ConZIC">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Decomposed_Soft_Prompt_Guided_Fusion_Enhancing_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.html">DecomposedSoft Prompt Guided Fusion Enhancing for Compositional Zero-ShotLearning</a></td><td>Xiaocheng Lu</td><td><a href="https://github.com/Forest-art/DFSP">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Peng_Hierarchical_Dense_Correlation_Distillation_for_Few-Shot_Segmentation_CVPR_2023_paper.html">HierarchicalDense Correlation Distillation for Few-Shot Segmentation</a></td><td>Bohao Peng</td><td><a href="https://github.com/Pbihao/HDMNet">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_ML2P-Encoder_On_Exploration_of_Channel-Class_Correlation_for_Multi-Label_Zero-Shot_Learning_CVPR_2023_paper.html">(ML)<spanclass="math inline">\(^2\)</span>P-Encoder: On Exploration ofChannel-Class Correlation for Multi-Label Zero-Shot Learning</a></td><td>Ziming Liu</td><td><a href="https://github.com/simonzmliu/cvpr23_mlzsl">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Transductive_Few-Shot_Learning_With_Prototype-Based_Label_Propagation_by_Iterative_Graph_CVPR_2023_paper.html">TransductiveFew-Shot Learning with Prototype-Based Label Propagation by IterativeGraph Refinement</a></td><td>Hao Zhu</td><td><a href="https://github.com/allenhaozhu/protoLP">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Boudiaf_Open-Set_Likelihood_Maximization_for_Few-Shot_Learning_CVPR_2023_paper.html">Open-SetLikelihood Maximization for Few-Shot Learning</a></td><td>Malik Boudiaf</td><td><ahref="https://github.com/ebennequin/few-shot-open-set">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Song_Learning_With_Fantasy_Semantic-Aware_Virtual_Contrastive_Constraint_for_Few-Shot_Class-Incremental_CVPR_2023_paper.html">Learningwith Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-ShotClass-Incremental Learning</a></td><td>Zeyin Song</td><td><a href="https://github.com/zysong0113/SAVC">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Hard_Sample_Matters_a_Lot_in_Zero-Shot_Quantization_CVPR_2023_paper.html">HardSample Matters a Lot in Zero-Shot Quantization</a></td><td>Huantong Li</td><td><a href="https://github.com/lihuantong/HAST">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.09309">StyleAdv: Meta StyleAdversarial Training for Cross-Domain Few-Shot Learning</a></td><td>Yuqian Fu</td><td><a href="https://github.com/lovelyqian/StyleAdv-CDFSL">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Benigmim_One-Shot_Unsupervised_Domain_Adaptation_With_Personalized_Diffusion_Models_CVPRW_2023_paper.html">One-shotUnsupervised Domain Adaptation with Personalized DiffusionModels</a></td><td>Yasser Benigmim</td><td><a href="https://github.com/yasserben/DATUM">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Shipard_Diversity_Is_Definitely_Needed_Improving_Model-Agnostic_Zero-Shot_Classification_via_Stable_CVPRW_2023_paper.html">Diversityis Definitely Needed: Improving Model-Agnostic Zero-shot Classificationvia Stable Diffusion</a></td><td>Jordan Shipard</td><td><ahref="https://github.com/Jordan-HS/Diversity_is_Definitely_Needed">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/html/Jha_APPLeNet_Visual_Attention_Parameterized_Prompt_Learning_for_Few-Shot_Remote_Sensing_CVPRW_2023_paper.html">APPLeNet:Visual Attention Parameterized Prompt Learning for Few-Shot RemoteSensing Image Generalization using CLIP</a></td><td>Mainak Singha</td><td><a href="https://github.com/mainaksingha01/APPLeNet">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/NAS/html/Zhu_AutoShot_A_Short_Video_Dataset_and_State-of-the-Art_Shot_Boundary_Detection_CVPRW_2023_paper.html">AutoShot:A Short Video Dataset and State-of-the-Art Shot BoundaryDetection</a></td><td>Wentao Zhu</td><td><a href="https://github.com/wentaozhu/AutoShot">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Belton_FewSOME_One-Class_Few_Shot_Anomaly_Detection_With_Siamese_Networks_CVPRW_2023_paper.html">FewSOME:One-Class Few Shot Anomaly Detection with Siamese Networks</a></td><td>Niamh Belton</td><td><a href="https://github.com/niamhbelton/FewSOME">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/DLGC/html/Anvekar_GPr-Net_Geometric_Prototypical_Network_for_Point_Cloud_Few-Shot_Learning_CVPRW_2023_paper.html">GPr-Net:Geometric Prototypical Network for Point Cloud Few-ShotLearning</a></td><td>Tejas Anvekar</td><td><a href="https://github.com/TejasAnvekar/GPr-Net">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/CVMI/html/Kato_One-Shot_and_Partially-Supervised_Cell_Image_Segmentation_Using_Small_Visual_Prompt_CVPRW_2023_paper.html">One-shotand Partially-Supervised Cell Image Segmentation Using Small VisualPrompt</a></td><td>Sota Kato</td><td><ahref="https://github.com/usagisukisuki/Oneshot-Part-CellSegmentation">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Lee_XDNet_A_Few-Shot_Meta-Learning_Approach_for_Cross-Domain_Visual_Inspection_CVPRW_2023_paper.html">XDNet:A Few-Shot Meta-Learning Approach for Cross-Domain VisualInspection</a></td><td>Xian Yeow Lee</td><td><a href="https://github.com/xylhal/XDNet-Dataset">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Shin_Zero-Shot_Unsupervised_Transfer_Instance_Segmentation_CVPRW_2023_paper.html">Zero-shotUnsupervised Transfer Instance Segmentation</a></td><td>Gyungin Shin</td><td><a href="https://github.com/NoelShin/zutis">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Aboah_Real-Time_Multi-Class_Helmet_Violation_Detection_Using_Few-Shot_Data_Sampling_Technique_CVPRW_2023_paper.html">Real-timeMulti-Class Helmet Violation Detection Using Few-Shot Data SamplingTechnique and YOLOv8</a></td><td>Armstrong Aboah</td><td><ahref="https://github.com/aboah1994/few-shot-Video-Data-Sampling">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/PCV/html/Ran_Few-Shot_Depth_Completion_Using_Denoising_Diffusion_Probabilistic_Model_CVPRW_2023_paper.html">Few-ShotDepth Completion Using Denoising Diffusion Probabilistic Model</a></td><td>Weihang Ran</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/CVSports/html/Deyzel_One-Shot_Skeleton-Based_Action_Recognition_on_Strength_and_Conditioning_Exercises_CVPRW_2023_paper.html">One-shotskeleton-based action recognition on strength and conditioningexercises</a></td><td>Michael Deyzel</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Devaraj_Incorporating_Visual_Grounding_in_GCN_for_Zero-Shot_Learning_of_Human_CVPRW_2023_paper.html">IncorporatingVisual Grounding In GCN For Zero-shot Learning Of Human ObjectInteraction Actions</a></td><td>Chinmaya Devaraj</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Shiba_Zero-Shot_Object_Classification_With_Large-Scale_Knowledge_Graph_CVPRW_2023_paper.html">Zero-shotObject Classification with Large-scale Knowledge Graph</a></td><td>Kohei Shiba</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Padmanabhan_LSFSL_Leveraging_Shape_Information_in_Few-Shot_Learning_CVPRW_2023_paper.html">LSFSL:Leveraging Shape Information in Few-shot Learning</a></td><td>Deepan Chakravarthi Padmanabhan</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Su_Language_Models_Are_Causal_Knowledge_Extractors_for_Zero-Shot_Video_Question_CVPRW_2023_paper.html">LanguageModels are Causal Knowledge Extractors for Zero-shot Video QuestionAnswering</a></td><td>Hung-Ting Su</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Doshi_Zero-Shot_Action_Recognition_With_Transformer-Based_Video_Semantic_Embedding_CVPRW_2023_paper.html">Zero-ShotAction Recognition with Transformer-based Video SemanticEmbedding</a></td><td>Keval Doshi</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Lin_An_Effective_Crop-Paste_Pipeline_for_Few-Shot_Object_Detection_CVPRW_2023_paper.html">AnEffective Crop-Paste Pipeline for Few-shot Object Detection</a></td><td>Shaobo Lin</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Sreenivas_Similar_Class_Style_Augmentation_for_Efficient_Cross-Domain_Few-Shot_Learning_CVPRW_2023_paper.html">SimilarClass Style Augmentation for Efficient Cross-Domain Few-ShotLearning</a></td><td>Manogna Sreenivas</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/EVW/html/Parmar_Fully-Binarized_Distance_Computation_Based_On-Device_Few-Shot_Learning_for_XR_Applications_CVPRW_2023_paper.html">Fully-BinarizedDistance Computation based On-device Few-Shot Learning for XRapplications</a></td><td>Vivek Parmar</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/VISION/html/Zhang_What_Makes_a_Good_Data_Augmentation_for_Few-Shot_Unsupervised_Image_CVPRW_2023_paper.html">WhatMakes a Good Data Augmentation for Few-Shot Unsupervised Image AnomalyDetection?</a></td><td>Lingrui Zhang</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Lin_Explore_the_Power_of_Synthetic_Data_on_Few-Shot_Object_Detection_CVPRW_2023_paper.html">Explorethe Power of Synthetic Data on Few-shot Object Detection</a></td><td>Shaobo Lin</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Molina_Zero-Shot_Classification_at_Different_Levels_of_Granularity_CVPRW_2023_paper.html">Zero-shotClassification at Different Levels of Granularity</a></td><td>Matías Molina</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/TCV/html/Gowda_Synthetic_Sample_Selection_for_Generalized_Zero-Shot_Learning_CVPRW_2023_paper.html">SyntheticSample Selection for Generalized Zero-Shot Learning</a></td><td>Shreyank N. Gowda</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Guirguis_NIFF_Alleviating_Forgetting_in_Generalized_Few-Shot_Object_Detection_via_Neural_CVPR_2023_paper.html">NIFF:Alleviating Forgetting in Generalized Few-Shot Object Detection viaNeural Instance Feature Forging</a></td><td>Karim Guirguis</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Careil_Few-Shot_Semantic_Image_Synthesis_With_Class_Affinity_Transfer_CVPR_2023_paper.html">Few-shotSemantic Image Synthesis with Class Affinity Transfer</a></td><td>Marlène Careil</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.html">CoWson Pasture: Baselines and Benchmarks for Language-Driven Zero-ShotObject Navigation</a></td><td>Samir Yitzhak Gadre</td><td>none</td></tr><tr class="odd"><td><a href="">AVFormer: Injecting Vision into Frozen Speech Models forZero-Shot AV-ASR</a></td><td>Paul Hongsuck Seo</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_PartSLIP_Low-Shot_Part_Segmentation_for_3D_Point_Clouds_via_Pretrained_CVPR_2023_paper.html">PartSLIP:Low-Shot Part Segmentation for 3D Point Clouds via PretrainedImage-Language Models</a></td><td>Minghua Liu</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/He_Few-Shot_Geometry-Aware_Keypoint_Localization_CVPR_2023_paper.html">Few-ShotGeometry-Aware Keypoint Localization</a></td><td>Xingzhe He</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Zero-Shot_Text-to-Parameter_Translation_for_Game_Character_Auto-Creation_CVPR_2023_paper.html">Zero-ShotText-to-Parameter Translation for Game Character Auto-Creation</a></td><td>Rui Zhao</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Ma_ProD_Prompting-To-Disentangle_Domain_Knowledge_for_Cross-Domain_Few-Shot_Image_Classification_CVPR_2023_paper.html">ProD:Prompting-to-disentangle Domain Knowledge for Cross-domain Few-shotImage Classification</a></td><td>Tianyi Ma</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Generating_Features_With_Increased_Crop-Related_Diversity_for_Few-Shot_Object_Detection_CVPR_2023_paper.html">GeneratingFeatures with Increased Crop-Related Diversity for Few-Shot ObjectDetection</a></td><td>Jingyi Xu</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Du_Weak-Shot_Object_Detection_Through_Mutual_Knowledge_Transfer_CVPR_2023_paper.html">Weak-shotObject Detection through Mutual Knowledge Transfer</a></td><td>Xuanyi Du</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.html">DistillingSelf-Supervised Vision Transformers for Weakly-Supervised Few-ShotClassification &amp; Segmentation</a></td><td>Dahyun Kang</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.html">WinCLIP:Zero-/Few-Shot Anomaly Classification and Segmentation</a></td><td>Jongheon Jeong</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Sanghi_CLIP-Sculptor_Zero-Shot_Generation_of_High-Fidelity_and_Diverse_Shapes_From_Natural_CVPR_2023_paper.html">CLIP-Sculptor:Zero-Shot Generation of High-Fidelity and Diverse Shapes from NaturalLanguage</a></td><td>Aditya Sanghi</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Tang_Master_Meta_Style_Transformer_for_Controllable_Zero-Shot_and_Few-Shot_Artistic_CVPR_2023_paper.html">Master:Meta Style Transformer for Controllable Zero-Shot and Few-Shot ArtisticStyle Transfer</a></td><td>Hao Tang</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_ViewNet_A_Novel_Projection-Based_Backbone_With_View_Pooling_for_Few-Shot_CVPR_2023_paper.html">ViewNet:A Novel Projection-Based Backbone with View Pooling for Few-shot PointCloud Classification</a></td><td>Jiajing Chen</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_JAWS_Just_a_Wild_Shot_for_Cinematic_Transfer_in_Neural_CVPR_2023_paper.html">JAWS:Just A Wild Shot for Cinematic Transfer in Neural RadianceFields</a></td><td>Xi Wang</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_Bi-Level_Meta-Learning_for_Few-Shot_Domain_Generalization_CVPR_2023_paper.html">Bi-LevelMeta-Learning for Few-Shot Domain Generalization</a></td><td>Xiaorong Qin</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Tao_Boosting_Transductive_Few-Shot_Fine-Tuning_With_Margin-Based_Uncertainty_Weighting_and_Probability_CVPR_2023_paper.html">BoostingTransductive Few-Shot Fine-tuning with Margin-based UncertaintyWeighting and Probability Regularization</a></td><td>Ran Tao</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Few-Shot_Non-Line-of-Sight_Imaging_With_Signal-Surface_Collaborative_Regularization_CVPR_2023_paper.html">Few-ShotNon-Line-of-Sight Imaging with Signal-Surface CollaborativeRegularization</a></td><td>Xintong Liu</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Luo_Zero-Shot_Model_Diagnosis_CVPR_2023_paper.html">Zero-ShotModel Diagnosis</a></td><td>Jinqi Luo</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Learning_Orthogonal_Prototypes_for_Generalized_Few-Shot_Semantic_Segmentation_CVPR_2023_paper.html">LearningOrthogonal Prototypes for Generalized Few-Shot SemanticSegmentation</a></td><td>Sun-Ao Liu</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.html">One-ShotModel for Mixed-Precision Quantization</a></td><td>Ivan Koryakovskiy</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhuang_GKEAL_Gaussian_Kernel_Embedded_Analytic_Learning_for_Few-Shot_Class_Incremental_CVPR_2023_paper.html">GKEAL:Gaussian Kernel Embedded Analytic Learning for Few-Shot ClassIncremental Task</a></td><td>Huiping Zhuang</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.html">Meta-TuningLoss Functions and Data Augmentation for Few-Shot ObjectDetection</a></td><td>Berkan Demirel</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Rethinking_the_Correlation_in_Few-Shot_Segmentation_A_Buoys_View_CVPR_2023_paper.html">Rethinkingthe Correlation in Few-Shot Segmentation: A Buoys View</a></td><td>Yuan Wang</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wanyan_Active_Exploration_of_Multimodal_Complementarity_for_Few-Shot_Action_Recognition_CVPR_2023_paper.html">ActiveExploration of Multimodal Complementarity for Few-Shot ActionRecognition</a></td><td>Yuyang Wanyan</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Sato_Prompt-Guided_Zero-Shot_Anomaly_Action_Recognition_Using_Pretrained_Deep_Skeleton_Features_CVPR_2023_paper.html">Prompt-GuidedZero-Shot Anomaly Action Recognition using Pretrained Deep SkeletonFeatures</a></td><td>Fumiaki Sato</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_SMAE_Few-Shot_Learning_for_HDR_Deghosting_With_Saturation-Aware_Masked_Autoencoders_CVPR_2023_paper.html">SMAE:Few-shot Learning for HDR Deghosting with Saturation-Aware MaskedAutoencoders</a></td><td>Qingsen Yan</td><td>none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Rethinking_Few-Shot_Medical_Segmentation_A_Vector_Quantization_View_CVPR_2023_paper.html">RethinkingFew-Shot Medical Segmentation: A Vector Quantization View</a></td><td>Shiqi Huang</td><td>none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Kania_BlendFields_Few-Shot_Example-Driven_Facial_Modeling_CVPR_2023_paper.html">BlendFields:Few-Shot Example-Driven Facial Modeling</a></td><td>Kacper Kania</td><td>none</td></tr></tbody></table><h4 id="icml">ICML</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/aggarwal23a.html">SemSup-XC:Semantic Supervision for Zero and Few-shot ExtremeClassification</a></td><td>Pranjal Aggarwal</td><td><a href="https://github.com/princeton-nlp/semsup-xc">url</a></td></tr><tr class="even"><td><a href="https://openreview.net/forum?id=CNq0JvrDfw">Adaptive IMLEfor Few-shot Pretraining-free Generative Modelling</a></td><td>Mehran Aghabozorgi</td><td><a href="https://github.com/mehranagh20/AdaIMLE">url</a></td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/datta23a.html">IntervalBound Interpolation for Few-shot Learning with Few Tasks</a></td><td>Shounak Datta</td><td><a href="https://github.com/SankhaSubhra/maml-ibp-ibi">url</a></td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/frantar23a.html">SparseGPT:Massive Language Models Can be Accurately Pruned in One-Shot</a></td><td>Elias Frantar</td><td><a href="https://github.com/IST-DASLab/sparsegpt">url</a></td></tr><tr class="odd"><td><ahref="https://proceedings.mlr.press/v202/humbert23a.html">One-ShotFederated Conformal Prediction</a></td><td>Pierre Humbert</td><td><a href="https://github.com/pierreHmbt/FedCP-QQ">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.00503">MEWL: Few-shot multimodalword learning with referential uncertainty</a></td><td>Guangyuan Jiang</td><td><a href="https://github.com/jianggy/MEWL">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.04831">Cooperative Open-endedLearning Framework for Zero-Shot Coordination</a></td><td>Yang Li</td><td><a href="https://sites.google.com/view/cole-2023/">url</a></td></tr><tr class="even"><td><a href="https://openreview.net/forum?id=kgXNDkztdJ">TR0N:Translator Networks for 0-Shot Plug-and-Play ConditionalGeneration</a></td><td>Zhaoyan Liu</td><td><a href="https://github.com/layer6ai-labs/tr0n">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2301.12246">A Closer Look at Few-shotClassification Again</a></td><td>Xu Luo</td><td><ahref="https://github.com/Frankluox/CloserLookAgainFewShot">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/meng23b.html">TuningLanguage Models as Training Data Generators for Augmentation-EnhancedFew-Shot Learning</a></td><td>Yu Meng</td><td><a href="https://github.com/yumeng5/FewGen">url</a></td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/novack23a.html">CHiLS:Zero-Shot Image Classification with Hierarchical Label Sets</a></td><td>Zachary Novack</td><td><a href="https://github.com/acmi-lab/CHILS">url</a></td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/severo23a.html">One-ShotCompression of Large Edge-Exchangeable Graphs using Bits-BackCoding</a></td><td>Daniel Severo</td><td><a href="https:/github.com/dsevero/Random-Edge-Coding">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.10309">MetaModulation: LearningVariational Feature Hierarchies for Few-Shot Learning with FewerTasks</a></td><td>Wenfang Sun</td><td><a href="https://github.com/lmsdss/MetaModulation">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2210.07658">Abstract-to-ExecutableTrajectory Translation for One-Shot Task Generalization</a></td><td>Stone Tao</td><td><ahref="https://github.com/abstract-to-executable/code">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.13284">Target-Aware GenerativeAugmentations for Single-Shot Adaptation</a></td><td>Kowshik Thopalli</td><td><a href="https://github.com/Rakshith-2905/SiSTA">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2304.14636">PreNAS: PreferredOne-Shot Learning Towards Efficient Neural Architecture Search</a></td><td>Haibin Wang</td><td><a href="https://github.com/tinyvision/PreNAS">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.07307">Online PrototypeAlignment for Few-shot Policy Transfer</a></td><td>Qi Yi</td><td><a href="https://github.com/albertcity/OPA">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2301.13166">ESC: Exploration withSoft Commonsense Constraints for Zero-shot Object Navigation</a></td><td>Kaiwen Zhou</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/shin23d.html">One-shotImitation in a Non-Stationary Environment via Multi-Modal Skill</a></td><td>Sangwoo Shin</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2301.11305">DetectGPT: Zero-ShotMachine-Generated Text Detection using Probability Curvature</a></td><td>Eric Mitchell</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2301.10941">GeCoNeRF: Few-shot NeuralRadiance Fields via Geometric Consistency</a></td><td>Min-seop Kwak</td><td>none</td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/j-reddi23a.html">EfficientTraining of Language Models using Few-Shot Learning</a></td><td>Sashank J. Reddi</td><td>none</td></tr><tr class="odd"><td><a href="https://proceedings.mlr.press/v202/garcia23a.html">TheUnreasonable Effectiveness of Few-shot Learning for MachineTranslation</a></td><td>Xavier Garcia</td><td>none</td></tr><tr class="even"><td><ahref="https://proceedings.mlr.press/v202/dong23d.html">Diversity-enhancingGenerative Network for Few-shot Hypothesis Adaptation</a></td><td>Ruijiang Dong</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.06931">Evolving SemanticPrototype Improves Generative Zero-Shot Learning</a></td><td>Shiming Chen</td><td>none</td></tr><tr class="even"><td><a href="https://proceedings.mlr.press/v202/allingham23a.html">ASimple Zero-shot Prompt Weighting Technique to Improve Prompt Ensemblingin Text-Image Models</a></td><td>James Urquhart Allingham</td><td>none</td></tr></tbody></table><h4 id="ijcai">IJCAI</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://www.ijcai.org/proceedings/2023/114">WBFlow:Few-shot White Balance for sRGB Images via Reversible NeuralFlows</a></td><td>Chunxiao Li</td><td><a href="https://github.com/ChunxiaoLe/WBFlow">url</a></td></tr><tr class="even"><td><ahref="https://www.ijcai.org/proceedings/2023/0122.pdf">CompositionalZero-Shot Artistic Font Synthesis</a></td><td>Xiang Li</td><td><a href="https://github.com/moonlight03/CAFS-GAN">url</a></td></tr><tr class="odd"><td><a href="https://www.ijcai.org/proceedings/2023/0194.pdf">FGNet:Towards Filling the Intra-class and Inter-class Gaps for Few-shotSegmentation</a></td><td>Yuxuan Zhang</td><td><a href="https://github.com/YXZhang979/FGNet">url</a></td></tr><tr class="even"><td><ahref="https://palm.seu.edu.cn/hxue/publications/Learning%20to%20Learn%20from%20Corrupted%20Data%20for%20Few-Shot%20Learning.pdf">Learningto Learn from Corrupted Data for Few-Shot Learning</a></td><td>Yuexuan An</td><td><a href="https://github.com/anyuexuan/PCL">url</a></td></tr><tr class="odd"><td><a href="https://www.ijcai.org/proceedings/2023/0390.pdf">BoostingFew-Shot Open-Set Recognition with Multi-Relation Margin Loss</a></td><td>Yongjuan Che</td><td><a href="https://github.com/Casie-che/MRM">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2301.12596">Learning to Speak fromText: Zero-Shot Multilingual Text-to-Speech with Unsupervised TextPretraining</a></td><td>Takaaki Saeki</td><td><a href="https://github.com/Takaaki-Saeki/zm-text-tts">url</a></td></tr><tr class="odd"><td><ahref="https://www.sciencedirect.com/science/article/abs/pii/S0004370222000054">Unsupervisedand Few-Shot Parsing from Pretrained Language Models (ExtendedAbstract)</a></td><td>Zhiyuan Zeng</td><td>none</td></tr><tr class="even"><td><a href="https://www.ijcai.org/proceedings/2023/589">LearningFew-shot Sample-set Operations for Noisy Multi-label Aspect CategoryDetection</a></td><td>Shiman Zhao</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2308.08563">KMF: Knowledge-AwareMulti-Faceted Representation Learning for Zero-Shot NodeClassification</a></td><td>Likang Wu</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.00454">Few-shot Classificationvia Ensemble Learning with Multi-Order Statistics</a></td><td>Sai Yang</td><td>none</td></tr><tr class="odd"><td><ahref="https://www.ijcai.org/proceedings/2023/0163.pdf">HierarchicalPrompt Learning for Compositional Zero-Shot Recognition</a></td><td>Henan Wang</td><td>none</td></tr><tr class="even"><td><a href="https://www.ijcai.org/proceedings/2023/0123.pdf">VS-Boost:Boosting Visual-Semantic Association for Generalized Zero-ShotLearning</a></td><td>Xiaofan Li</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2304.10093">Clustered-patch ElementConnection for Few-shot Learnin</a></td><td>Jinxiang Lai</td><td>none</td></tr><tr class="even"><td><a href="https://www.ijcai.org/proceedings/2023/0073.pdf">RZCR:Zero-shot Character Recognition via Radical-based Reasoning</a></td><td>Xiaolei Diao</td><td>none</td></tr><tr class="odd"><td><a href="https://www.ijcai.org/proceedings/2023/0069.pdf">Null-SpaceDiffusion Sampling for Zero-Shot Point Cloud Completion</a></td><td>Xinhua Cheng</td><td>none</td></tr></tbody></table><h3 id="期刊">期刊</h3><h4 id="ai">AI</h4><p>未查阅到相关文献。</p><h4 id="tpami">TPAMI</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2007.12107">Few-Shot Object Detectionand Viewpoint Estimation for Objects in the Wild</a></td><td>Yang Xiao</td><td><ahref="https://github.com/YoungXIAO13/FewShotDetection">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2011.14663">Revisiting UnsupervisedMeta-Learning via the Characteristics of Few-Shot Tasks</a></td><td>Han-Jia Ye</td><td><a href="https://github.com/hanlu-nju/revisiting-UML">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/9839487">HolisticPrototype Activation for Few-Shot Segmentation</a></td><td>Gong Cheng</td><td><a href="https://github.com/chunbolang/HPA">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2103.04379">Repurposing GANs forOne-Shot Semantic Part Segmentation</a></td><td>Nontawat Tritrong</td><td><a href="https://github.com/trolytrol/RepurposingGANs">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2003.06777">DeepEMD: DifferentiableEarth Mover's Distance for Few-Shot Learning</a></td><td>Chi Zhang</td><td><a href="https://github.com/icoz69/DeepEMD">url</a></td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/9916072">DefensiveFew-Shot Learning</a></td><td>Wenbin Li</td><td><a href="https://github.com/WenbinLee/DefensiveFSL">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10050784">Few-ShotDrug Synergy Prediction With a Prior-Guided HypernetworkArchitecture</a></td><td>Qing-Qing Zhang</td><td><a href="https://github.com/NWPU-903PR/HyperSynergy">url</a></td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/10041935">FromInstance to Metric Calibration: A Unified Framework for Open-WorldFew-Shot Learning</a></td><td>Yuexuan An</td><td><a href="https://github.com/anyuexuan/IDEAL">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10098188">Base andMeta: A New Perspective on Few-Shot Segmentation</a></td><td>Chunbo Lang</td><td><a href="https://github.com/chunbolang/BAM">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.10511">Dual AdaptiveRepresentation Alignment for Cross-Domain Few-Shot Learning</a></td><td>Yifan Zhao</td><td><a href="https://github.com/iCVTEAM/Dara">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2203.09301">One-Shot Adaptation ofGAN in Just One CLIP</a></td><td>Gihyun Kwon</td><td><a href="https://github.com/cyclomon/OneshotCLIP">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2108.05010">Prototype Completion forFew-Shot Learning</a></td><td>Baoquan Zhang</td><td><ahref="https://github.com/zhangbq-research/Prototype_Completion_for_FSL">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10137388">Zero-ShotHyperspectral Sharpening</a></td><td>Renwei Dian</td><td><a href="https://github.com/renweidian/ZSL">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2208.00219">Meta-DETR: Image-LevelFew-Shot Detection With Inter-Class Correlation Exploitation</a></td><td>Gongjie Zhang</td><td><a href="https://github.com/ZhangGongjie/Meta-DETR">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2112.08643">TransZero++: CrossAttribute-Guided Transformer for Zero-Shot Learning</a></td><td>Shiming Chen</td><td><a href="https://github.com/shiming-chen/TransZero_pp">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2105.01241">End-to-End One-Shot HumanParsing</a></td><td>Haoyu He</td><td><ahref="https://github.com/Charleshhy/One-shot-Human-Parsing">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2101.11606">Generative Multi-LabelZero-Shot Learning</a></td><td>Akshita Gupta</td><td><a href="https://github.com/akshitac8/Generative_MLZSL">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2109.04898">LibFewShot: AComprehensive Library for Few-Shot Learning</a></td><td>Wenbin Li</td><td><a href="https://github.com/RL-VIG/LibFewShot">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2110.07510">Omni-Training: BridgingPre-Training and Meta-Training for Few-Shot Learning</a></td><td>Yang Shu</td><td><a href="https://github.com/thuml/Omni-Training">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2108.00340">Reconstruction GuidedMeta-Learning for Few Shot Open Set Recognition</a></td><td>Sayak Nag</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10224305">MMT: CrossDomain Few-Shot Learning via Meta-Memory Transfer</a></td><td>Wenjian Wang</td><td>none</td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/10179167">MNGNAS:Distilling Adaptive Combination of Multiple Searched Networks forOne-Shot Neural Architecture Search</a></td><td>Zhihua Chen</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2203.17030">Few-ShotClass-Incremental Learning by Sampling Multi-Phase Tasks</a></td><td>Da-Wei Zhou</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2210.00232">Learnable DistributionCalibration for Few-Shot Class-Incremental Learning</a></td><td>Binghao Liu</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10158446">NoAdversaries to Zero-Shot Learning: Distilling an Ensemble of GaussianFeature Generators</a></td><td>Jacopo Cavazza</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2105.02046">Few-Shot PartialMulti-View Learning</a></td><td>Yuan Zhou</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10149393">Few-ShotMulti-Agent Perception With Ranking-Based Feature Learning</a></td><td>Chenyou Fan</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2106.03158">Transferring KnowledgeFrom Text to Video: Zero-Shot Anticipation for ProceduralActions</a></td><td>Fadime Sener</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2211.16019">PatchMix Augmentation toIdentify Causal Features in Few-Shot Learning</a></td><td>Chengming Xu</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/1907.09382">Domain-Specific Priorsand Meta Learning for Few-Shot First-Person Action Recognition</a></td><td>Huseyin Coskun</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2110.14711">A Survey ofSelf-Supervised and Few-Shot Object Detection</a></td><td>Gabriel Huang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2011.08641">A Review of GeneralizedZero-Shot Learning Methods</a></td><td>Farhad Pourpanah</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/9797852">TN-ZSTAD:Transferable Network for Zero-Shot Temporal Activity Detection</a></td><td>Lingling Zhang</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/9779071">DynamicSupport Network for Few-Shot Class Incremental Learning</a></td><td>Boyu Yang</td><td>none</td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9720733">DatasetBias in Few-Shot Image Recognition</a></td><td>Shuqiang Jiang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2201.05914">Towards Zero-Shot SignLanguage Recognition</a></td><td>Yunus Can Bilge</td><td>none</td></tr></tbody></table><h4 id="ijcv">IJCV</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://link.springer.com/article/10.1007/s11263-023-01846-2">UniversalPrototype Transport for Zero-Shot Action Recognition andLocalization</a></td><td>Pascal Mettes</td><td><a href="https://github.com/psmmettes/upt">url</a></td></tr><tr class="even"><td><ahref="https://link.springer.com/article/10.1007/s11263-023-01767-0">Semantics-GuidedIntra-Category Knowledge Transfer for Generalized Zero-ShotLearning</a></td><td>Fu-En Yang</td><td>none</td></tr><tr class="odd"><td><ahref="https://link.springer.com/article/10.1007/s11263-023-01760-7">MetaAttention-Generation Network for Cross-Granularity Few-ShotLearning</a></td><td>Wenwen Qiang</td><td>none</td></tr><tr class="even"><td><ahref="https://link.springer.com/article/10.1007/s11263-022-01731-4">ACloser Look at Few-Shot 3D Point Cloud Classification</a></td><td>Chuangguan Ye</td><td>none</td></tr><tr class="odd"><td><ahref="https://link.springer.com/article/10.1007/s11263-022-01700-x">Few-ShotLearning with Complex-Valued Neural Networks and DependableLearning</a></td><td>Runqi Wang</td><td>none</td></tr></tbody></table><h4 id="jmlr">JMLR</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://jmlr.org/papers/v24/23-0037.html">Atlas: Few-shotLearning with Retrieval Augmented Language Models</a></td><td>Gautier Izacard</td><td><a href="https://github.com/facebookresearch/atlas">url</a></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Few-shot Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021 AI CCF-A 图学习一窥</title>
    <link href="/2023/11/28/2021-ai-ccf-a-tu-xue-xi-yi-kui/"/>
    <url>/2023/11/28/2021-ai-ccf-a-tu-xue-xi-yi-kui/</url>
    
    <content type="html"><![CDATA[<h2 id="iccv">ICCV</h2><table><thead><tr class="header"><th>paper</th><th style="text-align: center;">authors</th><th style="text-align: center;">code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Xue_TGRNet_A_Table_Graph_Reconstruction_Network_for_Table_Structure_Recognition_ICCV_2021_paper.html">TGRNet:A Table Graph Reconstruction Network for Table StructureRecognition</a></td><td style="text-align: center;">Wenyuan Xue</td><td style="text-align: center;"><ahref="https://github.com/xuewenyuan/TGRNet">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Nguyen_In_Defense_of_Scene_Graphs_for_Image_Captioning_ICCV_2021_paper.html">InDefense of Scene Graphs for Image Captioning</a></td><td style="text-align: center;">Kien Nguyen</td><td style="text-align: center;"><ahref="https://github.com/%20Kien085/SG2Caps">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Dimiccoli_Graph_Constrained_Data_Representation_Learning_for_Human_Motion_Segmentation_ICCV_2021_paper.html">GraphConstrained Data Representation Learning for Human MotionSegmentation</a></td><td style="text-align: center;">Mariella Dimiccoli</td><td style="text-align: center;"><ahref="https://github.com/mdimiccoli/GCRL-for-HMS/">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Xing_Learning_Hierarchical_Graph_Neural_Networks_for_Image_Clustering_ICCV_2021_paper.html">LearningHierarchical Graph Neural Networks for Image Clustering</a></td><td style="text-align: center;">Yifan Xing</td><td style="text-align: center;"><ahref="https://github.com/dmlc/dgl/tree/master/%20examples/pytorch/hilander">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Feng_Free-Form_Description_Guided_3D_Visual_Graph_Network_for_Object_Grounding_ICCV_2021_paper.html">Free-formDescription Guided 3D Visual Graph Network for Object Grounding in PointCloud</a></td><td style="text-align: center;">Mingtao Feng</td><td style="text-align: center;"><ahref="https://github.com/PNXD/FFL-3DOG">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Mou_Dynamic_Attentive_Graph_Learning_for_Image_Restoration_ICCV_2021_paper.html">DynamicAttentive Graph Learning for Image Restoration</a></td><td style="text-align: center;">Chong Mou</td><td style="text-align: center;"><ahref="https://github.com/jianzhangcs/DAGL">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Deep_Structured_Instance_Graph_for_Distilling_Object_Detectors_ICCV_2021_paper.html">DeepStructured Instance Graph for Distilling Object Detectors</a></td><td style="text-align: center;">Yixin Chen</td><td style="text-align: center;"><ahref="https://github.%20com/dvlab-research/Dsig">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Light_Field_Saliency_Detection_With_Dual_Local_Graph_Learning_and_ICCV_2021_paper.html">LightField Saliency Detection with Dual Local Graph Learning andReciprocative Guidance</a></td><td style="text-align: center;">Nian Liu</td><td style="text-align: center;"><ahref="https://github.com/%20wangbo-zhao/2021ICCV-DLGLRG">url</a></td></tr><tr class="odd"><td><a href="">Video Matting via Consistency-Regularized Graph NeuralNetworks</a></td><td style="text-align: center;">Tiantian Wang</td><td style="text-align: center;"><ahref="https://github.com/TiantianWang/VideoMattingCRGNN">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Zhou_Adaptive_Graph_Convolution_for_Point_Cloud_Analysis_ICCV_2021_paper.html">AdaptiveGraph Convolution for Point Cloud Analysis</a></td><td style="text-align: center;">Haoran Zhou</td><td style="text-align: center;"><ahref="https://github.com/%20hrzhou2/AdaptConv-master">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Moreira_Rotation_Averaging_in_a_Split_Second_A_Primal-Dual_Method_and_ICCV_2021_paper.html">RotationAveraging in a Split Second: A Primal-Dual Method and a Closed-Form forCycle Graphs</a></td><td style="text-align: center;">Gabriel Moreira</td><td style="text-align: center;"><ahref="https://github.com/gabmoreira/maks">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Arrigoni_Viewing_Graph_Solvability_via_Cycle_Consistency_ICCV_2021_paper.html">ViewingGraph Solvability via Cycle Consistency</a></td><td style="text-align: center;">Federica Arrigoni</td><td style="text-align: center;"><ahref="https://github.com/federica-arrigoni/solvability">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Li_PoGO-Net_Pose_Graph_Optimization_With_Graph_Neural_Networks_ICCV_2021_paper.html">PoGO-Net:Pose Graph Optimization with Graph Neural Networks</a></td><td style="text-align: center;">Xinyi Li</td><td style="text-align: center;"><ahref="https://github.com/xxylii/PoGO-Net">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Hutschenreiter_Fusion_Moves_for_Graph_Matching_ICCV_2021_paper.html">FusionMoves for Graph Matchin</a></td><td style="text-align: center;">Lisa Hutschenreiter</td><td style="text-align: center;"><ahref="https://vislearn.github.io/libmpopt/iccv2021">url</a></td></tr><tr class="odd"><td><a href="">Graph Contrastive Clusterin</a></td><td style="text-align: center;">Huasong Zhong</td><td style="text-align: center;"><ahref="https://github.com/mynameischaos/GCC">url</a></td></tr><tr class="even"><td><a href="">CoMatch: Semi-supervised Learning with Contrastive GraphRegularization</a></td><td style="text-align: center;">Junnan Li</td><td style="text-align: center;"><ahref="https://github.com/salesforce/CoMatch/">url</a></td></tr><tr class="odd"><td></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Modelling_Neighbor_Relation_in_Joint_Space-Time_Graph_for_Video_Correspondence_ICCV_2021_paper.html">ModellingNeighbor Relation in Joint Space-Time Graph for Video CorrespondenceLearning</a></td><td style="text-align: center;">Zixu Zhao</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="">Unified Graph Structured Models for VideoUnderstanding</a></td><td style="text-align: center;">Anurag Arnab</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="">Adversarial Example Detection Using Latent NeighborhoodGraph</a></td><td style="text-align: center;">Ahmed Abusnaina</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="">Graph-BAS3Net: Boundary-Aware Semi-SupervisedSegmentation Network with Bilateral Graph Convolution</a></td><td style="text-align: center;">Huimin Huang</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="">Cascade Image Matting with Deformable GraphRefinement</a></td><td style="text-align: center;">Zijian Yu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Wong_Persistent_Homology_Based_Graph_Convolution_Network_for_Fine-Grained_3D_Shape_ICCV_2021_paper.html">PersistentHomology based Graph Convolution Network for Fine-grained 3D ShapeSegmentation</a></td><td style="text-align: center;">Chi-Chong Wong</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Para_Generative_Layout_Modeling_Using_Constraint_Graphs_ICCV_2021_paper.html">GenerativeLayout Modeling using Constraint Graphs</a></td><td style="text-align: center;">Wamiq Para</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="">Synchronization of Group-labelled Multi-graphs</a></td><td style="text-align: center;">Andrea Porfiri Dal Cin</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Yu_Auto_Graph_Encoder-Decoder_for_Neural_Network_Pruning_ICCV_2021_paper.html">AutoGraph Encoder-Decoder for Neural Network Pruning</a></td><td style="text-align: center;">Sixing Yu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Learning_To_Match_Features_With_Seeded_Graph_Matching_Network_ICCV_2021_paper.html">Learningto Match Features with Seeded Graph Matching Network</a></td><td style="text-align: center;">Hongkai Chen</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="">Learning to Bundle-adjust: A Graph Network Approach toFaster Optimization of Bundle Adjustment for Vehicular SLAM</a></td><td style="text-align: center;">Tetsuya Tanaka</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Yamaguchi_CanvasVAE_Learning_To_Generate_Vector_Graphic_Documents_ICCV_2021_paper.html">CanvasVAE:Learning to Generate Vector Graphic Documents</a></td><td style="text-align: center;">Kota Yamaguchi</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="">Meta-Aggregator: Learning to Aggregate for 1-bit GraphNeural Networks</a></td><td style="text-align: center;">Yongcheng Jing</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Xu_BlockPlanner_City_Block_Generation_With_Vectorized_Graph_Representation_ICCV_2021_paper.html">BlockPlanner:City Block Generation with Vectorized Graph Representation</a></td><td style="text-align: center;">Linning Xu</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Li_Cross-Patch_Graph_Convolutional_Network_for_Image_Denoising_ICCV_2021_paper.html">Cross-PatchGraph Convolutional Network for Image Denoising</a></td><td style="text-align: center;">Yao Li</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Li_Towards_Efficient_Graph_Convolutional_Networks_for_Point_Cloud_Handling_ICCV_2021_paper.html">TowardsEfficient Graph Convolutional Networks for Point Cloud Handling</a></td><td style="text-align: center;">Yawei Li</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Han_Query_Adaptive_Few-Shot_Object_Detection_With_Heterogeneous_Graph_Convolutional_Networks_ICCV_2021_paper.html">QueryAdaptive Few-Shot Object Detection with Heterogeneous GraphConvolutional Networks</a></td><td style="text-align: center;">Guangxing Han</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Guo_Semi-Supervised_Active_Learning_for_Semi-Supervised_Models_Exploit_Adversarial_Examples_With_ICCV_2021_paper.html">Semi-supervisedActive Learning for Semi-supervised Models: Exploit Adversarial Exampleswith Graph-based Virtual Labels</a></td><td style="text-align: center;">Jiannan Guo</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Zhou_PR-GCN_A_Deep_Graph_Convolutional_Network_With_Point_Refinement_for_ICCV_2021_paper.html">PR-GCN:A Deep Graph Convolutional Network with Point Refinement for 6D PoseEstimation</a></td><td style="text-align: center;">Guangyuan Zhou</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_GraphFPN_Graph_Feature_Pyramid_Network_for_Object_Detection_ICCV_2021_paper.html">GraphFPN:Graph Feature Pyramid Network for Object Detection</a></td><td style="text-align: center;">Gangming Zhao</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Dual_Bipartite_Graph_Learning_A_General_Approach_for_Domain_Adaptive_ICCV_2021_paper.html">DualBipartite Graph Learning: A General Approach for Domain Adaptive ObjectDetection</a></td><td style="text-align: center;">Chaoqi Chen</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Yin_Hierarchical_Graph_Attention_Network_for_Few-Shot_Visual-Semantic_Learning_ICCV_2021_paper.html">HierarchicalGraph Attention Network for Few-shot Visual-Semantic Learning</a></td><td style="text-align: center;">Chengxiang Yin</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Li_Adaptive_Hierarchical_Graph_Reasoning_With_Semantic_Coherence_for_Video-and-Language_Inference_ICCV_2021_paper.html">AdaptiveHierarchical Graph Reasoning with Semantic Coherence forVideo-and-Language Inference</a></td><td style="text-align: center;">Juncheng Li</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Zhong_Learning_To_Generate_Scene_Graph_From_Natural_Language_Supervision_ICCV_2021_paper.html">Learningto Generate Scene Graph from Natural Language Supervision</a></td><td style="text-align: center;">Yiwu Zhong</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9711037">WassersteinCoupled Graph Learning for Cross-Modal Retrieval</a></td><td style="text-align: center;">Yun Wang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Narayanan_Shape-Biased_Domain_Generalization_via_Shock_Graph_Embeddings_ICCV_2021_paper.html">Shape-BiasedDomain Generalization via Shock Graph Embeddings</a></td><td style="text-align: center;">Maruthi Narayanan</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Li_Graph-Based_Asynchronous_Event_Processing_for_Rapid_Object_Recognition_ICCV_2021_paper.html?ref=https://githubhelp.com">Graph-basedAsynchronous Event Processing for Rapid Object Recognition</a></td><td style="text-align: center;">Yijin Li</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Chatterjee_Visual_Scene_Graphs_for_Audio_Source_Separation_ICCV_2021_paper.html">VisualScene Graphs for Audio Source Separation</a></td><td style="text-align: center;">Moitreya Chatterjee</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Transformer-Based_Dual_Relation_Graph_for_Multi-Label_Image_Recognition_ICCV_2021_paper.html">Transformer-basedDual Relation Graph for Multi-label Image Recognition</a></td><td style="text-align: center;">Jiawei Zhao</td><td style="text-align: center;">none</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 AI CCF-A 图学习一窥（持续更新中）</title>
    <link href="/2023/11/19/2023-ai-ccf-a-tu-xue-xi-yi-kui/"/>
    <url>/2023/11/19/2023-ai-ccf-a-tu-xue-xi-yi-kui/</url>
    
    <content type="html"><![CDATA[<p>2023年度的 AI顶会已经出炉，在此把关于图的论文呢筛选出来，以供后续学习，整理内容已同步在<ahref="https://github.com/LFD-byte/awesome-graph-papers/blob/main/2023_top_papers.md">Github</a>。</p><h2 id="人工智能">人工智能</h2><h3 id="会议">会议</h3><h4 id="aaai">AAAI</h4><table><thead><tr class="header"><th style="text-align: left;">name</th><th style="text-align: center;">authors</th><th style="text-align: center;">code</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25082">Self-SupervisedGraph Learning for Long-Tailed Cognitive Diagnosis</a></td><td style="text-align: center;">Shanshan Wang et al.</td><td style="text-align: center;"><ahref="https://github.com/zeng-zhen/SCD">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25304">Exposingthe Self-Supervised Space-Time Correspondence Learning via GraphKernels</a></td><td style="text-align: center;">Zheyun Qin</td><td style="text-align: center;"><ahref="https://github.com/zyqin19/VideoHiGraph">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25356">Learningto Generate an Unbiased Scene Graph by Using Attribute-Guided PredicateFeatures</a></td><td style="text-align: center;">Lei Wang</td><td style="text-align: center;"><ahref="https://github.com/wanglei0618/A-PFG">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25435">UnbiasedHeterogeneous Scene Graph Generation with Relation-Aware Message PassingNeural Network</a></td><td style="text-align: center;">Kanghoon Yoon</td><td style="text-align: center;"><ahref="https://github.com/KanghoonYoon/hetsgg-torch">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25499">Gradient-BasedGraph Attention for Scene Text Image Super-resolution</a></td><td style="text-align: center;">Xiangyuan Zhu</td><td style="text-align: center;"><ahref="https://github.com/xyzhu1/TSAN">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25527">Graphs,Constraints, and Search for the Abstraction and ReasoningCorpus</a></td><td style="text-align: center;">Yudong Xu</td><td style="text-align: center;"><ahref="https://github.com/khalil-research/ARGA-AAAI23">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2304.09015">PaTeCon: A Pattern-BasedTemporal Constraint Mining Method for Conflict Detection on KnowledgeGraphs</a></td><td style="text-align: center;">Jianhao Chen</td><td style="text-align: center;"><ahref="https://github.com/JianhaoChen-nju/PaTeCon">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.01849">Entity-Agnostic RepresentationLearning for Parameter-Efficient Knowledge Graph Embedding</a></td><td style="text-align: center;">Mingyang Chen</td><td style="text-align: center;"><ahref="https://github.com/zjukg/EARL">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.02601">Learning Representations ofBi-level Knowledge Graphs for Reasoning beyond Link Prediction</a></td><td style="text-align: center;">Chanyoung Chung</td><td style="text-align: center;"><ahref="https://github.com/bdi-lab/BiVE">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25539">LifelongEmbedding Learning and Transfer for Growing Knowledge Graphs</a></td><td style="text-align: center;">Yuanning Cui</td><td style="text-align: center;"><ahref="https://github.com/nju-websoft/LKGE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2110.08245">Rule Induction in KnowledgeGraphs Using Linear Programming</a></td><td style="text-align: center;">Sanjeeb Dash</td><td style="text-align: center;"><ahref="https://github.com/IBM/LPRules">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25547">MA-GCL:Model Augmentation Tricks for Graph Contrastive Learning</a></td><td style="text-align: center;">Xumeng Gong</td><td style="text-align: center;"><ahref="https://github.com/GXM1141/MA-GCL">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25548">Genericand Dynamic Graph Representation Learning for Crowd FlowModeling</a></td><td style="text-align: center;">Liangzhe Han</td><td style="text-align: center;"><ahref="https://github.com/liangzhehan/GDCF">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.00427">Conditional Diffusion Based onDiscrete Graph Structures for Molecular Graph Generation</a></td><td style="text-align: center;">Han Huang</td><td style="text-align: center;"><ahref="https://github.com/GRAPH-0/CDGS">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25558">Let GraphBe the Go Board: Gradient-Free Node Injection Attack for Graph NeuralNetworks via Reinforcement Learning</a></td><td style="text-align: center;">Mingxuan Ju</td><td style="text-align: center;"><ahref="https://github.com/jumxglhf/G2A2C">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25570">IterDE: AnIterative Knowledge Distillation Framework for Knowledge GraphEmbeddings</a></td><td style="text-align: center;">Jiajun Liu</td><td style="text-align: center;"><ahref="https://github.com/seukgcode/IterDE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25573">BeyondSmoothing: Unsupervised Graph Representation Learning with EdgeHeterophily Discriminating</a></td><td style="text-align: center;">Yixin Liu</td><td style="text-align: center;"><ahref="https://github.com/yixinliu233/GREET">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25576">NQE: N-aryQuery Embedding for Complex Query Answering over Hyper-RelationalKnowledge Graphs</a></td><td style="text-align: center;">Haoran Luo</td><td style="text-align: center;"><ahref="https://github.com/LHRLAB/NQE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25578">GMDNet: AGraph-Based Mixture Density Network for Estimating Packages' MultimodalTravel Time Distribution</a></td><td style="text-align: center;">Xiaowei Mao</td><td style="text-align: center;"><ahref="https://github.com/maoxiaowei97/GMDNet">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25579">Logic andCommonsense-Guided Temporal Knowledge Graph Completion</a></td><td style="text-align: center;">Guanglin Niu</td><td style="text-align: center;"><ahref="https://github.com/ngl567/LCGE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25580">GraphStructure Learning on User Mobility Data for Social RelationshipInference</a></td><td style="text-align: center;">Guangming Qin</td><td style="text-align: center;"><ahref="https://github.com/qinguangming1999/SRINet">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25588">EfficientEmbeddings of Logical Variables for Query Answering over IncompleteKnowledge Graphs</a></td><td style="text-align: center;">Dingmin Wang</td><td style="text-align: center;"><ahref="https://github.com/wdimmy/Var2Vec">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25591">Cross-DomainGraph Anomaly Detection via Anomaly-Aware Contrastive Alignment</a></td><td style="text-align: center;">Qizhou Wang</td><td style="text-align: center;"><ahref="https://github.com/QZ-WANG/ACT">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.04318">Beyond Graph ConvolutionalNetwork: An Interpretable Regularizer-Centered OptimizationFramework</a></td><td style="text-align: center;">Shiping Wang</td><td style="text-align: center;"><ahref="https://github.com/ZhihaoWu99/tsGCN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25600">KnowledgeGraph Embedding by Normalizing Flows</a></td><td style="text-align: center;">Changyi Xiao</td><td style="text-align: center;"><ahref="https://github.com/changyi7231/NFE">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25601">TemporalKnowledge Graph Reasoning with Historical Contrastive Learning</a></td><td style="text-align: center;">Yi Xu</td><td style="text-align: center;"><ahref="https://github.com/xyjigsaw/CENET">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25605">AnalogicalInference Enhanced Knowledge Graph Embedding</a></td><td style="text-align: center;">Zhen Yao</td><td style="text-align: center;"><ahref="https://github.com/zjukg/AnKGE">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25608">Next POIRecommendation with Dynamic Graph and Explicit Dependency</a></td><td style="text-align: center;">Feiyu Yin</td><td style="text-align: center;"><ahref="https://github.com/Shirley-YFY/SNPM">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25618">Deep GraphStructural Infomax</a></td><td style="text-align: center;">Wenting Zhao</td><td style="text-align: center;"><ahref="https://github.com/wtzhao1631/dgsi">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25624">GRLSTM:Trajectory Similarity Computation with Graph-Based ResidualLSTM</a></td><td style="text-align: center;">Silin Zhou</td><td style="text-align: center;"><ahref="https://github.com/slzhou-xy/GRLSTM">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25643">HeterogeneousGraph Learning for Multi-Modal Medical Data Analysis</a></td><td style="text-align: center;">Sein Kim</td><td style="text-align: center;"><ahref="https://github.com/Sein-Kim/Multimodal-Medical">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25662">Molformer:Motif-Based Transformer on 3D Heterogeneous Molecular Graphs</a></td><td style="text-align: center;">Fang Wu</td><td style="text-align: center;"><ahref="https://github.com/smiles724/Molformer">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25665">Multi-RelationalContrastive Learning Graph Neural Network for Drug-Drug InteractionEvent Prediction</a></td><td style="text-align: center;">Zhankun Xiong</td><td style="text-align: center;"><ahref="https://github.com/Zhankun-Xiong/MRCGNN">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25667">KerPrint:Local-Global Knowledge Graph Enhanced Diagnosis Prediction forRetrospective and Prospective Interpretations</a></td><td style="text-align: center;">Kai Yang</td><td style="text-align: center;"><ahref="https://github.com/xyxpku/KerPrint">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25795">DHGE:Dual-View Hyper-Relational Knowledge Graph Embedding for Link Predictionand Entity Typing</a></td><td style="text-align: center;">Haoran Luo</td><td style="text-align: center;"><ahref="https://github.com/LHRLAB/DHGE">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25839">ScalableTheory-Driven Regularization of Scene Graph Generation Models</a></td><td style="text-align: center;">Davide Buffelli</td><td style="text-align: center;"><ahref="https://github.com/tsamoura/ngp">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.11814">FTM: A Frame-Level TimelineModeling Method for Temporal Graph Representation Learning</a></td><td style="text-align: center;">Bowen Cao</td><td style="text-align: center;"><ahref="https://github.com/yeeeqichen/FTM">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25855">Where WillPlayers Move Next? Dynamic Graphs and Hierarchical Fusion for MovementForecasting in Badminton</a></td><td style="text-align: center;">Kai-Shiang Chang</td><td style="text-align: center;"><ahref="https://github.com/wywyWang/CoachAI-Projects/tree/main/Movement%20Forecasting">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25856">GraphOrdering Attention Networks</a></td><td style="text-align: center;">Michail Chatzianastasis</td><td style="text-align: center;"><ahref="https://github.com/MichailChatzianastasis/GOAT">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25858">Attributeand Structure Preserving Graph Contrastive Learning</a></td><td style="text-align: center;">Jialu Chen</td><td style="text-align: center;"><ahref="https://github.com/JialuChenChina/ASP">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25861">Context-AwareSafe Medication Recommendations with Molecular Graph and DDI GraphEmbedding</a></td><td style="text-align: center;">Qianyu Chen</td><td style="text-align: center;"><ahref="https://github.com/bit1029public/Carmen">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2303.14543">Topological Pooling onGraphs</a></td><td style="text-align: center;">Yuzhou Chen</td><td style="text-align: center;"><ahref="https://github.com/topologicalpooling/TopologicalPool">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25880">ScalableSpatiotemporal Graph Neural Networks</a></td><td style="text-align: center;">Andrea Cini</td><td style="text-align: center;"><ahref="https://github.com/Graph-Machine-Learning-Group/sgp">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25898">ElicitingStructural and Semantic Global Knowledge in Unsupervised GraphContrastive Learning</a></td><td style="text-align: center;">Kaize Ding</td><td style="text-align: center;"><ahref="https://github.com/kaize0409/S-3-CL">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25905">InterpretingUnfairness in Graph Neural Networks via Training NodeAttribution</a></td><td style="text-align: center;">Yushun Dong</td><td style="text-align: center;"><ahref="https://github.com/yushundong/BIND">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25907">GraphAnomaly Detection via Multi-Scale Contrastive Learning Networks withAugmented View</a></td><td style="text-align: center;">Jingcan Duan</td><td style="text-align: center;"><ahref="https://github.com/FelixDJC/GRADATE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25913">DirectedAcyclic Graph Structure Learning from Dynamic Graphs</a></td><td style="text-align: center;">Shaohua Fan</td><td style="text-align: center;"><ahref="https://github.com/googlebaba/GraphNOTEARS">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25916">WassersteinGraph Distance Based on L1-Approximated Tree Edit Distance betweenWeisfeiler-Lehman Subtrees</a></td><td style="text-align: center;">Zhongxi Fang</td><td style="text-align: center;"><ahref="https://github.com/Fzx-oss/WWLS">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25918">ScalableAttributed-Graph Subspace Clustering</a></td><td style="text-align: center;">Chakib Fettal</td><td style="text-align: center;"><ahref="https://github.com/chakib401/sagsc">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25925">RobustCausal Graph Representation Learning against ConfoundingEffects</a></td><td style="text-align: center;">Hang Gao</td><td style="text-align: center;"><ahref="https://github.com/hang53/RCGRL">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25943">Self-SupervisedBidirectional Learning for Graph Matching</a></td><td style="text-align: center;">Wenqi Guo</td><td style="text-align: center;"><ahref="https://github.com/CMACH508/IA-SSGM">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25952">Self-SupervisedLearning for Anomalous Channel Detection in EEG Graphs: Application toSeizure Analysis</a></td><td style="text-align: center;">Thi Kieu Khanh Ho</td><td style="text-align: center;"><ahref="https://github.com/Armanfard-Lab/EEG-CGS">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25976">Spatio-TemporalMeta-Graph Learning for Traffic Forecasting</a></td><td style="text-align: center;">Renhe Jiang</td><td style="text-align: center;"><ahref="https://github.com/deepkashiwa20/MegaCRN">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25978">Energy-MotivatedEquivariant Pretraining for 3D Molecular Graphs</a></td><td style="text-align: center;">Rui Jiao</td><td style="text-align: center;"><ahref="https://github.com/jiaor17/3D-EMGP">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26014">LoNeSampler: Graph Node Embeddings by Coordinated Local NeighborhoodSampling</a></td><td style="text-align: center;">Konstantin Kutzkov</td><td style="text-align: center;"><ahref="https://github.com/konstantinkutzkov/lone_sampler">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26021">Time-AwareRandom Walk Diffusion to Improve Dynamic Graph Learning</a></td><td style="text-align: center;">Jong-whi Lee</td><td style="text-align: center;"><ahref="https://github.com/dev-jwel/TiaRa">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26038">RestructuringGraph for Higher Homophily via Adaptive Spectral Clustering</a></td><td style="text-align: center;">Shouheng Li</td><td style="text-align: center;"><ahref="https://github.com/seanli3/graph_restructure">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26040">TowardsFine-Grained Explainability for Heterogeneous Graph NeuralNetwork</a></td><td style="text-align: center;">Tong Li</td><td style="text-align: center;"><ahref="https://github.com/LITONG99/xPath">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26057">DualLabel-Guided Graph Refinement for Multi-View Graph Clustering</a></td><td style="text-align: center;">Yawen Ling</td><td style="text-align: center;"><ahref="https://github.com/YwL-zhufeng/DuaLGR">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26071">HardSample Aware Network for Contrastive Deep Graph Clustering</a></td><td style="text-align: center;">Yue Liu</td><td style="text-align: center;"><ahref="https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26085">Recoveringthe Graph Underlying Networked Dynamical Systems under PartialObservability: A Deep Learning Approach</a></td><td style="text-align: center;">Sérgio Machado</td><td style="text-align: center;"><ahref="https://github.com/ASanctvs/Structure-Identification">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26092">BoundaryGraph Neural Networks for 3D Simulations</a></td><td style="text-align: center;">Andreas Mayr</td><td style="text-align: center;"><ahref="https://github.com/ml-jku/bgnn/">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26168">NeighborContrastive Learning on Learnable Graph Augmentation</a></td><td style="text-align: center;">Xiao Shen</td><td style="text-align: center;"><ahref="https://github.com/shenxiaocam/NCLA">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26187">FederatedLearning on Non-IID Graphs via Structural Knowledge Sharing</a></td><td style="text-align: center;">Yue Tan</td><td style="text-align: center;"><ahref="https://github.com/yuetan031/FedStar">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26192">HeterogeneousGraph Masked Autoencoders</a></td><td style="text-align: center;">Yijun Tian</td><td style="text-align: center;"><ahref="https://github.com/meettyj/HGMAE">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.05889">USER: Unsupervised StructuralEntropy-Based Robust Graph Neural Network</a></td><td style="text-align: center;">Yifei Wang</td><td style="text-align: center;"><ahref="https://github.com/wangyifeibeijing/USER">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26223">FedGS:Federated Graph-Based Sampling with Arbitrary ClientAvailability</a></td><td style="text-align: center;">Zheng Wang</td><td style="text-align: center;"><ahref="https://github.com/WwZzz/FedGS">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26231">Non-IIDTransfer Learning on Graphs</a></td><td style="text-align: center;">Jun Wu</td><td style="text-align: center;"><ahref="https://github.com/jwu4sml/GRADE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2305.10758">Extracting Low-/High- FrequencyKnowledge from Graph Neural Networks and Injecting It into MLPs: AnEffective GNN-to-MLP Distillation Framework</a></td><td style="text-align: center;">Lirong Wu</td><td style="text-align: center;"><ahref="https://github.com/LirongWu/FF-G2M">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26256">GraphPrompt:Graph-Based Prompt Templates for Biomedical Synonym Prediction</a></td><td style="text-align: center;">Hanwen Xu</td><td style="text-align: center;"><ahref="https://github.com/HanwenXuTHU/GraphPrompt">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26267">GlobalConcept-Based Interpretability for Graph Neural Networks via NeuronAnalysis</a></td><td style="text-align: center;">Han Xuanyuan</td><td style="text-align: center;"><ahref="https://github.com/xuyhan/gnn-dissect">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26272">T2G-FORMER:Organizing Tabular Features into Relation Graphs Promotes HeterogeneousFeature Interaction</a></td><td style="text-align: center;">Jiahuan Yan</td><td style="text-align: center;"><ahref="https://github.com/jyansir/t2g-former">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26276">WaveForM:Graph Enhanced Wavelet Learning for Long Sequence Forecasting ofMultivariate Time Series</a></td><td style="text-align: center;">Fuhao Yang</td><td style="text-align: center;"><ahref="https://github.com/alanyoungCN/WaveForM">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26283">Simple andEfficient Heterogeneous Graph Neural Network</a></td><td style="text-align: center;">Xiaocheng Yang</td><td style="text-align: center;"><ahref="https://github.com/ICT-GIMLab/SeHGNN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.01098">Cluster-Guided Contrastive GraphClustering Network</a></td><td style="text-align: center;">Xihong Yang</td><td style="text-align: center;"><ahref="https://github.com/xihongyang1999/CCGC">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26292">LifelongCompression Mixture Model via Knowledge Relationship Graph</a></td><td style="text-align: center;">Fei Ye</td><td style="text-align: center;"><ahref="https://github.com/dtuzi123/LifelongCompressionMix">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26296">RandomWalk Conformer: Learning Graph Representation from Long and ShortRange</a></td><td style="text-align: center;">Pei-Kai Yeh</td><td style="text-align: center;"><ahref="https://github.com/b05901024/RandomWalkConformer">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26309">JointMultimodal Entity-Relation Extraction Based on Edge-Enhanced GraphAlignment Network and Word-Pair Relation Tagging</a></td><td style="text-align: center;">Li Yuan</td><td style="text-align: center;"><ahref="https://github.com/YuanLi95/EEGA-for-JMERE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26314">LinkingSketch Patches by Learning Synonymous Proximity for Graphic SketchRepresentation</a></td><td style="text-align: center;">Sicong Zang</td><td style="text-align: center;"><ahref="https://github.com/CMACH508/SP-gra2seq">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26318">SubstructureAware Graph Neural Networks</a></td><td style="text-align: center;">DingYi Zeng</td><td style="text-align: center;"><ahref="https://github.com/BlackHalo-Drake/SAGNN-Substructure-Aware-Graph-Neural-Networks">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.05083">DRGCN: Dynamic Evolving InitialResidual for Deep Graph Convolutional Networks</a></td><td style="text-align: center;">Lei Zhang</td><td style="text-align: center;"><ahref="https://github.com/anonymousaabc/DRGCN">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26333">Let theData Choose: Flexible and Diverse Anchor Graph Fusion for ScalableMulti-View Clustering</a></td><td style="text-align: center;">Pei Zhang</td><td style="text-align: center;"><ahref="https://github.com/Jeaninezpp/FDAGF">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26338">DynamicHeterogeneous Graph Attention Neural Architecture Search</a></td><td style="text-align: center;">Zeyang Zhang</td><td style="text-align: center;"><ahref="https://github.com/wondergo2017/DHGAS">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26348">DataImputation with Iterative Graph Reconstruction</a></td><td style="text-align: center;">Jiajun Zhong</td><td style="text-align: center;"><ahref="https://github.com/G-AILab/IGRM">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.04599">Principled and Efficient MotifFinding for Structure Learning of Lifted Graphical Models</a></td><td style="text-align: center;">Jonathan Feldstein</td><td style="text-align: center;"><ahref="https://github.com/jonathanfeldstein/PRISM">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26457">GRASMOS:Graph Signage Model Selection for Gene Regulatory Networks</a></td><td style="text-align: center;">Angelina Brilliantova</td><td style="text-align: center;"><ahref="https://github.com/Restel/grasmos">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26503">CogitoErgo Summ: Abstractive Summarization of Biomedical Papers via SemanticParsing Graphs and Consistency Rewards</a></td><td style="text-align: center;">Giacomo Frisoni</td><td style="text-align: center;"><ahref="https://github.com/disi-unibo-nlp/cogito-ergo-summ">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.07507">Graphix-T5: Mixing Pre-trainedTransformers with Graph-Aware Layers for Text-to-SQL Parsing</a></td><td style="text-align: center;">Jinyang Li</td><td style="text-align: center;"><ahref="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/graphix">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2303.06565">Compressed Heterogeneous Graphfor Abstractive Multi-Document Summarization</a></td><td style="text-align: center;">Miao Li</td><td style="text-align: center;"><ahref="https://github.com/oaimli/HGSum">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.01949">SPRING: Situated ConversationAgent Pretrained with Multimodal Questions from Incremental LayoutGraph</a></td><td style="text-align: center;">Yuxing Long</td><td style="text-align: center;"><ahref="https://github.com/LYX0501/SPRING">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26568">GraphComponent Contrastive Learning for Concept RelatednessEstimation</a></td><td style="text-align: center;">Yueen Ma</td><td style="text-align: center;"><ahref="https://github.com/Panmani/GCCL">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26578">Relation-AwareLanguage-Graph Transformer for Question Answering</a></td><td style="text-align: center;">Jinyoung Park</td><td style="text-align: center;"><ahref="https://github.com/mlvlab/QAT">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.02016">Improving Interpretability viaExplicit Word Interaction Graph Layer</a></td><td style="text-align: center;">Arshdeep Sekhon</td><td style="text-align: center;"><ahref="https://github.com/QData/WIGRAPH">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2304.04152">Continual Graph ConvolutionalNetwork for Text Classification</a></td><td style="text-align: center;">Tiandeng Wu</td><td style="text-align: center;"><ahref="https://github.com/Jyonn/ContGCN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26627">Orders AreUnwanted: Dynamic Deep Graph Convolutional Network for PersonalityDetection</a></td><td style="text-align: center;">Tao Yang</td><td style="text-align: center;"><ahref="https://github.com/djz233/D-DGCN">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26679">InterpretableChirality-Aware Graph Neural Network for Quantitative Structure ActivityRelationship Modeling in Drug Discovery</a></td><td style="text-align: center;">Yunchao (Lance) Liu</td><td style="text-align: center;"><ahref="https://github.com/meilerlab/MolKGNN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.00419">For the Underrepresented inGender Bias Research: Chinese Name Gender Prediction with HeterogeneousGraph Attention Network</a></td><td style="text-align: center;">Zihao Pan</td><td style="text-align: center;"><ahref="https://github.com/ZhangDataLab/CHGAT">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26702">Semi-supervisedCredit Card Fraud Detection via Attribute-Driven GraphRepresentation</a></td><td style="text-align: center;">Sheng Xiang</td><td style="text-align: center;"><ahref="https://github.com/finint/antifraud">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26864">AnimateSVG:Autonomous Creation and Aesthetics Evaluation of Scalable VectorGraphics Animations for the Case of Brand Logos</a></td><td style="text-align: center;">Deborah Mateja</td><td style="text-align: center;"><ahref="https://github.com/AnimateSVG/AnimateSVG">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26893">CLGT: AGraph Transformer for Student Performance Prediction in CollaborativeLearning</a></td><td style="text-align: center;">Tianhao Peng</td><td style="text-align: center;"><ahref="https://github.com/Tianhao-Peng/CLGT/">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26992">Can GraphNeural Networks Learn to Solve the MaxSAT Problem? (StudentAbstract)</a></td><td style="text-align: center;">Minghao Liu</td><td style="text-align: center;"><ahref="https://github.com/minghao-liu/GMS">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27062">TgrApp:Anomaly Detection and Visualization of Large-Scale Call Graphs</a></td><td style="text-align: center;">Mirela T. Cazzolato</td><td style="text-align: center;"><ahref="https://github.com/mtcazzolato/tgrapp">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27053">Graph ofGraphs: A New Knowledge Representation Mechanism for Graph Learning(Student Abstract)</a></td><td style="text-align: center;">Zhwiei Zhen</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27052">HaPPy:Harnessing the Wisdom from Multi-Perspective Graphs for Protein-LigandBinding Affinity Prediction (Student Abstract)</a></td><td style="text-align: center;">Xianfeng Zhang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.04373">Measuring the Privacy Leakagevia Graph Reconstruction Attacks on Simplicial Neural Networks (StudentAbstract)</a></td><td style="text-align: center;">Huixin Zhan</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27043">ImprovingDialogue Intent Classification with a Knowledge-Enhanced MultifactorGraph Model (Student Abstract)</a></td><td style="text-align: center;">Huinan Xu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27039">LearningDynamic Temporal Relations with Continuous Graph for Multivariate TimeSeries Forecasting (Student Abstract)</a></td><td style="text-align: center;">Zhiyuan Wang</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27030">Two-Streams:Dark and Light Networks with Graph Convolution for Action Recognitionfrom Dark Videos (Student Abstract)</a></td><td style="text-align: center;">Saurabh Suman</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27020">FakeKG: AKnowledge Graph of Fake Claims for Improving Automated Fact-Checking(Student Abstract)</a></td><td style="text-align: center;">Gautam Kishore Shahi</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/27013">AProbabilistic Graph Diffusion Model for Source Localization (StudentAbstract)</a></td><td style="text-align: center;">Tangjiang Qian</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.10153">Sequential Graph AttentionLearning for Predicting Dynamic Stock Trends (Student Abstract)</a></td><td style="text-align: center;">Tzu-Ya Lai</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26955">Multi-ModalProtein Knowledge Graph Construction and Applications (StudentAbstract)</a></td><td style="text-align: center;">Siyuan Cheng</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26954">Self-PacedLearning Based Graph Convolutional Neural Network for Mixed IntegerProgramming (Student Abstract)</a></td><td style="text-align: center;">Li Chen</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26944">ModelSelection of Graph Signage Models Using Maximum Likelihood (StudentAbstract)</a></td><td style="text-align: center;">Angelina Brilliantova</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26870">End-to-EndPipeline for Trigger Detection on Hit and Track Graphs</a></td><td style="text-align: center;">Tingting Xuan</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.12378">Cosmic Microwave BackgroundRecovery: A Graph-Based Bayesian Convolutional Network Approach</a></td><td style="text-align: center;">Jadie Adams</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26776">RobustGraph Meta-Learning via Manifold Calibration with ProxySubgraphs</a></td><td style="text-align: center;">Zhenzhong Wang</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26761">Task andModel Agnostic Adversarial Attack on Graph Neural Networks</a></td><td style="text-align: center;">Kartik Sharma</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26711">A Crowd-AICollaborative Duo Relational Graph Learning Framework towards SocialImpact Aware Photo Classification</a></td><td style="text-align: center;">Yang Zhang</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26703">Privacy-PreservedEvolutionary Graph Modeling via Gromov-WassersteinAutoregression</a></td><td style="text-align: center;">Yue Xiang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26684">NeighborAuto-Grouping Graph Neural Networks for Handover Parameter Configurationin Cellular Network</a></td><td style="text-align: center;">Mehrtash Mehrabi</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26678">HumanMobility Modeling during the COVID-19 Pandemic via Deep Graph DiffusionInfomax</a></td><td style="text-align: center;">Yang Liu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26669">Spatio-TemporalGraph Neural Point Process for Traffic Congestion EventPrediction</a></td><td style="text-align: center;">Guangyin Jin</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26662">CriticalFirms Prediction for Stemming Contagion Risk in Networked-Loans throughGraph-Based Deep Reinforcement Learning</a></td><td style="text-align: center;">Dawei Cheng</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26623">A GraphFusion Approach for Cross-Lingual Machine Reading Comprehension</a></td><td style="text-align: center;">Zenan Xu</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26600">LatentConstraints on Unsupervised Text-Graph Alignment with InformationAsymmetry</a></td><td style="text-align: center;">Jidong Tian</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26591">ExploringFaithful Rationale for Multi-Hop Fact Verification via Salience-AwareGraph Learning</a></td><td style="text-align: center;">Jiasheng Si</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26533">ReviewingLabels: Label Graph Network with Top-k Prediction Set for RelationExtraction</a></td><td style="text-align: center;">Bo Li</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26455">Fair ShortPaths in Vertex-Colored Graphs</a></td><td style="text-align: center;">Matthias Bentert</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26437">Score-BasedLearning of Graphical Event Models with Background KnowledgeAugmentation</a></td><td style="text-align: center;">Debarun Bhattacharjya</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.02889">Networked Anti-coordinationGames Meet Graphical Dynamical Systems: Equilibria andConvergence</a></td><td style="text-align: center;">Zirou Qiu</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26343">AutoGraph:Optimizing DNN Computation Graph for Parallel GPU KernelExecution</a></td><td style="text-align: center;">Yuxuan Zhao</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26340">TensorizedIncomplete Multi-View Clustering with Intrinsic GraphCompletion</a></td><td style="text-align: center;">Shuping Zhao</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26336">SpectralFeature Augmentation for Graph Contrastive Learning and Beyond</a></td><td style="text-align: center;">Yifei Zhang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26319">ImGCL:Revisiting Graph Contrastive Learning on Imbalanced NodeClassification</a></td><td style="text-align: center;">Liang Zeng</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26300">PrioriAnchor Labels Supervised Scalable Multi-View Bipartite GraphClustering</a></td><td style="text-align: center;">Jiali You</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.05345">GOHSP: A Unified Framework ofGraph and Optimization-Based Heterogeneous Structured Pruning for VisionTransformer</a></td><td style="text-align: center;">Miao Yin</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26274">ReinforcementCausal Structure Learning on Order Graph</a></td><td style="text-align: center;">Dezhi Yang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26239">AdversarialWeight Perturbation Improves Generalization in Graph NeuralNetworks</a></td><td style="text-align: center;">Yihan Wu</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26188">MetricMulti-View Graph Clustering</a></td><td style="text-align: center;">Yuze Tan</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26152">InferringPatient Zero on Temporal Networks via Graph Neural Networks</a></td><td style="text-align: center;">Xiaolei Ru</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26105">MultiplexGraph Representation Learning via Common and Private InformationMining</a></td><td style="text-align: center;">Yujie Mo</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26152">InferringPatient Zero on Temporal Networks via Graph Neural Networks</a></td><td style="text-align: center;">Xiaolei Ru</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26105">MultiplexGraph Representation Learning via Common and Private InformationMining</a></td><td style="text-align: center;">Yujie Mo</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26034">Scaling UpDynamic Graph Representation Learning via Spiking NeuralNetworks</a></td><td style="text-align: center;">Jintang Li</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/26005">GroupingMatrix Based Graph Pooling with Adaptive Number of Clusters</a></td><td style="text-align: center;">Sung Moon Ko</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25979">Local-GlobalDefense against Unsupervised Adversarial Attacks on Graphs</a></td><td style="text-align: center;">Di Jin</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.01593">Multi-View MOOC QualityEvaluation via Information-Aware Graph Representation Learning</a></td><td style="text-align: center;">Lu Jiang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25960">Self-SupervisedGraph Attention Networks for Deep Weighted Multi-ViewClustering</a></td><td style="text-align: center;">Zongmo Huang</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25944">BoostingGraph Neural Networks via Adaptive Knowledge Distillation</a></td><td style="text-align: center;">Zhichun Guo</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25942">GraphKnows Unknowns: Reformulate Zero-Shot Learning as Sample-Level GraphRecognition</a></td><td style="text-align: center;">Jingcai Guo</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25941">InterpolatingGraph Pair to Regularize Graph Classification</a></td><td style="text-align: center;">Hongyu Guo</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25928">HandlingMissing Data via Max-Entropy Regularized Graph Autoencoder</a></td><td style="text-align: center;">Ziqi Gao</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25870">WienerGraph Deconvolutional Network Improves Graph Self-SupervisedLearning</a></td><td style="text-align: center;">Jiashun Cheng</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25831">LearnableSpectral Wavelets on Dynamic Graphs to Capture GlobalInteractions</a></td><td style="text-align: center;">Anson Bastos</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25824">GeneralizingDownsampling from Regular Data to Graphs</a></td><td style="text-align: center;">Davide Bacciu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25701">ScalableEdge Blocking Algorithms for Defending Active Directory Style AttackGraphs</a></td><td style="text-align: center;">Mingyu Guo</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25695">TightInapproximability for Graphical Games</a></td><td style="text-align: center;">Argyrios Deligkas</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25679">BiddingGraph Games with Partially-Observable Budgets</a></td><td style="text-align: center;">Guy Avni</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25645">GRIP:Graph Representation of Immune Repertoire Using Graph Neural Network andTransformer</a></td><td style="text-align: center;">Yongju Lee</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.12814">GraphSR: A Data AugmentationAlgorithm for Imbalanced Node Classification</a></td><td style="text-align: center;">Mengting Zhou</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25621">A ProvableFramework of Learning Graph Embeddings via Summarization</a></td><td style="text-align: center;">Houquan Zhou</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25615">Cross-DomainFew-Shot Graph Classification with a Reinforced TaskCoordinator</a></td><td style="text-align: center;">Qiannan Zhang</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.03266">Learning to Count Isomorphismswith Graph Neural Networks</a></td><td style="text-align: center;">Xingtong Yu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25594">AugmentingAffective Dependency Graph via Iterative Incongruity Graph Learning forSarcasm Detection</a></td><td style="text-align: center;">Xiaobao Wang</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25590">Easy BegunIs Half Done: Spatial-Temporal Graph Modeling with ST-CurriculumDropout</a></td><td style="text-align: center;">Hongjun Wang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25587">Self-OrganizationPreserved Graph Structure Learning with Principle of RelevantInformation</a></td><td style="text-align: center;">Qingyun Sun</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25586">Self-SupervisedContinual Graph Learning in Adaptive Riemannian Spaces</a></td><td style="text-align: center;">Li Sun</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.03881">On Generalized Degree Fairnessin Graph Neural Networks</a></td><td style="text-align: center;">Zemin Liu</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25569">Multi-DomainGeneralized Graph Meta Learning</a></td><td style="text-align: center;">Mingkai Lin</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25568">Scalableand Effective Conductance-Based Graph Clustering</a></td><td style="text-align: center;">Longlong Lin</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25565">SignedLaplacian Graph Neural Networks</a></td><td style="text-align: center;">Yu Li</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25559">GLCC: AGeneral Framework for Graph-Level Clustering</a></td><td style="text-align: center;">Wei Ju</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25553">T2-GNN:Graph Neural Networks for Graphs with Incomplete Features and Structurevia Teacher-Student Distillation</a></td><td style="text-align: center;">Cuiying Huo</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25545">DropMessage:Unifying Random Dropping for Graph Neural Networks</a></td><td style="text-align: center;">Taoran Fang</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25536">DualLow-Rank Graph Autoencoder for Semantic and TopologicalNetworks</a></td><td style="text-align: center;">Zhaoliang Chen</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25532">EnhancedMulti-Relationships Integration Graph Convolutional Network forInferring Substitutable and Complementary Items</a></td><td style="text-align: center;">Huajie Chen</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25506">Separatebut Equal: Equality in Belief Propagation for Single CycleGraphs</a></td><td style="text-align: center;">Erel Cohen</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.10574">MulGT: Multi-TaskGraph-Transformer with Task-Aware Knowledge Injection and DomainKnowledge-Driven Pooling for Whole Slide Image Analysis</a></td><td style="text-align: center;">Weiqin Zhao</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25387">SceneGraph to Image Synthesis via Knowledge Consensus</a></td><td style="text-align: center;">Yang Wu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25343">LeveragingWeighted Cross-Graph Attention for Visual and Semantic Enhanced VideoCaptioning Network</a></td><td style="text-align: center;">Deepali Verma</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25336">AsynchronousEvent Processing with Local-Shift Graph Convolutional Network</a></td><td style="text-align: center;">Linhui Sun</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25326">CLIPVG:Text-Guided Image Manipulation Using Differentiable VectorGraphics</a></td><td style="text-align: center;">Yiren Song</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25290">UniversePoints Representation Learning for Partial Multi-Graph Matching</a></td><td style="text-align: center;">Zhakshylyk Nurlanov</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/25208">SymbolicReplay: Scene Graph as Prompt for Continual Learning on VQATask</a></td><td style="text-align: center;">Stan Weixian Lei</td><td style="text-align: center;">none</td></tr></tbody></table><h4 id="neurlps">NeurlPS</h4><p>官方暂未发布。</p><h4 id="acl">ACL</h4><p>文章链接为包含摘要的网页，没有网页则直接链接 PDF 文件。</p><table><thead><tr class="header"><th>paper</th><th style="text-align: center;">authors</th><th style="text-align: center;">code</th></tr></thead><tbody><tr class="odd"><td><a href="https://arxiv.org/abs/2306.06872">History Semantic GraphEnhanced Conversational KBQA with Temporal Information Modeling</a></td><td style="text-align: center;">Hao Sun et al.</td><td style="text-align: center;"><ahref="https://amritasaha1812.github.io/CSQA">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.04170">From the One, Judge ofthe Whole: Typed Entailment Graph Construction with PredicateGeneration</a></td><td style="text-align: center;">Zhibin Chen et al.</td><td style="text-align: center;"><ahref="https://github.com/ZacharyChenpk/TP-EGG">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.16195">Enhancing DialogueGeneration via Dynamic Graph Knowledge Aggregation</a></td><td style="text-align: center;">Chen Tang et al.</td><td style="text-align: center;"><ahref="https://github.com/tangg555/SaBART">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.13585">Query Structure Modelingfor Inductive Logical Reasoning Over Knowledge Graphs</a></td><td style="text-align: center;">Siyuan Wang et al.</td><td style="text-align: center;"><ahref="https://github.com/SiyuanWangw/InductiveLR">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.17542">Non-Sequential GraphScript Induction via Multimedia Grounding</a></td><td style="text-align: center;">Yu Zhou et al.</td><td style="text-align: center;"><ahref="https://github.com/bryanzhou008/Multimodal-Graph-Script-Learning/">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.12256">Scene Graph as Pivoting:Inference-time Image-free Unsupervised Multimodal Machine Translationwith Visual Scene Hallucination</a></td><td style="text-align: center;">Hao Fei et al.</td><td style="text-align: center;"><ahref="https://github.com/scofield7419/UMMT-VSH">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.08495">Similarity-weightedConstruction of Contextualized Commonsense Knowledge Graphs forKnowledge-intense Argumentation Tasks</a></td><td style="text-align: center;">Moritz Plenz et al.</td><td style="text-align: center;"><ahref="https://github.com/Heidelberg-NLP/CCKG">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.14126">To Copy Rather ThanMemorize: A Vertical Learning Paradigm for Knowledge GraphCompletion</a></td><td style="text-align: center;">Rui Li et al.</td><td style="text-align: center;"><ahref="https://github.com/rui9812/VLP">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.384/">CompoundingGeometric Operations for Knowledge Graph Completion</a></td><td style="text-align: center;">Xiou Ge et al.</td><td style="text-align: center;"><ahref="https://github.com/hughxiouge/CompoundE">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.08859">Curriculum Learning forGraph Neural Networks: A Multiview Competence-based Approach</a></td><td style="text-align: center;">Nidhi Vakil et al.</td><td style="text-align: center;"><ahref="https://clu.cs.uml.edu/tools.html">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.408/">DualGATs: DualGraph Attention Networks for Emotion Recognition inConversations</a></td><td style="text-align: center;">Duzhen Zhang et al.</td><td style="text-align: center;"><ahref="https://github.com/BladeDancer957/DualGATs">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.06472">Modeling StructuralSimilarities between Documents for Coherence Assessment with GraphConvolutional Networks</a></td><td style="text-align: center;">Wei Liu et al.</td><td style="text-align: center;"><ahref="https://github.com/liuwei1206/StruSim">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.06588">HAHE: HierarchicalAttention for Hyper-Relational Knowledge Graphs in Global and LocalLevel</a></td><td style="text-align: center;">Haoran Luo et al.</td><td style="text-align: center;"><ahref="https://github.com/LHRLAB/HAHE">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.584/">JointDocument-Level Event Extraction via Token-Token Bidirectional EventCompleted Graph</a></td><td style="text-align: center;">Qizhi Wan et al.</td><td style="text-align: center;"><ahref="https://github.com/hawisdom/EDEE">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.586/">MultilingualKnowledge Graph Completion with Language-Sensitive Multi-GraphAttention</a></td><td style="text-align: center;">Rongchuan Tang et al.</td><td style="text-align: center;"><ahref="https://github.com/RongchuanTang/LSMGA-MKGC">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2208.09137">GreenKGC: A LightweightKnowledge Graph Completion Method</a></td><td style="text-align: center;">Yun-Cheng Wang et al.</td><td style="text-align: center;"><ahref="https://github.com/yunchengwang/GreenKGC">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.597/">Are MessagePassing Neural Networks Really Helpful for Knowledge GraphCompletion?</a></td><td style="text-align: center;">Juanhui Li et al.</td><td style="text-align: center;"><ahref="https://github.com/Juanhui28/Are_MPNNs_helpful">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.607/">A NovelTable-to-Graph Generation Approach for Document-Level Joint Entity andRelation Extraction</a></td><td style="text-align: center;">Ruoyu Zhang et al.</td><td style="text-align: center;"><ahref="https://github.com/ridiculouz/TaG">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.16650">Multi-source SemanticGraph-based Multimodal Sarcasm Explanation Generation</a></td><td style="text-align: center;">Liqiang Jing et al.</td><td style="text-align: center;"><ahref="https://github.com/LiqiangJing/TEAM">url</a></td></tr><tr class="even"><td><ahref="https://aclanthology.org/2023.acl-long.637/">Multi-granularityTemporal Question Answering over Knowledge Graphs</a></td><td style="text-align: center;">Ziyang Chen et al.</td><td style="text-align: center;"><ahref="https://github.com/czy1999/MultiTQ">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2307.00335">Single SequencePrediction over Reasoning Graphs for Multi-hop QA</a></td><td style="text-align: center;">Gowtham Ramesh et al.</td><td style="text-align: center;"><ahref="https://github.com/gowtham1997/SeqGraph">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.09360">GIFT: Graph-InducedFine-Tuning for Multi-Party Conversation Understanding</a></td><td style="text-align: center;">Jia-Chen Gu et al.</td><td style="text-align: center;"><ahref="https://github.com/JasonForJoy/MPC-BERT">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.02177">Transforming Visual SceneGraphs to Image Captions</a></td><td style="text-align: center;">Xu Yang et al.</td><td style="text-align: center;"><ahref="https://anonymous.4open.science/r/ACL23_TSG">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.765/">HolisticPrediction on a Time-Evolving Attributed Graph</a></td><td style="text-align: center;">Shohei Yamasaki et al.</td><td style="text-align: center;"><ahref="https://github.com/yuya-s/AGATE/">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.775/">DialogueSummarization with Static-Dynamic Structure Fusion Graph</a></td><td style="text-align: center;">Shen Gao et al.</td><td style="text-align: center;"><ahref="https://github.com/Hannibal046/SDDS">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.790/">Graph-basedRelation Mining for Context-free Out-of-vocabulary Word EmbeddingLearning</a></td><td style="text-align: center;">Ziran Liang et al.</td><td style="text-align: center;"><ahref="https://github.com/liangzrtvjivo/GRM">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.862/">TeAST:Temporal Knowledge Graph Embedding via Archimedean SpiralTimeline</a></td><td style="text-align: center;">Jiang Li et al.</td><td style="text-align: center;"><ahref="https://github.com/IMU-MachineLearningSXD/TeAST">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.892/">Evaluate AMRGraph Similarity via Self-supervised Learning</a></td><td style="text-align: center;">Ziyi Shou et al.</td><td style="text-align: center;"><ahref="https://github.com/zzshou/AMRSim">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.06590">FactKG: Fact Verificationvia Reasoning on Knowledge Graphs</a></td><td style="text-align: center;">Jiho Kim et al.</td><td style="text-align: center;"><ahref="https://github.com/jiho283/FactKG">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-short.11/">GraphPropagation based Data Augmentation for Named EntityRecognition</a></td><td style="text-align: center;">Jiong Cai</td><td style="text-align: center;"><ahref="https://github.com/modelscope/AdaSeq/tree/master/examples/GPDA">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.03319">HiPool: Modeling LongDocuments Using Graph Neural Networks</a></td><td style="text-align: center;">Irene Li</td><td style="text-align: center;"><ahref="https://github.com/IreneZihuiLi/HiPool">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-short.23/">SimpleAugmentations of Logical Rules for Neuro-Symbolic Knowledge GraphCompletion</a></td><td style="text-align: center;">Ananjan Nandi</td><td style="text-align: center;"><ahref="https://github.com/dair-iitd/NS-KGC-AUG">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.06395">ACTC: Active ThresholdCalibration for Cold-Start Knowledge Graph Completion</a></td><td style="text-align: center;">Anastasiia Sedova</td><td style="text-align: center;"><ahref="https://github.com/anasedova/ACTC">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.24/">A UnifiedKnowledge Graph Augmentation Service for Boosting Domain-specific NLPTasks</a></td><td style="text-align: center;">Ruiqing Ding</td><td style="text-align: center;"><ahref="https://github.com/RuiqingDing/KnowledgeDA">url</a></td></tr><tr class="odd"><td><ahref="https://www.pure.ed.ac.uk/ws/portalfiles/portal/378031141/Align_then_Enhance_WU_DOA02052023_VOR_CC_BY.pdf">Align-then-Enhance:Multilingual Entailment Graph Enhancement with Soft PredicateAlignment</a></td><td style="text-align: center;">Yuting Wu</td><td style="text-align: center;"><ahref="https://github.com/StephanieWyt/Align-then-Enhance">url</a></td></tr><tr class="even"><td><ahref="https://www.amazon.science/publications/folkscope-intention-knowledge-graph-construction-for-e-commerce-commonsense-discovery">FolkScope:Intention Knowledge Graph Construction for E-commerce CommonsenseDiscovery</a></td><td style="text-align: center;">Changlong Yu</td><td style="text-align: center;"><ahref="https://github.com/HKUSTKnowComp/FolkScope">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.13467">Incorporating GraphInformation in Transformer-based AMR Parsing</a></td><td style="text-align: center;">Pavlo Vasylenko</td><td style="text-align: center;"><ahref="http://www.github.com/sapienzanlp/LeakDistill">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.06574">A FusedGromov-Wasserstein Framework for Unsupervised Knowledge Graph EntityAlignment</a></td><td style="text-align: center;">Jianheng Tang</td><td style="text-align: center;"><ahref="https://github.com/squareRoot3/FusedGW-Entity-Alignment">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.309/">BertNet:Harvesting Knowledge Graphs with Arbitrary Relations from PretrainedLanguage Models</a></td><td style="text-align: center;">Shibo Hao</td><td style="text-align: center;"><ahref="https://github.com/tanyuqian/knowledge-harvest-from-lms">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17219">GVdoc - Graph-basedVisual DOcument Classification</a></td><td style="text-align: center;">Fnu Mohbat</td><td style="text-align: center;"><ahref="https://github.com/mohbattharani/GVdoc">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.358/">EnhancingHierarchical Text Classification through Knowledge GraphIntegration</a></td><td style="text-align: center;">Ye Liu</td><td style="text-align: center;"><ahref="https://github.com/liuyeah/K-HTC">url</a></td></tr><tr class="even"><td><ahref="https://aclanthology.org/2023.findings-acl.391/">Structure-DiscourseHierarchical Graph for Conditional Question Answering on LongDocuments</a></td><td style="text-align: center;">Haowei Du</td><td style="text-align: center;"><ahref="https://github.com/yanmenxue/ConditionalQA">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.17497">FACTUAL: A Benchmark forFaithful and Consistent Textual Scene Graph Parsing</a></td><td style="text-align: center;">Zhuang Li</td><td style="text-align: center;"><ahref="https://github.com/zhuang-li/FACTUAL">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.407/">GuidingDialogue Agents to Complex Semantic Targets by Dynamically CompletingKnowledge Graph</a></td><td style="text-align: center;">Yue Tan</td><td style="text-align: center;"><ahref="https://github.com/tanyue2019/ACL-P">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.448/">KnowledgeGraph Embeddings using Neural Ito Process: From Multiple Walks toStochastic Trajectories</a></td><td style="text-align: center;">Mojtaba Nayyeri</td><td style="text-align: center;"><ahref="https://github.com/ColdMist/ItoE">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.450/">Predictionand Calibration: Complex Reasoning over Knowledge Graph withBi-directional Directed Acyclic Graph Neural Network</a></td><td style="text-align: center;">Yao Xu</td><td style="text-align: center;"><ahref="https://github.com/YaooXu/BiDAG">url</a></td></tr><tr class="odd"><td><ahref="https://aclanthology.org/2023.findings-acl.488/">MultilingualKnowledge Graph Completion from Pretrained Language Models withKnowledge Constraints</a></td><td style="text-align: center;">Ran Song</td><td style="text-align: center;"><ahref="https://github.com/Maxpa1n/gcplm-kgc">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.18675">History repeats:Overcoming catastrophic forgetting for event-centric temporal knowledgegraph completion</a></td><td style="text-align: center;">Mehrnoosh Mirtaheri</td><td style="text-align: center;"><ahref="https://github.com/INK-USC/RE-Net">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.07912">Pre-trained LanguageModel with Prompts for Temporal Knowledge Graph Completion</a></td><td style="text-align: center;">Wenjie Xu</td><td style="text-align: center;"><ahref="https://github.com/JaySaligia/PPT">url</a></td></tr><tr class="even"><td><ahref="https://aclanthology.org/2023.findings-acl.536/">ConstructingProcedural Graphs with Multiple Dependency Relations: A New Dataset andBaseline</a></td><td style="text-align: center;">Haopeng Ren</td><td style="text-align: center;"><ahref="https://github.com/betterAndTogether/WHPG">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.553/">RHGN:Relation-gated Heterogeneous Graph Network for Entity Alignment inKnowledge Graphs</a></td><td style="text-align: center;">Xukai Liu</td><td style="text-align: center;"><ahref="https://github.com/laquabe/RGHN">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.00652">Explanation GraphGeneration via Generative Pre-training over Synthetic Graphs</a></td><td style="text-align: center;">Han Cui</td><td style="text-align: center;"><ahref="https://github.com/cccccent/EG3P">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.15597">Text Augmented OpenKnowledge Graph Completion via Pre-Trained Language Models</a></td><td style="text-align: center;">Pengcheng Jiang</td><td style="text-align: center;"><ahref="https://github.com/pat-jj/TagReal">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.01709">Dipping PLMs Sauce:Bridging Structure and Text for Effective Knowledge Graph Completion viaConditional Soft Prompting</a></td><td style="text-align: center;">Chen Chen</td><td style="text-align: center;"><ahref="https://github.com/chenchens190009/CSProm-KG">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.827/">ProbingGraph Decomposition for Argument Pair Extraction</a></td><td style="text-align: center;">Yang Sun</td><td style="text-align: center;"><ahref="https://github.com/syiswell/PIGEON">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.02235">AttenWalker: UnsupervisedLong-Document Question Answering via Attention-based GraphWalking</a></td><td style="text-align: center;">Yuxiang Nie</td><td style="text-align: center;"><ahref="https://github.com/JerrryNie/Unsupervised-Long-Document-QA">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.871/">ImprovingLong Dialogue Summarization with Semantic Graph Representation</a></td><td style="text-align: center;">Yilun Hua</td><td style="text-align: center;"><ahref="https://github.com/Bobby-Hua/summarization-via-semantic-graph">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17019">Commonsense KnowledgeGraph Completion Via Contrastive Pretraining and NodeClustering</a></td><td style="text-align: center;">Siwei Wu</td><td style="text-align: center;"><ahref="https://github.com/NUSTM/CPNC">url</a></td></tr><tr class="odd"><td><ahref="https://aclanthology.org/2023.findings-acl.900/">ContrastiveLearning with Generated Representations for Inductive Knowledge GraphEmbedding</a></td><td style="text-align: center;">Qian Li</td><td style="text-align: center;"><ahref="https://github.com/feiwangyuzhou/VMCL">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-demo.13/">DIAGRAPH: AnOpen-Source Graphic Interface for Dialog Flow Design</a></td><td style="text-align: center;">Dirk Väth</td><td style="text-align: center;"><ahref="https://github.com/DigitalPhonetics/diagraph">url</a></td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-industry.2/">"Knowledgeis Power": Constructing Knowledge Graph of Abdominal Organs and UsingThem for Automatic Radiology Report Generation</a></td><td style="text-align: center;">Kaveri Kale</td><td style="text-align: center;"><ahref="https://github.com/kaverikale/RadiologyKGConstruction">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.17652">CocaCLIP: ExploringDistillation of Fully-Connected Knowledge Interaction Graph forLightweight Text-Image Retrieval</a></td><td style="text-align: center;">Jiapeng Wang</td><td style="text-align: center;"><ahref="https://github.com/alibaba/EasyNLP">url</a></td></tr><tr class="odd"><td><ahref="https://aclanthology.org/2023.acl-industry.16/">FashionKLIP:Enhancing E-Commerce Image-Text Retrieval with Fashion Multi-ModalConceptual Knowledge Graph</a></td><td style="text-align: center;">Xiaodan Wang</td><td style="text-align: center;"><ahref="https://github.com/alibaba/EasyNLP">url</a></td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-industry.14/">AVEN-GR:Attribute Value Extraction and Normalization using productGRaphs</a></td><td style="text-align: center;">Thomas Ricatte</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.655/">EnhancingEvent Causality Identification with Event Causal Label and Event PairInteraction Graph</a></td><td style="text-align: center;">Ruili Pu</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.01933">Concept2Box: JointGeometric Embeddings for Learning Two-View Knowledge Graphs</a></td><td style="text-align: center;">Zijie Huang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.575/">Learningto Leverage High-Order Medical Knowledge Graph for Joint Entity andRelation Extraction</a></td><td style="text-align: center;">Zhe Yang</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.458/">TuckerDecomposition with Frequency Attention for Temporal Knowledge GraphCompletion</a></td><td style="text-align: center;">Likang Xiao</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.455/">Few-shotLow-resource Knowledge Graph Completion with Reinforced TaskGeneration</a></td><td style="text-align: center;">Shichao Pei</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.362/">ImprovingKnowledge Graph Completion with Generative Hard Negative Mining</a></td><td style="text-align: center;">Zile Qiao</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2210.14456">Question-InterlocutorScope Realized Graph Modeling over Key Utterances for Dialogue ReadingComprehension</a></td><td style="text-align: center;">Jiangnan Li</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2306.12018">A Semi-AutoregressiveGraph Generative Model for Dependency Graph Parsing</a></td><td style="text-align: center;">Ye Ma</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.09173">Unsupervised Task GraphGeneration from Instructional Video Transcripts</a></td><td style="text-align: center;">Lajanugen Logeswaran</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.18742">Graph Reasoning forQuestion Answering with Triplet Retrieval</a></td><td style="text-align: center;">Shiyang Li</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.07512">Noisy Positive-UnlabeledLearning with Self-Training for Speculative Knowledge GraphReasoning</a></td><td style="text-align: center;">Ruijie Wang</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.findings-acl.28/">LearningJoint Structural and Temporal Contextualized Knowledge Embeddings forTemporal Knowledge Graph Completion</a></td><td style="text-align: center;">Yifu Gao</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.findings-acl.23/">G³R: AGraph-Guided Generate-and-Rerank Framework for Complex and Cross-domainText-to-SQL Generation</a></td><td style="text-align: center;">Yanzheng Xiang</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.850/">Double-BranchMulti-Attention based Graph Neural Network for Knowledge GraphCompletion</a></td><td style="text-align: center;">Hongcai Xu et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.15872">Jointprop: JointSemi-supervised Learning for Entity and Relation Extraction withHeterogeneous Graph-based Propagation</a></td><td style="text-align: center;">Yandan Zheng et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.785/">DynamicHeterogeneous-Graph Reasoning with Language Models and KnowledgeRepresentation Learning for Commonsense Question Answering</a></td><td style="text-align: center;">Yujie Wang et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.705/">LearningLatent Relations for Temporal Knowledge Graph Reasoning</a></td><td style="text-align: center;">Mengqi Zhang et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2208.08320">BIC: Twitter BotDetection with Text-Graph Interaction and Semantic Consistency</a></td><td style="text-align: center;">Zhenyu Lei et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2305.12416">Direct Fact Retrievalfrom Knowledge Graphs without Entity Linking</a></td><td style="text-align: center;">Jinheon Baek et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2305.02549">FormNetV2: MultimodalGraph Contrastive Learning for Form Document InformationExtraction</a></td><td style="text-align: center;">Chen-Yu Lee et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><a href="https://aclanthology.org/2023.acl-long.281/">UnsupervisedGraph-Text Mutual Conversion with a Unified Pretrained LanguageModel</a></td><td style="text-align: center;">Yi Xu et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td><a href="https://aclanthology.org/2023.acl-long.71/">TECHS: TemporalLogical Graph Networks for Explainable Extrapolation Reasoning</a></td><td style="text-align: center;">Qika Lin et al.</td><td style="text-align: center;">none</td></tr></tbody></table><h4 id="cvpr">CVPR</h4><table><thead><tr class="header"><th>paper</th><th style="text-align: center;">authors</th><th style="text-align: center;">code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_VectorFloorSeg_Two-Stream_Graph_Attention_Network_for_Vectorized_Roughcast_Floorplan_Segmentation_CVPR_2023_paper.html">VectorFloorSeg:Two-Stream Graph Attention Network for Vectorized Roughcast FloorplanSegmentation</a></td><td style="text-align: center;">Bingchen Yang</td><td style="text-align: center;"><ahref="https://github.com/DrZiji/VecFloorSeg">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Firoze_Tree_Instance_Segmentation_With_Temporal_Contour_Graph_CVPR_2023_paper.html">TreeInstance Segmentation with Temporal Contour Graph</a></td><td style="text-align: center;">Adnan Firoze</td><td style="text-align: center;"><ahref="https://github.com/adnan0819/Tree-InstanceSegmentation-using-Temporal-Structured-Images">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_Improving_Commonsense_in_Vision-Language_Models_via_Knowledge_Graph_Riddles_CVPR_2023_paper.html">ImprovingCommonsense in Vision-Language Models via Knowledge GraphRiddles</a></td><td style="text-align: center;">Shuquan Ye</td><td style="text-align: center;"><ahref="https://github.com/pleaseconnectwifi/DANCE">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Multi-Granularity_Archaeological_Dating_of_Chinese_Bronze_Dings_Based_on_a_CVPR_2023_paper.html">Multi-GranularityArchaeological Dating of Chinese Bronze Dings Based on aKnowledge-Guided Relation Graph</a></td><td style="text-align: center;">Rixin Zhou</td><td style="text-align: center;"><ahref="https://github.com/zhourixin/bronze-Ding">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Dynamic_Graph_Enhanced_Contrastive_Learning_for_Chest_X-Ray_Report_Generation_CVPR_2023_paper.html">DynamicGraph Enhanced Contrastive Learning for Chest X-Ray ReportGeneration</a></td><td style="text-align: center;">Mingjie Li</td><td style="text-align: center;"><ahref="https://github.com/mlii0117/DCL">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/VS_Instance_Relation_Graph_Guided_Source-Free_Domain_Adaptive_Object_Detection_CVPR_2023_paper.html">InstanceRelation Graph Guided Source-Free Domain Adaptive ObjectDetection</a></td><td style="text-align: center;">Vibashan VS</td><td style="text-align: center;"><ahref="https://viudomain.github.io/irg-sfda-web/">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Incremental_3D_Semantic_Scene_Graph_Prediction_From_RGB_Sequences_CVPR_2023_paper.html">Incremental3D Semantic Scene Graph Prediction from RGB Sequences</a></td><td style="text-align: center;">Shun-Cheng Wu</td><td style="text-align: center;"><ahref="https://shunchengwu.github.io/MonoSSG">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_Deep_Graph-Based_Spatial_Consistency_for_Robust_Non-Rigid_Point_Cloud_Registration_CVPR_2023_paper.html">DeepGraph-based Spatial Consistency for Robust Non-rigid Point CloudRegistration</a></td><td style="text-align: center;">Zheng Qin</td><td style="text-align: center;"><ahref="https://github.com/qinzheng93/GraphSCNet">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wei_Adaptive_Graph_Convolutional_Subspace_Clustering_CVPR_2023_paper.html">AdaptiveGraph Convolutional Subspace Clustering</a></td><td style="text-align: center;">Lai Wei</td><td style="text-align: center;"><ahref="https://github.com/weilyshmtu/AGCSC">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Deep_Learning_of_Partial_Graph_Matching_via_Differentiable_Top-K_CVPR_2023_paper.html">DeepLearning of Partial Graph Matching via Differentiable Top-K</a></td><td style="text-align: center;">Runzhong Wang</td><td style="text-align: center;"><ahref="https://github.com/Thinklab-SJTU/ThinkMatch">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Kundu_IS-GGT_Iterative_Scene_Graph_Generation_With_Generative_Transformers_CVPR_2023_paper.html">IS-GGT:Iterative Scene Graph Generation with Generative Transformers</a></td><td style="text-align: center;">Sanjoy Kundu</td><td style="text-align: center;"><ahref="https://saakur.github.io/Projects/IS_GGT/">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_A_Unified_Knowledge_Distillation_Framework_for_Deep_Directed_Graphical_Models_CVPR_2023_paper.html">AUnified Knowledge Distillation Framework for Deep Directed GraphicalModels</a></td><td style="text-align: center;">Yizhuo Chen</td><td style="text-align: center;"><ahref="https://github.com/YizhuoChen99/KD4DGM-CVPR">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_From_Node_Interaction_To_Hop_Interaction_New_Effective_and_Scalable_CVPR_2023_paper.html">FromNode Interaction to Hop Interaction: New Effective and Scalable GraphLearning Paradigm</a></td><td style="text-align: center;">Jie Chen</td><td style="text-align: center;"><ahref="https://github.com/JC-202/HopGNN">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_3D_Spatial_Multimodal_Knowledge_Accumulation_for_Scene_Graph_Prediction_in_CVPR_2023_paper.html">3DSpatial Multimodal Knowledge Accumulation for Scene Graph Prediction inPoint Cloud</a></td><td style="text-align: center;">Mingtao Feng</td><td style="text-align: center;"><ahref="https://github.com/HHrEtvP/SMKA">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Robust_Multiview_Point_Cloud_Registration_With_Reliable_Pose_Graph_Initialization_CVPR_2023_paper.html">RobustMultiview Point Cloud Registration with Reliable Pose GraphInitialization and History Reweighting</a></td><td style="text-align: center;">Haiping Wang</td><td style="text-align: center;"><ahref="https://github.com/WHUUSI3DV/SGHR">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Unsupervised_Visible-Infrared_Person_Re-Identification_via_Progressive_Graph_Matching_and_Alternate_CVPR_2023_paper.html">UnsupervisedVisible-Infrared Person Re-Identification via Progressive Graph Matchingand Alternate Learning</a></td><td style="text-align: center;">Zesen Wu</td><td style="text-align: center;"><ahref="https://github.com/zesenwu23/USL-VI-ReID">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Biswas_Probabilistic_Debiasing_of_Scene_Graphs_CVPR_2023_paper.html">ProbabilisticDebiasing of Scene Graphs</a></td><td style="text-align: center;">Bashirul Azam Biswas</td><td style="text-align: center;"><ahref="https://github.com/bashirulazam/within-triplet-debias">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.html">Multivariate,Multi-Frequency and Multimodal: Rethinking Graph Neural Networks forEmotion Recognition in Conversation</a></td><td style="text-align: center;">Feiyu Chen</td><td style="text-align: center;"><ahref="https://github.com/feiyuchen7/M3NET">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2023_paper.html">Mindthe Label Shift of Augmentation-based Graph OOD Generalization</a></td><td style="text-align: center;">Junchi Yu</td><td style="text-align: center;"><ahref="https://github.com/Samyu0304/LiSA">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Buchner_Learning_and_Aggregating_Lane_Graphs_for_Urban_Automated_Driving_CVPR_2023_paper.html">Learningand Aggregating Lane Graphs for Urban Automated Driving</a></td><td style="text-align: center;">Martin Büchner</td><td style="text-align: center;"><ahref="http://urbanlanegraph.cs.uni-freiburg.de">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Rowe_FJMP_Factorized_Joint_Multi-Agent_Motion_Prediction_Over_Learned_Directed_Acyclic_CVPR_2023_paper.html">FJMP:Factorized Joint Multi-Agent Motion Prediction over Learned DirectedAcyclic Interaction Graphs</a></td><td style="text-align: center;">Luke Rowe</td><td style="text-align: center;"><ahref="https://rluke22.github.io/FJMP">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Urooj_Learning_Situation_Hyper-Graphs_for_Video_Question_Answering_CVPR_2023_paper.html">LearningSituation Hyper-Graphs for Video Question Answering</a></td><td style="text-align: center;">Aisha Urooj</td><td style="text-align: center;"><ahref="https://github.com/aurooj/SHGVQA">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.html">HistopathologyWhole Slide Image Analysis with Heterogeneous Graph RepresentationLearning</a></td><td style="text-align: center;">Tsai Hor Chan</td><td style="text-align: center;"><ahref="https://github.com/HKU-MedAI/WSI-HGNN">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Luan_High_Fidelity_3D_Hand_Shape_Reconstruction_via_Scalable_Graph_Frequency_CVPR_2023_paper.html">HighFidelity 3D Hand Shape Reconstruction via Scalable Graph FrequencyDecomposition</a></td><td style="text-align: center;">Tianyu Luan</td><td style="text-align: center;"><ahref="https://github.com/tyluann/FreqHand">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Grigorev_HOOD_Hierarchical_Graphs_for_Generalized_Modelling_of_Clothing_Dynamics_CVPR_2023_paper.html">HOOD:Hierarchical Graphs for Generalized Modelling of ClothingDynamics</a></td><td style="text-align: center;">Artur Grigorev</td><td style="text-align: center;"><ahref="https://dolorousrtur.github.io/hood/">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Jung_Devils_on_the_Edges_Selective_Quad_Attention_for_Scene_Graph_CVPR_2023_paper.html">Devil'son the Edges: Selective Quad Attention for Scene GraphGeneration</a></td><td style="text-align: center;">Deunsol Jung</td><td style="text-align: center;"><ahref="https://github.com/hesedjds/SQUAT">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.html">PanopticVideo Scene Graph Generation</a></td><td style="text-align: center;">Jingkang Yang</td><td style="text-align: center;"><ahref="https://github.com/LilyDaytoy/OpenPVSG">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_VL-SAT_Visual-Linguistic_Semantics_Assisted_Training_for_3D_Semantic_Scene_Graph_CVPR_2023_paper.html">VL-SAT:Visual-Linguistic Semantics Assisted Training for 3D Semantic SceneGraph Prediction in Point Cloud</a></td><td style="text-align: center;">Ziqin Wang</td><td style="text-align: center;"><ahref="https://github.com/wz7in/CVPR2023-VLSAT">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Roetzer_Conjugate_Product_Graphs_for_Globally_Optimal_2D-3D_Shape_Matching_CVPR_2023_paper.html">ConjugateProduct Graphs for Globally Optimal 2D-3D Shape Matching</a></td><td style="text-align: center;">Paul Roetzer</td><td style="text-align: center;"><ahref="https://github.com/paul0noah/sm-2D3D">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Rao_TranSG_Transformer-Based_Skeleton_Graph_Prototype_Contrastive_Learning_With_Structure-Trajectory_Prompted_CVPR_2023_paper.html">TranSG:Transformer-Based Skeleton Graph Prototype Contrastive Learning withStructure-Trajectory Prompted Reconstruction for PersonRe-Identification</a></td><td style="text-align: center;">Haocong Rao</td><td style="text-align: center;"><ahref="https://github.com/KaliHac/TranSG">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Eisenberger_G-MSM_Unsupervised_Multi-Shape_Matching_With_Graph-Based_Affinity_Priors_CVPR_2023_paper.html">G-MSM:Unsupervised Multi-Shape Matching with Graph-Based AffinityPriors</a></td><td style="text-align: center;">Marvin Eisenberger</td><td style="text-align: center;"><ahref="https://github.com/marvin-eisenberger/gmsm-matching">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.html">Prototype-BasedEmbedding Network for Scene Graph Generation</a></td><td style="text-align: center;">Chaofan Zheng</td><td style="text-align: center;"><ahref="https://github.com/VL-Group/PENET">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Nag_Unbiased_Scene_Graph_Generation_in_Videos_CVPR_2023_paper.html">UnbiasedScene Graph Generation in Videos</a></td><td style="text-align: center;">Sayak Nag</td><td style="text-align: center;"><ahref="https://github.com/sayaknag/unbiasedSGG">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Cetintas_Unifying_Short_and_Long-Term_Tracking_With_Graph_Hierarchies_CVPR_2023_paper.html">UnifyingShort and Long-Term Tracking with Graph Hierarchies</a></td><td style="text-align: center;">Orcun Cetintas</td><td style="text-align: center;"><ahref="https://github.com/dvl-tum/SUSHI">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Transductive_Few-Shot_Learning_With_Prototype-Based_Label_Propagation_by_Iterative_Graph_CVPR_2023_paper.html">TransductiveFew-Shot Learning with Prototype-Based Label Propagation by IterativeGraph Refinement</a></td><td style="text-align: center;">Hao Zhu</td><td style="text-align: center;"><ahref="https://github.com/allenhaozhu/protoLP">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/WAD/html/Fent_RadarGNN_Transformation_Invariant_Graph_Neural_Network_for_Radar-Based_Perception_CVPRW_2023_paper.html">RadarGNN:Transformation Invariant Graph Neural Network for Radar-basedPerception</a></td><td style="text-align: center;">Felix Fent</td><td style="text-align: center;"><ahref="https://github.com/TUMFTM/RadarGNN">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/WMF/html/Xiang_MTN_Forensic_Analysis_of_MP4_Video_Files_Using_Graph_Neural_CVPRW_2023_paper.html">MTN:Forensic Analysis of MP4 Video Files Using Graph NeuralNetworks</a></td><td style="text-align: center;">Ziyue Xiang</td><td style="text-align: center;"><ahref="https://gitlab.com/viper-purdue/mtn">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/MobileAI/html/Munir_MobileViG_Graph-Based_Sparse_Attention_for_Mobile_Vision_Applications_CVPRW_2023_paper.html">MobileViG:Graph-Based Sparse Attention for Mobile Vision Applications</a></td><td style="text-align: center;">Mustafa Munir</td><td style="text-align: center;"><ahref="https://github.com/SLDGroup/MobileViG">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Kachole_Asynchronous_Events-Based_Panoptic_Segmentation_Using_Graph_Mixer_Neural_Network_CVPRW_2023_paper.html">AsynchronousEvents-based Panoptic Segmentation using Graph Mixer NeuralNetwork</a></td><td style="text-align: center;">Sanket Kachole</td><td style="text-align: center;"><ahref="https://github.com/sanket0707/GNN-Mixer">url</a></td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Wang_Spatial-Temporal_Graph-Based_AU_Relationship_Learning_for_Facial_Action_Unit_Detection_CVPRW_2023_paper.html">Spatial-TemporalGraph-Based AU Relationship Learning for Facial Action UnitDetection</a></td><td style="text-align: center;">Zihan Wang</td><td style="text-align: center;"><ahref="https://github.com/wzh125/ABAW-5">url</a></td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/html/Nejatishahidin_Graph-CoVis_GNN-Based_Multi-View_Panorama_Global_Pose_Estimation_CVPRW_2023_paper.htmlv">Graph-CoVis:GNN-based Multi-view Panorama Global Pose Estimation</a></td><td style="text-align: center;">Negar Nejatishahidin</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/ABAW/html/Kumar_Relational_Edge-Node_Graph_Attention_Network_for_Classification_of_Micro-Expressions_CVPRW_2023_paper.html">RelationalEdge-Node Graph Attention Network for Classification ofMicro-Expressions</a></td><td style="text-align: center;">Ankith Jain Rakesh Kumar</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/html/Shiba_Zero-Shot_Object_Classification_With_Large-Scale_Knowledge_Graph_CVPRW_2023_paper.html">Zero-shotObject Classification with Large-scale Knowledge Graph</a></td><td style="text-align: center;">Kohei Shiba</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/ECV/html/Sahbi_Phase-Field_Models_for_Lightweight_Graph_Convolutional_Networks_CVPRW_2023_paper.html">Phase-fieldModels for Lightweight Graph Convolutional Networks</a></td><td style="text-align: center;">Hichem Sahbi</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Dalgaty_HUGNet_Hemi-Spherical_Update_Graph_Neural_Network_Applied_to_Low-Latency_Event-Based_CVPRW_2023_paper.html">HUGNet:Hemi-Spherical Update Graph Neural Network applied to low-latencyevent-based optical flow</a></td><td style="text-align: center;">Thomas Dalgaty</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/DL-UIA/html/Somani_Image_Inpainting_With_Hypergraphs_for_Resolution_Improvement_in_Scanning_Acoustic_CVPRW_2023_paper.html">ImageInpainting With Hypergraphs for Resolution Improvement in ScanningAcoustic Microscopy</a></td><td style="text-align: center;">Ayush Somani</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Shukla_Scene_Graph_Driven_Text-Prompt_Generation_for_Image_Inpainting_CVPRW_2023_paper.html">SceneGraph Driven Text-Prompt Generation for Image Inpainting</a></td><td style="text-align: center;">Tripti Shukla</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/TAG-PRA/html/Liu_Hamming_Similarity_and_Graph_Laplacians_for_Class_Partitioning_and_Adversarial_CVPRW_2023_paper.html">HammingSimilarity and Graph Laplacians for Class Partitioning and AdversarialImage Detection</a></td><td style="text-align: center;">Huma Jamil</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Rubio_Wildlife_Image_Generation_From_Scene_Graphs_CVPRW_2023_paper.html">WildlifeImage Generation from Scene Graphs</a></td><td style="text-align: center;">Yoshio Rubio</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Jing_Deep_Graph_Reprogramming_CVPR_2023_paper.html">DeepGraph Reprogramming</a></td><td style="text-align: center;">Yongcheng Jing</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Sample-Level_Multi-View_Graph_Clustering_CVPR_2023_paper.html">Sample-levelMulti-view Graph Clustering</a></td><td style="text-align: center;">Yuze Tan</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_CIGAR_Cross-Modality_Graph_Reasoning_for_Domain_Adaptive_Object_Detection_CVPR_2023_paper.html">CIGAR:Cross-Modality Graph Reasoning for Domain Adaptive ObjectDetection</a></td><td style="text-align: center;">Yabo Liu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Qiu_Graph_Representation_for_Order-Aware_Visual_Transformation_CVPR_2023_paper.html">GraphRepresentation for Order-aware Visual Transformation</a></td><td style="text-align: center;">Yue Qiu</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Graphics_Capsule_Learning_Hierarchical_3D_Face_Representations_From_2D_Images_CVPR_2023_paper.html">GraphicsCapsule: Learning Hierarchical 3D Face Representations from 2DImages</a></td><td style="text-align: center;">Chang Yu</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Gu_Text_With_Knowledge_Graph_Augmented_Transformer_for_Video_Captioning_CVPR_2023_paper.html">TextWith Knowledge Graph Augmented Transformer for Video Captioning</a></td><td style="text-align: center;">Xin Gu</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_LayoutFormer_Conditional_Graphic_Layout_Generation_via_Constraint_Serialization_and_Decoding_CVPR_2023_paper.html">LayoutFormer++:Conditional Graphic Layout Generation via Constraint Serialization andDecoding Space Restriction</a></td><td style="text-align: center;">Zhaoyun Jiang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Park_ViPLO_Vision_Transformer_Based_Pose-Conditioned_Self-Loop_Graph_for_Human-Object_Interaction_CVPR_2023_paper.html">ViPLO:Vision Transformer Based Pose-Conditioned Self-Loop Graph forHuman-Object Interaction Detection</a></td><td style="text-align: center;">Jeeseung Park</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Turning_Strengths_Into_Weaknesses_A_Certified_Robustness_Inspired_Attack_Framework_CVPR_2023_paper.html">TurningStrengths into Weaknesses: A Certified Robustness Inspired AttackFramework against Graph Neural Networks</a></td><td style="text-align: center;">Binghui Wang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wen_Highly_Confident_Local_Structure_Based_Consensus_Graph_Learning_for_Incomplete_CVPR_2023_paper.html">HighlyConfident Local Structure Based Consensus Graph Learning for IncompleteMulti-view Clustering</a></td><td style="text-align: center;">Jie Wen</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Nakhli_Sparse_Multi-Modal_Graph_Transformer_With_Shared-Context_Processing_for_Representation_Learning_CVPR_2023_paper.html">SparseMulti-Modal Graph Transformer with Shared-Context Processing forRepresentation Learning of Giga-pixel Images</a></td><td style="text-align: center;">Ramin Nakhli</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Cao_SVGformer_Representation_Learning_for_Continuous_Vector_Graphics_Using_Transformers_CVPR_2023_paper.html">SVGformer:Representation Learning for Continuous Vector Graphics usingTransformers</a></td><td style="text-align: center;">Defu Cao</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_Adversarially_Robust_Neural_Architecture_Search_for_Graph_Neural_Networks_CVPR_2023_paper.html">AdversariallyRobust Neural Architecture Search for Graph Neural Networks</a></td><td style="text-align: center;">Beini Xie</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Dynamic_Graph_Learning_With_Content-Guided_Spatial-Frequency_Relation_Reasoning_for_Deepfake_CVPR_2023_paper.html">DynamicGraph Learning with Content-guided Spatial-Frequency Relation Reasoningfor Deepfake Detection</a></td><td style="text-align: center;">Yuan Wang</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_An_Actor-Centric_Causality_Graph_for_Asynchronous_Temporal_Inference_in_Group_CVPR_2023_paper.html">AnActor-centric Causality Graph for Asynchronous Temporal Inference inGroup Activity</a></td><td style="text-align: center;">Zhao Xie</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Fast_Contextual_Scene_Graph_Generation_With_Unbiased_Context_Augmentation_CVPR_2023_paper.html">FastContextual Scene Graph Generation with Unbiased ContextAugmentation</a></td><td style="text-align: center;">Tianlei Jin</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_Robust_3D_Shape_Classification_via_Non-Local_Graph_Attention_Network_CVPR_2023_paper.html">Robust3D Shape Classification via Non-local Graph Attention Network</a></td><td style="text-align: center;">Shengwei Qin</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_To_Generate_Language-Supervised_and_Open-Vocabulary_Scene_Graph_Using_Pre-Trained_CVPR_2023_paper.html">Learningto Generate Language-Supervised and Open-Vocabulary Scene Graph UsingPre-Trained Visual-Semantic Space</a></td><td style="text-align: center;">Yong Zhang</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Tang_Graph_Transformer_GANs_for_Graph-Constrained_House_Generation_CVPR_2023_paper.html">GraphTransformer GANs for Graph-Constrained House Generation</a></td><td style="text-align: center;">Hao Tang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Worchel_Differentiable_Shadow_Mapping_for_Efficient_Inverse_Graphics_CVPR_2023_paper.html">DifferentiableShadow Mapping for Efficient Inverse Graphics</a></td><td style="text-align: center;">Markus Worchel</td><td style="text-align: center;">none</td></tr><tr class="even"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Improving_Graph_Representation_for_Point_Cloud_Segmentation_via_Attentive_Filtering_CVPR_2023_paper.html">ImprovingGraph Representation for Point Cloud Segmentation via AttentiveFiltering</a></td><td style="text-align: center;">Nan Zhang</td><td style="text-align: center;">none</td></tr><tr class="odd"><td><ahref="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_TopDiG_Class-Agnostic_Topological_Directional_Graph_Extraction_From_Remote_Sensing_Images_CVPR_2023_paper.html">TopDiG:Class-agnostic Topological Directional Graph Extraction from RemoteSensing Images</a></td><td style="text-align: center;">Bingnan Yang</td><td style="text-align: center;">none</td></tr></tbody></table><h4 id="icml">ICML</h4><p>文章链接为 PDF 地址。</p><table><thead><tr class="header"><th style="text-align: left;">paper</th><th style="text-align: center;">authors</th><th style="text-align: center;">code</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.12159.pdf">ClusterFuG: Clustering Fullyconnected Graphs by Multicut</a></td><td style="text-align: center;">A. Abbas et al.</td><td style="text-align: center;"><ahref="https://github.com/aabbas90/cluster-fug">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=lXczFIwQkv">Half-Hop: A graphupsampling approach for slowing down message passing</a></td><td style="text-align: center;">M. Azabou et al.</td><td style="text-align: center;"><ahref="https://github.com/nerdslab/halfhop">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=KTJ6E8t9Cy">Answering ComplexLogical Queries on Knowledge Graphs via Query Computation TreeOptimization</a></td><td style="text-align: center;">Y. Bai et al.</td><td style="text-align: center;"><ahref="https://github.com/bys0318/QTO">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=Q8k4WzGgnK">Implicit Graph NeuralNetworks: A Monotone Operator Viewpoint</a></td><td style="text-align: center;">J. Baker et al.</td><td style="text-align: center;"><ahref="https://github.com/Utah-Math-Data-Science/MIGNN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/behmanesh23a/behmanesh23a.pdf">TIDE:Time Derivative Diffusion for Deep Learning on Graphs</a></td><td style="text-align: center;">M. Behmanesh et al.</td><td style="text-align: center;"><ahref="https://github.com/maysambehmanesh/TIDE">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/bihani23a/bihani23a.pdf">StriderNet:A Graph Reinforcement Learning Approach to Optimize Atomic Structures onRough Energy Landscapes</a></td><td style="text-align: center;">V Bihani et al.</td><td style="text-align: center;"><ahref="https://github.com/M3RG-IITD/StriderNET">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.11956.pdf">On the Connection BetweenMPNN and Graph Transformer</a></td><td style="text-align: center;">C Cai et al.</td><td style="text-align: center;"><ahref="https://github.com/Chen-Cai-OSU/MPNN-GT-Connection">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=2Mbo7IEtZW">Efficient Learning ofMesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph NeuralNetwork</a></td><td style="text-align: center;">Y Cao et al.</td><td style="text-align: center;"><ahref="https://github.com/Eydcao/BSMS-GNN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.18951.pdf">Subequivariant GraphReinforcement Learning in 3D Environments</a></td><td style="text-align: center;">R Chen et al.</td><td style="text-align: center;"><ahref="https://alpc91.github.io/SGRL/">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.04111.pdf">Efficient and Degree-GuidedGraph Generation via Discrete Diffusion Modeling</a></td><td style="text-align: center;">X Chen et al.</td><td style="text-align: center;"><ahref="https://github.com/tufts-ml/graph-generation-EDGE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.07580.pdf">Fisher Information Embeddingfor Node and Graph Learning</a></td><td style="text-align: center;">D Chen et al.</td><td style="text-align: center;"><ahref="https://github.com/BorgwardtLab/fisher_information_embedding">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.08854.pdf">A Gromov-WassersteinGeometric View of Spectrum-Preserving Graph Coarsening</a></td><td style="text-align: center;">Y Chen et al.</td><td style="text-align: center;"><ahref="https:%20//github.com/ychen-stat-ml/GW-Graph-Coarsening">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=LMay53U4ke">GREAD: Graph NeuralReaction-Diffusion Networks</a></td><td style="text-align: center;">J Choi et al.</td><td style="text-align: center;"><ahref="https://github.com/jeongwhanchoi/GREAD">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/choromanski23a/choromanski23a.pdf">Taminggraph kernels with random features</a></td><td style="text-align: center;">KM Choromanski et al.</td><td style="text-align: center;"><ahref="https://github.com/vlivashkin/community-graphs">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/choromanski23b/choromanski23b.pdf">EfficientGraph Field Integrators Meet Point Clouds</a></td><td style="text-align: center;">KM Choromanski et al.</td><td style="text-align: center;"><ahref="https://github.com/topographers/efficient_graph_algorithms">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=Z1I4WrV5TG">Wasserstein BarycenterMatching for Graph Size Generalization of Message Passing NeuralNetworks</a></td><td style="text-align: center;">X Chu et al.</td><td style="text-align: center;"><ahref="https://github.com/JinYujie99/WBM">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/diamant23a/diamant23a.pdf">ImprovingGraph Generation by Restricting Graph Bandwidth</a></td><td style="text-align: center;">NL Diamant et al.</td><td style="text-align: center;"><ahref="https://github.com/Genentech/bandwidth-graph-generation">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=wO6ExWRO3c">Towards Understandingand Reducing Graph Structural Noise for GNNs</a></td><td style="text-align: center;">M Dong et al.</td><td style="text-align: center;"><ahref="https://github.com/MingzeDong/ESNR">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.09129.pdf">Graph Reinforcement Learningfor Network Control via Bi-Level Optimization</a></td><td style="text-align: center;">D Gammelli et al.</td><td style="text-align: center;"><ahref="https://github.com/DanieleGammelli/graph-rl-for-network-optimization">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2302.12432.pdf">Graph Neural Networks withLearnable and Optimal Polynomial Bases</a></td><td style="text-align: center;">Y Guo et al.</td><td style="text-align: center;"><ahref="https://github.com/yuziGuo/FarOptBasis">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/gupta23b/gupta23b.pdf">GRAFENNE:Learning on Graphs with Heterogeneous and Dynamic Feature Sets</a></td><td style="text-align: center;">S Gupta et al.</td><td style="text-align: center;"><ahref="https://github.com/data-iitd/Grafenne">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=zGf8J0bNfX">Conformal PredictionSets for Graph Neural Networks</a></td><td style="text-align: center;">SH Zargarbashi et al.</td><td style="text-align: center;"><ahref="https://github.com/soroushzargar/DAPS">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/han23c/han23c.pdf">AlternatelyOptimized Graph Neural Networks</a></td><td style="text-align: center;">H Han et al.</td><td style="text-align: center;"><ahref="https://github.com/haoyuhan1/ALT-OPT/">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=PRexNmId50">Total Variation GraphNeural Networks</a></td><td style="text-align: center;">JB Hansen et al.</td><td style="text-align: center;"><ahref="https://github.com/FilippoMB/Total-variation-graph-neural-networks">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/he23a/he23a.pdf">AGeneralization of ViT/MLP-Mixer to Graphs</a></td><td style="text-align: center;">X He et al.</td><td style="text-align: center;"><ahref="https://github.com/XiaoxinHe/Graph-ViT-MLPMixer">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/jaiswal23a/jaiswal23a.pdf">GraphLadling: Shockingly Simple Parallel GNN Training without IntermediateCommunication</a></td><td style="text-align: center;">AK Jaiswal et al.</td><td style="text-align: center;"><ahref="https://github.com/VITA-Group/graph_ladling">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.00139.pdf">Leveraging LabelNon-Uniformity for Node Classification in Graph Neural Networks</a></td><td style="text-align: center;">F Ji et al.</td><td style="text-align: center;"><ahref="http://github.com/amblee0306/label-non-uniformity-gnn">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.09308.pdf">On the Expressive Power ofGeometric Graph Neural Networks</a></td><td style="text-align: center;">CK Joshi et al.</td><td style="text-align: center;"><ahref="https://github.com/chaitjo/geometric-gnn-dojo">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.18965.pdf">Node Embedding from NeuralHamiltonian Orbits in Graph Neural Networks</a></td><td style="text-align: center;">Q Kang et al.</td><td style="text-align: center;"><ahref="https://github.com/zknus/Hamiltonian-GNN">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=Le2dVIoQun">GOAT: A GlobalTransformer on Large-scale Graphs</a></td><td style="text-align: center;">K Kong et al.</td><td style="text-align: center;"><ahref="https://github.com/devnkong/GOAT">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.04004.pdf">Randomized Schur ComplementViews for Graph Contrastive Learning</a></td><td style="text-align: center;">V Kothapalli et al.</td><td style="text-align: center;"><ahref="https://github.com/kvignesh1420/rlap">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/kurenkov23a/kurenkov23a.pdf">ModelingDynamic Environments with Scene Graph Memory</a></td><td style="text-align: center;">A Kurenkov et al.</td><td style="text-align: center;"><ahref="https://github.com/andreykurenkov/modeling_env_dynamics">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/laenen23a/laenen23a.pdf">Nearly-OptimalHierarchical Clustering for Well-Clustered Graphs</a></td><td style="text-align: center;">S Laenen et al.</td><td style="text-align: center;"><ahref="https://github.com/steinarlaenen/nearlyoptimal-hierarchical-clustering-for-well-clustered-graphs">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.02376.pdf">Towards Deep Attention inGraph Neural Networks: Problems and Remedies</a></td><td style="text-align: center;">SY Lee et al.</td><td style="text-align: center;"><ahref="https://github.com/syleeheal/AERO-GNN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.19987.pdf">InGram: Inductive KnowledgeGraph Embedding via Relation Graphs</a></td><td style="text-align: center;">J Lee et al.</td><td style="text-align: center;"><ahref="https://github.com/bdi-lab/InGram">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.01520.pdf">Conditional GraphInformation Bottleneck for Molecular Relational Learning</a></td><td style="text-align: center;">N Lee et al.</td><td style="text-align: center;"><ahref="https://github.com/Namkyeong/CGIB">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=szL4HN4hkH">Local Vertex ColouringGraph Neural Networks</a></td><td style="text-align: center;">S Li et al.</td><td style="text-align: center;"><ahref="https://github.com/seanli3/lvc">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.00015.pdf">GraphCleaner: DetectingMislabelled Samples in Popular Graph Learning Benchmarks</a></td><td style="text-align: center;">Y Li et al.</td><td style="text-align: center;"><ahref="https://github.com/lywww/GraphCleaner/tree/master">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.09595.pdf">Structured CooperativeLearning with Graphical Model Priors</a></td><td style="text-align: center;">S Li et al.</td><td style="text-align: center;"><ahref="https://github.com/ShuangtongLi/SCooL">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2306.06788">Graph Mixup with SoftAlignments</a></td><td style="text-align: center;">H Ling et al.</td><td style="text-align: center;"><ahref="https://github.com/divelab/DIG">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/ling23b.html">Deep GraphRepresentation Learning and Optimization for InfluenceMaximization</a></td><td style="text-align: center;">C Ling et al.</td><td style="text-align: center;"><ahref="https://github.com/triplej0079/DeepIM">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/liu23u/liu23u.pdf">StructuralRe-weighting Improves Graph Domain Adaptation</a></td><td style="text-align: center;">S Liu et al.</td><td style="text-align: center;"><ahref="https://github.com/Graph-COM/StruRW">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.18405.pdf">Dink-Net: Neural Clusteringon Large Graphs</a></td><td style="text-align: center;">Y Liu et al.</td><td style="text-align: center;"><ahref="https://github.com/yueliu1999/Dink-Net">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2306.00370">Graph Switching DynamicalSystems</a></td><td style="text-align: center;">Y Liu et al.</td><td style="text-align: center;"><ahref="https://github.com/yongtuoliu/Graph-SwitchingDynamical-Systems">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/liu23ad/liu23ad.pdf">RSC:Accelerate Graph Neural Networks Training via Randomized SparseComputations</a></td><td style="text-align: center;">Z Liu et al.</td><td style="text-align: center;"><ahref="https://github.com/warai-0toko/RSC-ICML">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.17589.pdf">Graph Inductive Biases inTransformers without Message Passing</a></td><td style="text-align: center;">L Ma et al.</td><td style="text-align: center;"><ahref="https://github.com/LiamMa/GRIT">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2306.07758">Generated GraphDetection</a></td><td style="text-align: center;">Y Ma et al.</td><td style="text-align: center;"><ahref="https://github.com/Yvonnemamama/GGD">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2303.05786.pdf">Vertical Federated GraphNeural Network for Recommender System</a></td><td style="text-align: center;">P Mai et al.</td><td style="text-align: center;"><ahref="https://github.com/maiph123/VerticalGNN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.07858.pdf">Additive Causal Bandits withUnknown Graph</a></td><td style="text-align: center;">A Malek et al.</td><td style="text-align: center;"><ahref="https://github.com/deepmind/additive_cb">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/michel23a/michel23a.pdf">PathNeural Networks: Expressive and Accurate Graph Neural Networks</a></td><td style="text-align: center;">G Michel et al.</td><td style="text-align: center;"><ahref="https://github.com/gasmichel/PathNNs_expressive">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=lYZOjMvxws">Disentangled MultiplexGraph Representation Learning</a></td><td style="text-align: center;">Y Mo et al.</td><td style="text-align: center;"><ahref="https://github.com/YujieMo/DMG">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.02931.pdf">Beyond Homophily:Reconstructing Structure for Graph-agnostic Clustering</a></td><td style="text-align: center;">E Pan et al.</td><td style="text-align: center;"><ahref="https://github.com/Panern/DGCN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/rustamov23a/rustamov23a.pdf">IntrinsicSliced Wasserstein Distances for Comparing Collections of ProbabilityDistributions on Manifolds and Graphs</a></td><td style="text-align: center;">RM Rustamov et al.</td><td style="text-align: center;"><ahref="https://github.com/shubhobm/isw">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.08617.pdf">Multi-class Graph Clusteringvia Approximated Effective p-Resistance</a></td><td style="text-align: center;">S Saito et al.</td><td style="text-align: center;"><ahref="https://github.com/ShotaSAITO/approximated-presistance">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.10956.pdf">Graph Neural Networks canRecover the Hidden Features Solely from the Graph Structure</a></td><td style="text-align: center;">R Sato et al.</td><td style="text-align: center;"><ahref="https://github.com/joisino/gnnrecover">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=9L6f6Y7GUS">One-Shot Compression ofLarge Edge-Exchangeable Graphs using Bits-Back Coding</a></td><td style="text-align: center;">D Severo et al.</td><td style="text-align: center;"><ahref="https://github.com/dsevero/Random-Edge-Coding">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2303.06147.pdf">Exphormer: SparseTransformers for Graphs</a></td><td style="text-align: center;">H Shirzad et al.</td><td style="text-align: center;"><ahref="https://github.com/hamed1375/Exphormer">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=OcKwZhPwHA">RGE: A Repulsive GraphRectification for Node Classification via Influence</a></td><td style="text-align: center;">J Song et al.</td><td style="text-align: center;"><ahref="https://github.com/Jaeyun-Song/RGE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=DdQ8ewIgY9">Towards Robust GraphIncremental Learning on Evolving Graphs</a></td><td style="text-align: center;">J Su et al.</td><td style="text-align: center;"><ahref="https://github.com/littleTown93/NGIL_Evolve">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.02622.pdf">What Makes Entities Similar?A Similarity Flooding Perspective for Multi-sourced Knowledge GraphEmbeddings</a></td><td style="text-align: center;">Z Sun et al.</td><td style="text-align: center;"><ahref="https://github.com/nju-websoft/Unify-EA-SF">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=AHWSZhGXbs">All in a Row: CompressedConvolution Networks for Graphs</a></td><td style="text-align: center;">J Sun et al.</td><td style="text-align: center;"><ahref="https://github.com/sunjss/CoCN">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/sun23p/sun23p.pdf">FeatureExpansion for Graph Neural Networks</a></td><td style="text-align: center;">J Sun et al.</td><td style="text-align: center;"><ahref="https://github.com/sajqavril/Feature-Extension-Graph-Neural-Networks">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.11378.pdf">MG-GNN: Multigrid GraphNeural Networks for Learning Multilevel Domain DecompositionMethods</a></td><td style="text-align: center;">A Taghibakhshi et al.</td><td style="text-align: center;"><ahref="https://github.com/JRD971000/Code-Multilevel-MLORAS/">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.00152.pdf">Learning the Right Layers aData-Driven Layer-Aggregation Strategy for Semi-Supervised Learning onMultilayer Graphs</a></td><td style="text-align: center;">S Venturini et al.</td><td style="text-align: center;"><ahref="https://github.com/saraventurini/Learning-the-right-layers-on-multilayer-graphs">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.17284.pdf">GC-Flow: A Graph-Based FlowNetwork for Effective Clustering</a></td><td style="text-align: center;">Tianchun Wang et al.</td><td style="text-align: center;"><ahref="https://github.com/xztcwang/GCFlow">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.12383.pdf">On Heterogeneous TreatmentEffects in Heterogeneous Causal Graphs</a></td><td style="text-align: center;">Richard A Watson et al.</td><td style="text-align: center;"><ahref="https://github.com/richard-watson/ISL">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/wei23c/wei23c.pdf">BoostingGraph Contrastive Learning via Graph Contrastive Saliency</a></td><td style="text-align: center;">Chunyu Wei et al.</td><td style="text-align: center;"><ahref="https://github.com/weicy15/GCS">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/weilbach23a/weilbach23a.pdf">GraphicallyStructured Diffusion Models</a></td><td style="text-align: center;">Christian Dietrich Weilbach et al.</td><td style="text-align: center;"><ahref="https://github.com/plai-group/gsdm">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.05838.pdf">Expectation-Complete GraphRepresentations with Homomorphisms</a></td><td style="text-align: center;">Pascal Welke et al.</td><td style="text-align: center;"><ahref="https://github.com/pwelke/homcount">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.04501.pdf">SEGA: Structural EntropyGuided Anchor View for Graph Contrastive Learning</a></td><td style="text-align: center;">Junran Wu et al.</td><td style="text-align: center;"><ahref="https://github.com/Wu-Junran/SEGA">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=MocsSAUKlk">Rethinking ExplainingGraph Neural Networks via Non-parametric Subgraph Matching</a></td><td style="text-align: center;">Fang Wu et al.</td><td style="text-align: center;"><ahref="https://github.com/smiles724/MatchExplainer">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=BDYIci7bVs">Relevant Walk Search forExplaining Graph Neural Networks</a></td><td style="text-align: center;">Ping Xiong et al.</td><td style="text-align: center;"><ahref="https://github.com/xiong-ping/rel_walk_gnnlrp">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2302.01503.pdf">LazyGNN: Large-Scale GraphNeural Networks via Lazy Propagation</a></td><td style="text-align: center;">Rui Xue et al.</td><td style="text-align: center;"><ahref="https://github.com/RXPHD/Lazy_GNN">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.06102.pdf">Towards Better GraphRepresentation Learning with Parameterized Decomposition &amp;Filtering</a></td><td style="text-align: center;">Mingqi Yang et al.</td><td style="text-align: center;"><ahref="https://github.com/qslim/PDF">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2301.13187">Weighted Flow Diffusion forLocal Graph Clustering with Node Attributes: an Algorithm andStatistical Guarantees</a></td><td style="text-align: center;">Shenghao Yang et al.</td><td style="text-align: center;"><ahref="https://github.com/s-h-yang/WFD">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.19666.pdf">Efficient Algorithms forExact Graph Matching on Correlated Stochastic Block Models with ConstantCorrelation</a></td><td style="text-align: center;">Joonhyuk Yang et al.</td><td style="text-align: center;"><ahref="https://github.com/cabaksa/cSBM_Matching">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=33fj5Ph3ot">Personalized FederatedLearning with Inferred Collaboration Graphs</a></td><td style="text-align: center;">Rui Ye et al.</td><td style="text-align: center;"><ahref="https://github.com/MediaBrainSJTU/pFedGraph">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/ying23a/ying23a.pdf">AdaptiveEstimation of Graphical Models under Total Positivity</a></td><td style="text-align: center;">Jiaxi Ying et al.</td><td style="text-align: center;"><ahref="https://github.com/jxying/ddmtp">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=SpA7YFu02k">Graph Generative Modelfor Benchmarking Graph Neural Networks</a></td><td style="text-align: center;">Minji Yoon et al.</td><td style="text-align: center;"><ahref="https://github.com/minjiyoon/CGT">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/yu23h/yu23h.pdf">SeedGNN: GraphNeural Network for Supervised Seeded Graph Matching</a></td><td style="text-align: center;">Liren Yu et al.</td><td style="text-align: center;"><ahref="https://github.com/Leron33/SeedGNN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.04922.pdf">Efficient and EquivariantGraph Networks for Predicting Quantum Hamiltonian</a></td><td style="text-align: center;">Haiyang Yu et al.</td><td style="text-align: center;"><ahref="https://github.com/divelab/AIRS">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=vTSyiXwoPK">Generative GraphDictionary Learning</a></td><td style="text-align: center;">Zhichen Zeng et al.</td><td style="text-align: center;"><ahref="https://github.com/zhichenz98/FraMe-ICML23">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/zhang23p/zhang23p.pdf">Spatial-TemporalGraph Learning with Adversarial Contrastive Adaptation</a></td><td style="text-align: center;">Qianru Zhang et al.</td><td style="text-align: center;"><ahref="https://github.com/HKUDS/GraphST">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.07608.pdf">Finding the Missing-half:Graph Complementary Learning for Homophily-prone and Heterophily-proneGraphs</a></td><td style="text-align: center;">Yizhen Zheng et al.</td><td style="text-align: center;"><ahref="https://github.com/zyzisastudyreallyhardguy/GOALGraph-Complementary-Learning">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=G6L1kwy9AA">SlotGAT: Slot-basedMessage Passing for Heterogeneous Graphs</a></td><td style="text-align: center;">Ziang Zhou et al.</td><td style="text-align: center;"><ahref="https://github.com/scottjiao/SlotGAT_ICML23/">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/zhou23k/zhou23k.pdf">FastOnline Node Labeling for Very Large Graphs</a></td><td style="text-align: center;">Baojian Zhou et al.</td><td style="text-align: center;"><ahref="https://github.com/baojian/FastONL">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.09104.pdf">On Strengthening andDefending Graph Reconstruction Attack with Markov ChainApproximation</a></td><td style="text-align: center;">Zhanke Zhou et al.</td><td style="text-align: center;"><ahref="https://github.com/tmlr-group/MC-GRA">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.04963.pdf">From Relational Pooling toSubgraph GNNs: A Universal Framework for More Expressive Graph NeuralNetworks</a></td><td style="text-align: center;">Cai Zhou et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/zhang23aq/zhang23aq.pdf">DemystifyingUneven Vulnerability of Link Stealing Attacks against Graph NeuralNetworks</a></td><td style="text-align: center;">He Zhang et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/zhang23o/zhang23o.pdf">WhenSparsity Meets Contrastive Models: Less Graph Data Can Bring BetterClass-Balanced Representations</a></td><td style="text-align: center;">Chunhui Zhang et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=BfVkbfJGW4">Graph ContrastiveBackdoor Attacks</a></td><td style="text-align: center;">Hangfan Zhang et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.04979.pdf">CoCo: A Coupled ContrastiveFramework for Unsupervised Domain Adaptive Graph Classification</a></td><td style="text-align: center;">Nan Yin et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=mLOWz0e1Yq">Quantum 3D GraphLearning with Applications to Molecule Embedding</a></td><td style="text-align: center;">Ge Yan et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/xu23w/xu23w.pdf">Do Not TrainIt: A Linear Neural Architecture Search of Graph NeuralNetworks</a></td><td style="text-align: center;">Peng Xu et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2304.11153.pdf">Low-Variance GradientEstimation in Unrolled Computation Graphs with ES-Single</a></td><td style="text-align: center;">P Vicol et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.08048.pdf">Towards UnderstandingGeneralization of Graph Neural Networks</a></td><td style="text-align: center;">H Tang et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.07971.pdf">Tight and fastgeneralization error bound of graph embedding in metric space</a></td><td style="text-align: center;">A Suzuki et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=eXtJRDCGye">Causal Bounds inQuasi-Markovian Graphs</a></td><td style="text-align: center;">M Shridharan et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/puny23a/puny23a.pdf">EquivariantPolynomials for Graph Neural Networks</a></td><td style="text-align: center;">O Puny et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.11419.pdf">Efficiently predicting highresolution mass spectra with graph neural networks</a></td><td style="text-align: center;">M Murphy et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=9iChKP4k32">HOPE: High-order GraphODE For Modeling Interacting Dynamics</a></td><td style="text-align: center;">X Luo et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/li23y/li23y.pdf">On theInitialization of Graph Neural Networks</a></td><td style="text-align: center;">J Li et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=wVGreJ2338">Featured GraphCoarsening with Similarity Guarantees</a></td><td style="text-align: center;">M Kumar et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.10808.pdf">Graph Neural Tangent Kernel:Convergence on Large Graphs</a></td><td style="text-align: center;">S Krishnagopal et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2303.04791.pdf">Ewald-based Long-RangeMessage Passing for Molecular Graphs</a></td><td style="text-align: center;">A Kosmala et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.05097.pdf">Self-Repellent Random Walkson General Graphs - Achieving Minimal Sampling Variance via NonlinearMarkov Chains</a></td><td style="text-align: center;">V Doshi et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2303.02918.pdf">Graph Positional Encodingvia Random Feature Propagation</a></td><td style="text-align: center;">M Eliasof et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=5jFy5MQvUj">Improving Graph NeuralNetworks with Learnable Propagation Operators</a></td><td style="text-align: center;">M Eliasof et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://proceedings.mlr.press/v202/geisler23a/geisler23a.pdf">TransformersMeet Directed Graphs</a></td><td style="text-align: center;">S Geisler et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.02971.pdf">Online Learning withFeedback Graphs: The True Shape of Regret</a></td><td style="text-align: center;">T Kocák et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=KVmFqS1cMP">Structure Learning ofLatent Factors via Clique Search on Correlation ThresholdedGraphs</a></td><td style="text-align: center;">D Kim et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=dtg76BRYCG">Autoregressive DiffusionModel for Graph Generation</a></td><td style="text-align: center;">L Kong et al.</td><td style="text-align: center;">none</td></tr></tbody></table><h4 id="ijcai">IJCAI</h4><p>文章链接为 PDF 地址。</p><table style="width:100%;"><thead><tr class="header"><th style="text-align: left;">paper</th><th style="text-align: center;">authors</th><th style="text-align: center;">code</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0192.pdf">Learning ObjectConsistency and Interaction in Image Generation from SceneGraphs</a></td><td style="text-align: center;">Y. Zhang et al.</td><td style="text-align: center;"><ahref="https://github.com/yangkzz/LOCI">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.04658.pdf">CSGCL:Community-Strength-Enhanced Graph Contrastive Learning</a></td><td style="text-align: center;">H. Chen et al.</td><td style="text-align: center;"><ahref="https://github.com/HanChen-HUST/CSGCL">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2304.12604.pdf">Adaptive Path-Memory Networkfor Temporal Knowledge Graph Reasoning</a></td><td style="text-align: center;">H. Dong et al.</td><td style="text-align: center;"><ahref="https://github.com/hhdo/DaeMon">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://ijcai.org/proceedings/2023/0234.pdf">Beyond Homophily:Robust Graph Anomaly Detection via Neural Sparsification</a></td><td style="text-align: center;">Z. Gong et al.</td><td style="text-align: center;"><ahref="https://github.com/KellyGong/SparseGAD">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.03756.pdf">Continuous-Time GraphLearning for Cascade Popularity Prediction</a></td><td style="text-align: center;">X. Lu et al.</td><td style="text-align: center;"><ahref="https://github.com/lxd99/CTCP">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.03555.pdf">Contrastive Graph Clusteringin Curvature Spaces</a></td><td style="text-align: center;">L. Sun et al.</td><td style="text-align: center;"><ahref="https://github.com/CurvCluster/Congregate">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.13573.pdf">SAD: Semi-Supervised AnomalyDetection on Dynamic Graphs</a></td><td style="text-align: center;">S. Tian et al.</td><td style="text-align: center;"><ahref="https://github.com/D10Andy/SAD">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0259.pdf">ACanonicalization-Enhanced Known Fact-Aware Framework For Open KnowledgeGraph Link Prediction</a></td><td style="text-align: center;">Y. Wang et al.</td><td style="text-align: center;"><ahref="https://github.com/wylResearch/CEKFA">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.08676.pdf">An Ensemble Approach forAutomated Theorem Proving Based on Efficient Name Invariant Graph NeuralRepresentations</a></td><td style="text-align: center;">A. Fokoue et al.</td><td style="text-align: center;"><ahref="https://github.com/ibm/TRAIL">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.04225.pdf">LSGNN: Towards General GraphNeural Network in Node Classification by Local Similarity</a></td><td style="text-align: center;">Y. Chen et al.</td><td style="text-align: center;"><ahref="https://github.com/draym28/LSGNN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.11424.pdf">Graph PropagationTransformer for Graph Representation Learning</a></td><td style="text-align: center;">Z. Chen et al.</td><td style="text-align: center;"><ahref="https://github.com/czczup/GPTrans">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.09729.pdf">FedHGN: A FederatedFramework for Heterogeneous Graph Neural Networks</a></td><td style="text-align: center;">X. Fu et al.</td><td style="text-align: center;"><ahref="https://github.com/cynricfu/FedHGN">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.lamda.nju.edu.cn/publication/ijcai23ablkg.pdf">EnablingAbductive Learning to Exploit Knowledge Graph</a></td><td style="text-align: center;">YX. Huang et al.</td><td style="text-align: center;"><ahref="https://github.com/AbductiveLearning/ABL-KG">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://creddy.net/papers/IJCAI23.pdf">A Unification Framework forEuclidean and Hyperbolic Graph Neural Networks</a></td><td style="text-align: center;">M. Khatir et al.</td><td style="text-align: center;"><ahref="https://github.com/oom-debugger/ijcai23">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0466.pdf">Graph-basedSemi-supervised Local Clustering with Few Labeled Nodes</a></td><td style="text-align: center;">Z. Shen et al.</td><td style="text-align: center;"><ahref="https://github.com/zzzzms/LocalClustering">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2208.10227.pdf">One Model, Any CSP: GraphNeural Networks as Fast Global Search Heuristics for ConstraintSatisfaction</a></td><td style="text-align: center;">J. Tönshoff et al.</td><td style="text-align: center;"><ahref="https://github.com/toenshoff/ANYCSP">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0495.pdf">Violin: VirtualOverbridge Linking for Enhancing Semi-supervised Learning on Graphs withLimited Labels</a></td><td style="text-align: center;">S. Xie et al.</td><td style="text-align: center;"><ahref="https://github.com/xslangley/violin">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0501.pdf">LGI-GT: GraphTransformers with Local and Global Operators Interleaving</a></td><td style="text-align: center;">S. Yin et al.</td><td style="text-align: center;"><ahref="https://github.com/shuoyinn/LGI-GT">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2308.05463.pdf">G2Pxy: Generative Open-SetNode Classification on Graphs with Proxy Unknowns</a></td><td style="text-align: center;">Q. Zhang et al.</td><td style="text-align: center;"><ahref="https://github.com/ejfomxhue2o3239djnwkk/G2Pxy">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.16780.pdf">Graph NeuralConvection-Diffusion with Heterophily</a></td><td style="text-align: center;">K. Zhao et al.</td><td style="text-align: center;"><ahref="https://github.com/zknus/Graph-Diffusion-CDE">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.02866.pdf">Hierarchical Transformer forScalable Graph Learning</a></td><td style="text-align: center;">W. Zhu et al.</td><td style="text-align: center;"><ahref="https://drive.google.com/file/d/1HI5HYRdrv-B-o6HSECpwLxs9vq6DsjPo/view?usp=share_link">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.16780.pdf">Graph Sampling-basedMeta-Learning for Molecular Property Prediction</a></td><td style="text-align: center;">X Zhuang</td><td style="text-align: center;"><ahref="https://github.com/HICAI-ZJU/GS-Meta">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0540.pdf">Toward ConvexManifolds: A Geometric Perspective for Deep Graph Clustering ofSingle-cell RNA-seq Data</a></td><td style="text-align: center;">N. Mrabah et al.</td><td style="text-align: center;"><ahref="https://github.com/MMAMAR/scTConvexMan">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.08457.pdf">MolHF: A HierarchicalNormalizing Flow for Molecular Graph Generatio</a></td><td style="text-align: center;">Y. Zhu et al.</td><td style="text-align: center;"><ahref="https://github.com/violet-sto/MolHF">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2307.14928.pdf">Graph-based PolyphonicMultitrack Music Generation</a></td><td style="text-align: center;">E. Cosenza et al.</td><td style="text-align: center;"><ahref="https://github.com/EmanueleCosenza/polyphemus">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0675.pdf">PreventingAttacks in Interbank Credit Rating with Selective-aware Graph NeuralNetwork</a></td><td style="text-align: center;">J. Liu et al.</td><td style="text-align: center;"><ahref="https://github.com/finint/interbank">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0681.pdf">Fighting againstOrganized Fraudsters Using Risk Diffusion-based Parallel Graph NeuralNetwork</a></td><td style="text-align: center;">J. Ma et al.</td><td style="text-align: center;"><ahref="https://github.com/finint/antifraud">url</a></td></tr><tr class="even"><td style="text-align: left;"><ahref="https://dl.acm.org/doi/abs/10.14778/3538598.3538614">Sancus:Staleness-Aware Communication-Avoiding Full-Graph Decentralized Trainingin Large-Scale Graph Neural Networks (Extended Abstract)</a></td><td style="text-align: center;">J. Peng et al.</td><td style="text-align: center;"><ahref="https://github.com/chenzhao/light-dist-gnn">url</a></td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2302.02926">Curriculum Graph MachineLearning: A Survey</a></td><td style="text-align: center;">Haoyang Li et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2201.08236.pdf">Temporal Knowledge GraphCompletion: A Survey</a></td><td style="text-align: center;">B. Cai et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2302.01859.pdf">Generalizing to UnseenElements: A Survey on Knowledge Extrapolation for KnowledgeGraphs</a></td><td style="text-align: center;">M. Chen et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2302.02591.pdf">Generative diffusion modelson graphs: Methods and applications</a></td><td style="text-align: center;">W. Fan et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2204.07321">Graph Pooling for Graph NeuralNetworks: Progress, Challenges, and Opportunities</a></td><td style="text-align: center;">Chuang Liu et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2301.10503.pdf">In Which Graph StructuresCan We Efficiently Find Temporally Disjoint Paths and Walks?</a></td><td style="text-align: center;">P. Kunz et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0024.pdf">Deep HierarchicalCommunication Graph in Multi-Agent Reinforcement Learning</a></td><td style="text-align: center;">Z. Liu et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0131.pdf">GTR: AGrafting-Then-Reassembling Framework for Dynamic Scene GraphGeneration</a></td><td style="text-align: center;">J. Liang et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0244.pdf">Gapformer: GraphTransformer with Graph Pooling for Node Classification</a></td><td style="text-align: center;">C. Liu et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0249.pdf">Capturing theLong-Distance Dependency in the Control Flow Graph via Structural-GuidedAttention for Bug Localization</a></td><td style="text-align: center;">YF. Ma et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2309.10773.pdf">Semi-supervised DomainAdaptation in Graph Transfer Learning</a></td><td style="text-align: center;">Z. Qiao et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0254.pdf">Self-supervisedGraph Disentangled Networks for Review-based Recommendation</a></td><td style="text-align: center;">Y. Ren et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.13115.pdf">Causal-Based Supervision ofAttention in Graph Neural Network: A Better and Simpler Choice towardsPowerful Attention</a></td><td style="text-align: center;">H. Wang et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0260.pdf">Intent-awareRecommendation via Disentangled Graph Contrastive Learning</a></td><td style="text-align: center;">Y. Wang et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0263.pdf">KDLGT: A LinearGraph Transformer Framework via Kernel Decomposition Approach</a></td><td style="text-align: center;">Y. Wu et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0269.pdf">CommonsenseKnowledge Enhanced Sentiment Dependency Graph for SarcasmDetection</a></td><td style="text-align: center;">Z. Yu et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0315.pdf">ApproximatingFair Division on D-Claw-Free Graphs</a></td><td style="text-align: center;">Z. Lonc et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.06864.pdf">Fair Division of a Graphinto Compact Bundles</a></td><td style="text-align: center;">J. Madathil et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2304.11659.pdf">Approximate Envy-Freeness inGraphical Cake Cutting</a></td><td style="text-align: center;">SM. Yuen et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2207.04869.pdf">Graph-based MolecularRepresentation Learning</a></td><td style="text-align: center;">Z. Guo et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.06939.pdf">Deep Multi-view SubspaceClustering with Anchor Graph</a></td><td style="text-align: center;">C. Cui et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0419.pdf">GloballyConsistent Federated Graph Autoencoder for Non-IID Graphs</a></td><td style="text-align: center;">K. Guo et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0426.pdf">Federated GraphSemantic and Structural Learning</a></td><td style="text-align: center;">W. Huang et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2304.11989.pdf">Generative Flow Networks forPrecise Reward-Oriented Active Learning on Graphs</a></td><td style="text-align: center;">Y. Li et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0449.pdf">Multi-View RobustGraph Representation Learning for Graph Classification</a></td><td style="text-align: center;">G. Ma et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2306.06119.pdf">Doubly StochasticGraph-based Non-autoregressive Reaction Prediction</a></td><td style="text-align: center;">Z. Meng et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.05882.pdf">Deep Partial Multi-LabelLearning with Graph Disambiguation</a></td><td style="text-align: center;">H. Wang et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/513">Multi-level GraphContrastive Prototypical Clustering</a></td><td style="text-align: center;">Y. Zhang et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0532.pdf">Relation-enhancedDETR for Component Detection in Graphic Design ReverseEngineering</a></td><td style="text-align: center;">X. Hao et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2305.08316.pdf">SemiGNN-PPI: Self-EnsemblingMulti-Graph Neural Network for Efficient and GeneralizableProtein-Protein Interaction Prediction</a></td><td style="text-align: center;">Z. Zhao et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://openreview.net/pdf?id=eGWEfaW-5Yt">PPAT: Progressive GraphPairwise Attention Network for Event Causality Identification</a></td><td style="text-align: center;">Z. Xu et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2112.08797.pdf">Minimizing ReachabilityTimes on Temporal Graphs via Shifting Label</a></td><td style="text-align: center;">A. Deligkas et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0621.pdf">ExploringStructural Similarity in Fitness Landscapes via Graph Data Mining: ACase Study on Number Partitioning Problems</a></td><td style="text-align: center;">M. Huang et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0649.pdf">Learn and SampleTogether: Collaborative Generation for Graphic Design Layout</a></td><td style="text-align: center;">H. Weng et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/abs/2212.06565">Forecasting Soil Moisture UsingDomain Inspired Temporal Graph Convolution Neural Networks To GuideSustainable Crop Management</a></td><td style="text-align: center;">M Azmat et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4467937">InterpretESG Rating's Impact on the Industrial Chain Using Graph NeuralNetworks</a></td><td style="text-align: center;">B. Liu et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://ieeexplore.ieee.org/abstract/document/9931135">FastGR:Global Routing on CPU-GPU with Heterogeneous Task Graph Scheduler(Extended Abstract)</a></td><td style="text-align: center;">S. Liu et al.</td><td style="text-align: center;">none</td></tr><tr class="even"><td style="text-align: left;"><ahref="https://www.ijcai.org/proceedings/2023/0831.pdf">NeoMaPy: AFramework for Computing MAP Inference on Temporal KnowledgeGraphs</a></td><td style="text-align: center;">V. David et al.</td><td style="text-align: center;">none</td></tr><tr class="odd"><td style="text-align: left;"><ahref="https://arxiv.org/pdf/2307.05380.pdf">Optimized CrystallographicGraph Generation for Material Science</a></td><td style="text-align: center;">A. Klipfel et al.</td><td style="text-align: center;">none</td></tr></tbody></table><h3 id="期刊">期刊</h3><h4 id="ai">AI</h4><p>无法查看的文章，code 栏置空。</p><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://www.sciencedirect.com/science/article/abs/pii/S0004370223000450">AutoSTG+:An automatic framework to discover the optimal network forspatio-temporal graph prediction</a></td><td>Songyu Ke et al.</td><td></td></tr><tr class="even"><td><ahref="https://www.sciencedirect.com/science/article/abs/pii/S000437022200159X">Improvedlocal search for the minimum weight dominating set problem in massivegraphs by using a deep optimization mechanism</a></td><td>Jiejiang Chen et al.</td><td></td></tr><tr class="odd"><td><ahref="https://www.sciencedirect.com/science/article/pii/S000437022300084X?via%3Dihub">Corrigendumto “Separators and adjustment sets in causal graphs: Complete criteriaand an algorithmic framework”</a></td><td>Benito van der Zander et al.</td><td></td></tr><tr class="even"><td><ahref="https://www.sciencedirect.com/science/article/abs/pii/S0004370223001327?via%3Dihub">Multi-modalgraph contrastive encoding for neural machine translation</a></td><td>Yongjing Yin et al.</td><td></td></tr><tr class="odd"><td><ahref="https://www.sciencedirect.com/science/article/abs/pii/S000437022300142X?via%3Dihub">Privacy-preservinggraph convolution network for federated item recommendation</a></td><td>Pengqing Hu et al.</td><td></td></tr></tbody></table><h4 id="tpami">TPAMI</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9699378">LearningRepresentations by Graphical Mutual Information Estimation andMaximization</a></td><td>Zhen Peng</td><td><a href="https://github.com/zpeng27/GMI">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9684725">NeighborhoodPreserving Kernels for Attributed Graphs</a></td><td>Asif Salim</td><td><ahref="https://github.com/asif-salim/NP-graph-kernels">url</a></td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9720092">RepresentingGraphs via Gromov-Wasserstein Factorization</a></td><td>Hongteng Xu</td><td><ahref="https://github.com/HongtengXu/Relational-Factorization-Model">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9729658">BilinearScoring Function Search for Knowledge Graph Learning</a></td><td>Yongqi Zhang</td><td><a href="https://github.com/LARS-research/AutoSF">url</a></td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9763421">DifferentiableGraph Module (DGM) for Graph Convolutional Networks</a></td><td>Anees Kazi</td><td><a href="https://github.com/lcosmo/DGM_pytorch">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9763330">ReinforcedCausal Explainer for Graph Neural Networks</a></td><td>Xiang Wang</td><td><ahref="https://github.com/xiangwang1223/reinforced_causal_explainer">url</a></td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9764632">Self-SupervisedLearning of Graph Neural Networks: A Unified Review</a></td><td>Yaochen Xie</td><td><a href="https://github.com/divelab/DIG/">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9737358">UnsupervisedFeature Selection via Graph Regularized Nonnegative CPDecomposition</a></td><td>Bilian Chen</td><td><a href="https://github.com/Kwan1997/CPUFS">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/abstract/document/9773017">Bagof Tricks for Training Deeper Graph Neural Networks: A ComprehensiveBenchmark Study</a></td><td>Tianlong Chen</td><td><ahref="https://github.com/VITA-Group/Deep_GCN_Benchmarking">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9795073">SIGN:Statistical Inference Graphs Based on Probabilistic Network ActivityInterpretation</a></td><td>Yael Konforti</td><td><a href="https://github.com/yaelkon/GMM-CNN">url</a></td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9858006">DebiasedScene Graph Generation for Dual Imbalance Learning</a></td><td>Hao Zhou</td><td><ahref="https://github.com/zhouhao0515/unbiasedSGG-DSDI">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9808404">HierarchicalPrototype Networks for Continual Graph Representation Learning</a></td><td>Xikun Zhang</td><td><a href="https://github.com/QueuQ/HPNs">url</a></td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9815553">IntegratingMulti-Label Contrastive Learning With Dual Adversarial Graph NeuralNetworks for Cross-Modal Retrieval</a></td><td>Shengsheng Qian</td><td><a href="https://github.com/LivXue/GNN4CMR/">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9816105">Permute MeSoftly: Learning Soft Permutations for Graph Representations</a></td><td>Giannis Nikolentzos</td><td><a href="https://github.com/giannisnik/pi-GNN">url</a></td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9813586">TensorizedBipartite Graph Learning for Multi-View Clustering</a></td><td>Wei Xia</td><td><a href="https://github.com/xdweixia/TBGL-MVC">url</a></td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/9895141">Dynamic GraphMessage Passing Networks</a></td><td>Li Zhang</td><td><a href="https://github.com/fudan-zvg/DGMN2">url</a></td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9875989">Explainabilityin Graph Neural Networks: A Taxonomic Survey</a></td><td>Hao Yuan</td><td><a href="https://github.com/divelab/DIG/">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2106.03535">Graph Neural Networks inNetwork Neuroscience</a></td><td>Alaa Bessadok</td><td><ahref="https://github.com/basiralab/GNNs-in-Network-Neuroscience">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2103.04256">Robust Point CloudRegistration Framework Based on Deep Graph Matching</a></td><td>Kexue Fu</td><td><a href="https://github.com/fukexue/RGM">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2206.05416">Semi-SupervisedHierarchical Graph Classification</a></td><td>Jia Li</td><td><a href="https://github.com/kochsnow/SEAL_MASTER">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2007.10467">Second-Order Pooling forGraph Neural Networks</a></td><td>Zhengyang Wang</td><td><a href="https://github.com/divelab/sopool">url</a></td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/9241410">HiGCIN:Hierarchical Graph-Based Cross Inference Network for Group ActivityRecognition</a></td><td>Rui Yan</td><td><a href="https://github.com/ruiyan1995/HiGCIN">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/9128045">CombinatorialLearning of Robust Deep Graph Matching: An Embedding BasedApproach</a></td><td>Runzhong Wang</td><td><a href="https://github.com/Thinklab-SJTU/ThinkMatch">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2104.14236">Learning Multi-AttentionContext Graph for Group-Based Re-Identification</a></td><td>Yichao Yan</td><td><a href="https://github.com/daodaofr/group_reid">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2010.13242">Co-Embedding of Nodes andEdges With Graph Neural Networks</a></td><td>Xiaodong Jiang</td><td><a href="https://github.com/ronghangzhu/CensNet">url</a></td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/document/9980390">DifferentiallyPrivate Graph Neural Networks for Whole-Graph Classification</a></td><td>Tamara T. Mueller</td><td><a href="https://github.com/tamaramueller/DP-GNNs">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/9947327">LearningView-Based Graph Convolutional Network for Multi-View 3D ShapeAnalysis</a></td><td>Xin Wei</td><td><a href="https://github.com/weixmath/view-GCN">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2109.13098">One-Hot Graph EncoderEmbedding</a></td><td>Cencheng Shen</td><td><a href="https://github.com/cshen6/GraphEmd">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/9982412">A GenericGraph-Based Neural Architecture Encoding Scheme With MultifacetedInformation</a></td><td>Xuefei Ning</td><td><a href="https://github.com/walkerning/aw_nas">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2205.07404">A New Outlier RemovalStrategy Based on Reliability of Correspondence Graph for Fast PointCloud Registration</a></td><td>Li Yan</td><td><a href="https://github.com/WPC-WHU/GROR">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2203.02177">GCNet: Graph CompletionNetwork for Incomplete Multimodal Learning in Conversation</a></td><td>Zheng Lian</td><td><a href="https://github.com/zeroQiaoba/GCNet">url</a></td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/10012542">SIGMA++:Improved Semantic-Complete Graph Matching for Domain Adaptive ObjectDetection</a></td><td>Wuyang Li</td><td><a href="https://github.com/CityU-AIM-Group/SIGMA">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2206.04665">AGConv: Adaptive GraphConvolution on 3D Point Clouds</a></td><td>Mingqiang Wei</td><td><a href="https://github.com/hrzhou2/AdaptConv-master">url</a></td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/10086644">muxGNN:Multiplex Graph Neural Network for Heterogeneous Graphs</a></td><td>Joshua Melton</td><td><a href="https://github.com/NASCL/muxGNN">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2201.11460">RelTR: RelationTransformer for Scene Graph Generation</a></td><td>Yuren Cong</td><td><a href="https://github.com/yrcong/RelTR">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2202.06174">Source-Free ProgressiveGraph Learning for Open-Set Domain Adaptation</a></td><td>Yadan Luo</td><td><a href="https://github.com/Luoyadan/SF-PGL">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2206.08842">Entity-Graph EnhancedCross-Modal Pretraining for Instance-Level Product Retrieval</a></td><td>Xiao Dong</td><td><ahref="https://github.com/Xiaodongsuper/Entity-Graph-Enhanced-Cross-Modal-Pretraining-for-Instance-level-Product-Retrieval">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2302.13668">Contrastive VideoQuestion Answering via Video Graph Transformer</a></td><td>Junbin Xiao</td><td><a href="https://github.com/doc-doc/CoVGT">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10194972">AHeterogeneous Graph to Abstract Syntax Tree Framework forText-to-SQL</a></td><td>Ruisheng Cao</td><td><ahref="https://github.com/rhythmcao/text2sql-graph2tree">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2205.09753">HDGT: HeterogeneousDriving Graph Transformer for Multi-Agent Trajectory Prediction viaScene Encoding</a></td><td>Xiaosong Jia</td><td><a href="https://github.com/OpenDriveLab/HDGT">url</a></td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2103.13225">STAR-FC: Structure-AwareFace Clustering on Ultra-Large-Scale Graphs</a></td><td>Shuai Shen</td><td><a href="https://github.com/sstzal/STAR-FC">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.14617">Multiscale Dynamic GraphRepresentation for Biometric Recognition With Occlusions</a></td><td>Min Ren</td><td><ahref="https://github.com/RenMin1991/Dyamic-Graph-Representation">url</a></td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10193837">Long andShort-Range Dependency Graph Structure Learning Framework on PointCloud</a></td><td>Jiye Liang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2204.08646">Label EfficientRegularization and Propagation for Graph Node Classification</a></td><td>Tian Xie</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2206.15174">Graph-Time ConvolutionalNeural Networks: Architecture and Theoretical Analysis</a></td><td>Mohammad Sabbaqi</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2207.04602">Adaptive Fine-GrainedPredicates Learning for Scene Graph Generation</a></td><td>Xinyu Lyu</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2306.09114">Relational Temporal GraphReasoning for Dual-Task Dialogue Language Understanding</a></td><td>Bowen Xing</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/10149528/">GraphNeural Network Meets Sparse Representation: Graph Sparse Neural Networksvia Exclusive Group Lasso</a></td><td>Bo Jiang</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2301.09071">Variational Cross-GraphReasoning and Adaptive Structured Semantics Learning for CompositionalTemporal Grounding</a></td><td>Juncheng Li</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2307.05276">Unbiased Scene GraphGeneration via Two-Stage Causal Modeling</a></td><td>Shuzhou Sun</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10120982">Learning toInfer Unseen Single-/ Multi-Attribute-Object Compositions With GraphNetworks</a></td><td>Hui Chen</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/10145799">LearningHidden Graphs From Samples</a></td><td>Ahmad Abniki</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2207.01450">Discourse-Aware GraphNetworks for Textual Logical Reasoning</a></td><td>Yinya Huang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2201.11697">Constrained StructureLearning for Scene Graph Generation</a></td><td>Daqi Liu</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10091452">SparseQuadratic Approximation for Graph Learning</a></td><td>Dimosthenis Pasadakis</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/10073537">UnsupervisedLearning of Graph Matching With Mixture of Modes via DiscrepancyMinimization</a></td><td>Runzhong Wang</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2112.05727">Neural Belief Propagationfor Scene Graph Generation</a></td><td>Daqi Liu</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/10129060">Large-ScaleClustering With Structured Optimal Bipartite Graph</a></td><td>Han Zhang</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10068311">Graph TheoryBased Large-Scale Machine Learning With Multi-Dimensional ConstrainedOptimization Approaches for Exact Epidemiological Modeling of PandemicDiseases</a></td><td>Onder Tutsoy</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2207.12659">Graph Neural Network andSpatiotemporal Transformer Attention for 3D Video Object Detection FromPoint Clouds</a></td><td>Junbo Yin</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2302.10237">SceneHGN: HierarchicalGraph Networks for 3D Indoor Scene Generation With Fine-GrainedGeometry</a></td><td>Lin Gao</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/9996549">Non-GraphData Clustering via <span class="math inline">\(\mathcal{O}(n)\)</span>Bipartite Graph Convolution</a></td><td>Hongyuan Zhang</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10023982">GraphDiffusion Convolutional Network for Skeleton Based Semantic Recognitionof Two-Person Actions</a></td><td>Shuai Li</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/9966846">StructureEvolution on Manifold for Graph Learning</a></td><td>Hai Wan</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/2011.04055">Fourier-Based andRational Graph Filters for Spectral Processing</a></td><td>Giuseppe Patanè</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/1908.07141">LogicENN: A Neural BasedKnowledge Graphs Embedding Model With Logical Rules</a></td><td>Mojtaba Nayyeri</td><td>none</td></tr><tr class="odd"><td><a href="https://arxiv.org/abs/1908.11754">Fashion Retrieval viaGraph Reasoning Networks on a Similarity Pyramid</a></td><td>Zhanghui Kuang</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/9369105">LearningGraph Convolutional Networks for Multi-Label Recognition andApplications</a></td><td>Zhao-Min Chen</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/9234715">LearningMulti-View Interactional Skeleton Graph for Action Recognition</a></td><td>Minsi Wang</td><td>none</td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2105.11016">Revisiting 2DConvolutional Neural Networks for Graph-Based Applications</a></td><td>Yecheng Lyu</td><td>none</td></tr><tr class="odd"><td><a href="https://ieeexplore.ieee.org/document/10120667">GuestEditorial: Introduction to the Special Section on Graphs in Vision andPattern Analysis</a></td><td>Song Bai</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/abstract/document/9920219">ASystematic Survey on Deep Generative Models for GraphGeneration</a></td><td>Xiaojie Guo</td><td>none</td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9868157">UnsupervisedGraph Embedding via Adaptive Graph Learning</a></td><td>Rui Zhang</td><td>none</td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9786681">RelationMatters: Foreground-Aware Graph-Based Relational Reasoning for DomainAdaptive Object Detection</a></td><td>Chaoqi Chen</td><td>none</td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9780235">GroupContrastive Self-Supervised Learning on Graphs</a></td><td>Xinyi Xu</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/abstract/document/9796468">AreGraph Convolutional Networks With Random Weights Feasible?</a></td><td>Changqin Huang</td><td>none</td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9756344">GraphLearning on Millions of Data in Seconds: Label Propagation Accelerationon Graph Using Data Distribution</a></td><td>Yubo Zhang</td><td>none</td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9741534">Duality-InducedRegularizer for Semantic Matching Knowledge Graph Embeddings</a></td><td>Jie Wang</td><td>none</td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9681162">Point CloudSampling via Graph Balancing and Gershgorin Disc Alignment</a></td><td>Chinthaka Dinesh</td><td>none</td></tr><tr class="even"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9721082">ImprovingGraph Neural Network Expressivity via Subgraph IsomorphismCounting</a></td><td>Giorgos Bouritsas</td><td>none</td></tr><tr class="odd"><td><ahref="https://ieeexplore.ieee.org/abstract/document/9672718">EfficientVariational Bayes Learning of Graphical Models With Smooth StructuralChanges</a></td><td>Hang Yu</td><td>none</td></tr><tr class="even"><td><a href="https://ieeexplore.ieee.org/document/9661322">AComprehensive Survey of Scene Graphs: Generation andApplication</a></td><td>Xiaojun Chang</td><td>none</td></tr></tbody></table><h4 id="ijcv">IJCV</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><ahref="https://link.springer.com/article/10.1007/s11263-022-01703-8#Sec4">RecurrentGraph Neural Networks for Video Instance Segmentation</a></td><td>Emil Brissman</td><td><a href="https://github.com/emibr948/RGNNVIS-PlusPlus">url</a></td></tr><tr class="even"><td><a href="https://arxiv.org/abs/2211.06719">Bipartite Graph ReasoningGANs for Person Pose and Facial Image Synthesis</a></td><td>Hao Tang</td><td><a href="https://github.com/Ha0Tang/BiGraphGAN">url</a></td></tr><tr class="odd"><td><ahref="https://link.springer.com/article/10.1007/s11263-023-01817-7">ImportanceFirst: Generating Scene Graph of Human Interest</a></td><td>Wenbin Wang</td><td><a href="https://github.com/Kenneth-Wong/TGIR">url</a></td></tr><tr class="even"><td><ahref="https://link.springer.com/article/10.1007/s11263-023-01839-1">AutomaticGeneration of 3D Scene Animation Based on Dynamic Knowledge Graphs andContextual Encoding</a></td><td>Wenfeng Song</td><td><ahref="https://github.com/ZxyLinkstart/Automatic-Generation-of-3D-Scene-Animation">url</a></td></tr></tbody></table><h4 id="jmlr">JMLR</h4><table><thead><tr class="header"><th>paper</th><th>authors</th><th>code</th></tr></thead><tbody><tr class="odd"><td><a href="https://jmlr.org/papers/v24/20-1206.html">Bayesian SpikedLaplacian Graphs</a></td><td>Leo L Duan</td><td><ahref="https://github.com/leoduan/BayesSpikedLaplacian">url</a></td></tr><tr class="even"><td><a href="https://jmlr.org/papers/v24/20-449.html">Sampling randomgraph homomorphisms and applications to network data analysis</a></td><td>Hanbaek Lyu</td><td><a href="https://github.com/HanbaekLyu/motif_sampling">url</a></td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/21-0877.html">Graph-AidedOnline Multi-Kernel Learning</a></td><td>Pouya M. Ghari</td><td><ahref="https://github.com/pouyamghari/Graph-Aided-Online-Multi-Kernel-Learning">url</a></td></tr><tr class="even"><td><a href="https://www.jmlr.org/papers/v24/22-0567.html">BenchmarkingGraph Neural Networks</a></td><td>Vijay Prakash Dwivedi</td><td><ahref="https://github.com/graphdeeplearning/benchmarking-gnns">url</a></td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/21-0855.html">Inference for aLarge Directed Acyclic Graph with Unspecified Interventions</a></td><td>Chunlin Li</td><td><a href="https://github.com/chunlinli/intdag">url</a></td></tr><tr class="even"><td><a href="https://jmlr.org/papers/v24/22-0337.html">FittingAutoregressive Graph Generative Models through Maximum LikelihoodEstimation</a></td><td>Xu Han</td><td><ahref="https://github.com/tufts-ml/Graph-Generation-MLE">url</a></td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/22-1085.html">A UnifiedFramework for Optimization-Based Graph Coarsening</a></td><td>Manoj Kumar</td><td><ahref="https://github.com/GraphCoarsening/Featured-Graph-Coarsening">url</a></td></tr><tr class="even"><td><a href="https://jmlr.org/papers/v24/20-998.html">Graph Clusteringwith Graph Neural Networks</a></td><td>Anton Tsitsulin</td><td><ahref="https://github.com/google-research/google-research/tree/master/graph_embedding/dmon">url</a></td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/21-1254.html">Large samplespectral analysis of graph-based multi-manifold clustering</a></td><td>Nicolas Garcia Trillos</td><td><a href="https://github.com/chl781/manifold-clustering">url</a></td></tr><tr class="even"><td><a href="https://jmlr.org/papers/v24/21-0434.html">Factor GraphNeural Networks</a></td><td>Zhen Zhang</td><td><ahref="https://github.com/zzhang1987/Factor-Graph-Neural-Network">url</a></td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/21-082.html">QuantifyingNetwork Similarity using Graph Cumulants</a></td><td>Gecia Bravo-Hermsdorff</td><td><ahref="https://github.com/TheGravLab/GraphCumulantComparison">url</a></td></tr><tr class="even"><td><a href="https://jmlr.org/papers/v24/22-1154.html">Sparse GraphLearning from Spatiotemporal Time Series</a></td><td>Andrea Cini</td><td><ahref="https://github.com/andreacini/sparse-graph-learning">url</a></td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/22-125.html">Graph AttentionRetrospective</a></td><td>Kimon Fountoulakis</td><td><ahref="https://github.com/opallab/Graph-Attention-Retrospective">url</a></td></tr><tr class="even"><td><a href="https://jmlr.org/papers/v24/21-0449.html">CombinatorialOptimization and Reasoning with Graph Neural Networks</a></td><td>Quentin Cappart</td><td>none</td></tr><tr class="odd"><td><a href="https://jmlr.org/papers/v24/22-1138.html">An AnnotatedGraph Model with Differential Degree Heterogeneity for DirectedNetworks</a></td><td>Stefan Stein</td><td>none</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Few-shot Learning（小样本学习）初试</title>
    <link href="/2023/11/12/few-shot-learning/"/>
    <url>/2023/11/12/few-shot-learning/</url>
    
    <content type="html"><![CDATA[<h2 id="问题定义">问题定义</h2><p>小明和妈妈去动物园，小明看到了一种从未见过的动物，妈妈为了帮助小明认识小动物拿出一些动物卡片，卡片上有动物图片和他们的名称，小明看了一会儿，发现有一张标有海獭的卡片和刚才不认识的动物很相似，妈妈告诉他刚才看到的动物就是海獭。</p><p>这种通过极少样本识别一个新物体的学习方式就是 Few-shotLearning（小样本学习，FSL）。Few-shot learning 是 <strong>MetaLearning</strong> 的 一种，Meta Leanring就是<strong>去学习如何学习（Learn tolearn）</strong>。普通机器学习是学习到方法，元学习是学习到学习能力。</p><h2 id="算法介绍">算法介绍</h2><p>Few-shot learning 在实际的训练过程常分为两个过程 Traing Stage 和 TestStage，每个 Stage 的数据又被分为 Support set 和 Query set 两部分。FSL通常会被分成 N-way K-shot 问题，这里的 N 指的是 Test Stage的过程中用的是多少类样本，K 指的是 Test Stage的过程中每一类有多少个样本。</p><p>依据这个定义，我们首先来看 S' 和 Q'是如何得到的？如上图右半部分所示，假设 Testing data所代表的大的矩形框对应着原始的测试数据，这些数据可能包含有很多类。现在，我们从中随机选出N 个类，每个类都可能含有很多个样本。然后，我们再从这 N个类中，每类都随机选出 k+x 个样本（x 代表可以选任意个，但必须满足 k+x不超过每个类最大的样本个数），其中的 k 个样本将被用作 Support setS'，另外的 x 个样本将被用作 Query set Q'。由此，我们便得到了Meta-testing 过程中需要用到的两个数据集。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Few-shot Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文研读-Machine learning meets omics applications and perspectives</title>
    <link href="/2023/11/10/lun-wen-yan-du-machine-learning-meets-omics-applications-and-perspectives/"/>
    <url>/2023/11/10/lun-wen-yan-du-machine-learning-meets-omics-applications-and-perspectives/</url>
    
    <content type="html"><![CDATA[<p>2022年的一篇生物信息和深度学习综述《Machine learning meets omics:applications and perspectives》，记录一下。</p><p>本文描述了人工智能如何应用于组学研究，并回顾了机器学习和最广泛的组学之间接口的最新进展，包括基因组学、转录组学、蛋白质组学、代谢组学、放射组学以及单组学细胞分辨率。坐着还讨论并综合了组学中机器学习的想法、新见解、当前挑战和观点。</p><h2 id="ml-in-genomics">ML in Genomics</h2><p>基因组学是对生物体完整 DNA集的研究，重点关注基因组的结构、功能、进化、绘图和编辑。新一代基因组技术使生物医学研究人员能够获得大量基因组数据，原则上可以通过高通量测量获得数十亿个基因组坐标和其他相关数据信息[15]。然而，基因组数据通常是高维且稀疏的，这使得分析变得困难，从而为机器学习提供了捕获数据中的依赖关系，然后得出新的生物学假设的机会。随着硬件和算法的不断完善，加上大规模、多方面的生物医学数据，机器学习方法在广泛的基因组学研究中取得了显着的成功，包括但不限于3D 基因组的重建（reconstruction of 3Dgenome）、表观基因组和染色质状态的建模（modeling of epigenomic andchromatin states）、基因组注释和转录（genome annotation andtranscription）以及基因组编辑（genome editing）。</p><h3 id="d-基因组的重建">3D 基因组的重建</h3><p>真核基因组的空间组织确保基因组 DNA折叠以适合细胞核内，但仍可用于许多遗传和生物功能，例如基因调节（generegulation）、基因表达（gene expression）、转录调节（transcriptionregulation）、DNA 修复（DNA repair）和DNA复制（DNAreplication）。了解真核基因组的 3 维 (3D)组织对于解释细胞内重要的染色体活动至关重要。在过去的十年中，实验和计算工作一直致力于揭示3D 基因组组织。各种基于染色体构象捕获 (3C) 的技术（例如3C、4C、5C、Hi-C、TCC、ChIA-PET 以及后来的单细胞 Hi-C）已被开发用于研究3D 分层结构，例如染色体区室、拓扑相关域 (TAD)和染色质环。尽管高通量实验技术已经证明了各种核和细胞过程中的基因组组织和功能相关性，但它们没有足够高的分辨率来解决调控元件之间的联系，研究base-level水平遗传变异对基因组结构或经济高效地应用于大规模样品。因此，已经开发了计算方法，特别是机器学习模型来补充和推进3D 基因组的实验研究。用于预测 3D基因组结构的计算方法在机器学习模型中有所不同，但可以根据其输入（训练）数据分为四类：(1)单独的基因组序列，(2) 基于 3C 的相互作用，(3)源自于的染色质状态表观遗传修饰和（4）上述数据的混合。这些方法的目标或输出也不同，包括3D 基因组重建、基因组区室化检测、拓扑关联域 (TAD)识别和染色质环（调控元件之间的相互作用）预测。我们根据训练数据类别对这些机器学习方法进行分组总结。</p><p>使用基因组序列预测染色质相互作用不仅可以用最少的实验数据扩展对新细胞类型中基因组组织的理解，还可以使用CRISPR等基因组编辑技术预测非编码调控区域扰动的影响。因此，许多努力都集中在利用DNA 序列特征来预测染色质相互作用。Fudenberg 等人设计了一个基于 CNN的模型叫做 Akita，仅根据 DNA序列即可准确预测基因组折叠，可用于进行计算机模拟饱和诱变、解释eQTL、预测结构变异和探测物种特异性基因组折叠。Cao等开发了染色质相互作用神经网络 (ChINN)，仅使用相互作用的开放染色质区域的DNA 序列来预测开放染色质区域之间的染色质相互作用。更具体地说，Trieu等人开发了 DeepMILO，这是一种深度学习方法，用于预测非编码序列变异对 3D染色质结构的影响。</p><p>表观基因组修饰和 3D基因组相互作用紧密相关，但目前，它们是通过不同的技术来测量的。已经开发出用于综合解释甚至从表观基因组和染色质状态重建3D 基因组的计算方法。 Zhu等人提出 EpiTensor可以根据组蛋白修饰、染色质可及性和 RNA-seq 的 1D表观基因组在拓扑关联域（TAD）内构建 3D 空间关联。Fortin等人使用来自几个不同平台的表观遗传数据的长期相关性来估计不同细胞类型中的A/B 区室，以及细胞类型之间的变化。Qi等人引入了一种计算模型，可通过染色质状态以五千碱基分辨率预测 3D基因组组织</p><p>高通量、下一代基于 3C的技术的出现使我们能够确定一对（或多对）基因组区域（箱）是否在 3D物理位置上非常接近。空间邻近的基因组区域通常被称为“相互作用”。在过去的十年中，人们提出了许多计算方法和算法，用于根据3C 数据构建染色体和基因组 3D 结构。例如，Schwessinger等人基于迁移学习和DNN开发了DeepC，并使用组织特异性Hi-C数据训练模型。麦凯等人。Oluwadare等人全面回顾了从 3C 数据重建 3D 染色体和基因组结构的机器学习方法。</p><p>基因组、表观基因组、转录组和染色体构象数据的积累催生了许多机器学习方法，这些方法整合了多个组学数据以进行3D 基因组预测。Bkhetan 等人开发了 3DEpiLoop算法来根据表观基因组数据和转录因子谱预测染色质循环相互作用。Whalen等人实施了一种名为 TargetFinder 的算法，该算法集成了TF、组蛋白标记、DNase-seq、表达和 DNA甲基化的数据，以预测整个基因组中各个启动子-增强子的相互作用[39]。最近，为了减少迄今为止仅在极少数人类细胞系中同时可用的表观基因组数据的数量，Li等人开发了DeepTACT，这是一种引导深度学习模型，利用序列特征和染色质可及性信息来预测单个调控元件水平的染色质接触。</p><h3id="表观基因组和染色质状态的计算建模">表观基因组和染色质状态的计算建模</h3><p>表观基因组由 DNA 和组蛋白的化学修饰记录组成，这些修饰独立于 DNA序列调节基因组活性。表观基因组的改变可能导致染色质结构的变化和基因组功能的变化。表观基因组技术的最新进展使得涉及高通量数据和基于机器学习的生物信息学的研究能够识别易受表观遗传修饰影响的基因组区域，包括DNA 甲基化变化（例如CpG）、组蛋白修饰或染色质结构变化（例如核小体定位）。人们已经开发了各种将表观遗传数据的特征生成、特征选择和机器学习相结合的计算方法，包括结合主动学习（ACL）来解决生成表观遗传数据的费用问题，不平衡类学习（ICL）来解决相对较低的发生率数据和深度学习(DL) 中的表观突变可解决手动定义相关基因组特征的困难。</p><p>基因组特征可以包括 DNA序列和表观遗传成分。许多努力已将遗传特征（例如重复元件、CpG密度、响应元件或特定序列）和表观遗传特征（例如 DNA甲基化或组蛋白介导的核小体定位和染色质状态）融合到机器学习模型中，以进行功能解释和分析。基因组的预测。Whitaker 等人提出了 Epigram，用于根据 DNA 基序预测组蛋白修饰和 DNA甲基化模式。李等人建立了 gapped k-mer 支持向量机(gkm-SVM)，用于学习人类调控元件和调控变异的影响，这些调控变异是针对DNase I 超敏反应、独特的组蛋白修饰和转录因子结合而生成的。Zhou等开发了一种基于深度学习的算法框架DeepSEA，它直接从大规模染色质分析数据中学习调控序列代码，从而能够以单核苷酸敏感性预测序列改变的染色质影响。Alipanahi等人引入 DeepBind 通过深度学习预测 DNA 和 RNA结合蛋白的序列特异性。Kelley 等人开发了 Basset 深度卷积神经网络(CNN)，研究人员可以使用它对感兴趣的细胞类型进行单一测序分析，同时了解细胞的染色质可及性代码，并注释基因组中的每个突变及其对当前可及性和潜在的影响可访问性的潜力。Kelley等人开发了另一种基于 CNN 的机器学习系统，可以仅根据 DNA序列预测大型哺乳动物基因组中细胞类型特异性的表观遗传和转录谱。Hoffman等人开发了深度学习模型 DeepFIGV，仅使用 DNA序列作为输入即可准确预测遗传变异对染色质可及性和组蛋白修饰的影响。Zeng等人介绍了 CpGenie，这是一个基于序列的框架，它使用深度卷积神经网络 (CNN)学习 DNA 甲基化的调控代码，并使用该网络来预测序列变异对近端 CpG 位点 DNA甲基化的影响。机器学习模型也应用于单细胞表观基因组。例如，Angermueller等人开发了 DeepCpG，一种基于深度神经网络 (DNN)的计算方法，用于预测单细胞中的甲基化状态。</p><h2 id="ml-for-genome-annotation-and-transcription-regulation">ML forGenome Annotation and Transcription Regulation</h2><p>基因组注释是标记生物体 DNA序列并表征其生物学功能的过程。基因组学的第一步和早期工作自然集中于构建基因组注释，为此开发了机器学习方法来识别一些主要类别的基因组元素，例如蛋白质编码基因（proteincodinggenes）、非编码RNA（ncRNA）、微小RNA（miRNA）、转录剪接异构体（transcriptsplicing isoforms）、调控元件（regulatoryelements）、蛋白质结合位点/基序（protein-bindingsites/motifs）和顺式调控结合模块（cis-regulatory bindingmodules）。基因组、转录组和表观基因组信息的快速积累为组学信息的整合创造了新的机遇，也带来了挑战，而不仅仅是依靠序列本身来注释基因组。同时，基因组注释的目标已经从识别不同的基因组部分（例如基因和非编码区）扩展到阐明它们的功能，包括调控元件及其相互作用。例如，Encyclopediaof DNA Elements (ENCODE) 项目代表了为此目的最集体的努力。</p><h3 id="从基因组序列中查找基因">从基因组序列中查找基因</h3><p>随着新测序技术的出现，海量基因组序列数据可用于基因和非编码元件的计算识别。主要工作是识别原核和真核基因组中的基因，以及预测真核生物中复杂的外显子结构。Luedi 等人基于使用 DNA 序列作为特征的多种分类算法的预测，识别出了 156个新型人类印记基因。 Mark Borodovsky 使用具有长短期记忆 (LSTM)的循环神经网络 (RNN) 来证明深度学习网络注释基因组序列和评估来自 NCBI数据库的原核序列的不同方法的潜力。后来他提出了两种新的基因组注释算法：(1)GeneMarkS2+，除了 PGAP 之外，NCBI 开发的原核基因组注释管道，以及 (2)真核自我训练基因查找器GeneMarkEP+，在迭代参数化中利用直系同源蛋白质的足迹基因组组织的隐马尔可夫模型（HMM）统计模型。</p><h3 id="识别顺式调控元件和反式作用-bandings">识别顺式调控元件和反式作用bandings</h3><p>在人类基因组中，大多数DNA序列都是非编码区，其中含有许多顺式调控元件，包括启动子、增强子等。近年来，利用机器学习和深度学习方法在非编码区域识别方面取得了许多成果。仅使用DNA 序列或 ATAC-seq特征就可以在基因组规模上准确识别增强子。此外，在非编码RNA的识别和分类方面也取得了良好的成果。然而，传统的神经网络需要大量的训练数据。CNN 是一类深度学习神经网络，已成功用于学习 DNA 序列模式，例如 DNA 和 RNA结合蛋白的序列模式、DNA 甲基化或染色质分析数据。例如，Alipanahi等人构建了 DeepBind 模型来预测 DNA 和 RNA结合蛋白的序列特异性。使用该模型，即使模式在序列中的位置未知，也可以找到新的模式。最近，Long等人。提出了一种通过整合基因组序列、结构数据和统计学习来预测转录因子结合位点的方法。</p><h3id="预测增强子-启动子相互作用和基因表达">预测增强子-启动子相互作用和基因表达</h3><p>启动子和增强子是控制基因表达的空间和时间模式的最重要的顺式调控元件。除了识别这些调控元件之外，一个主要的挑战是表征染色体环，通过该染色体环，远端增强子在三维空间中靠近靶基因并作用于靶启动子。这种长程增强子-启动子（E-P）相互作用正在成为组织特异性表达和解释调控变异的重要决定因素。尽管基于3C 的实验技术揭示了染色质相互作用，但它们只能覆盖更高级别的 3D基因组架构，例如TAD，而不是特定的增强子-启动子相互作用。因此，用于预测跨多个组织或细胞系的增强子和启动子之间相互作用的计算方法正在出现。</p><p>Roy 等人提出了一种基于监督机器学习框架的预测建模方法，名为 RIPPLE。RIPPLE 使用 5C实验检测到的相互作用来预测细胞系特异性的长期调控相互作用。后来，惠伦等人提出的TargetFinder，这是一种机器学习方法，可根据功能基因组数据预测增强子-启动子相互作用，例如来自组蛋白修饰或转录因子的ChIP-seq 和 DNase I 超敏感位点测序 (DNase-seq)。然后，Cao等人提出了一种监督机器学习方法，该方法结合全局和样本特定的表观遗传信息，通过使用随机森林分类器预测增强子-启动子相互作用，称为JEME。最近，曹等人。受到以下事实的启发：使用 DeepSEA 和 DeepBind等方法仅从 DNA序列即可预测染色质状态和许多转录因子结合位点。他们开发了一种计算方法，即染色质相互作用神经网络(ChINN) ，仅使用相互作用的开放染色质区域的 DNA序列来预测开放染色质区域之间的染色质相互作用。</p><h3 id="机器学习遗传变异的影响">机器学习遗传变异的影响</h3><p>除了那些相对较大的调控元件之外，单个核苷酸的变异也对基因表达和表型具有关键影响，尽管它们可能位于非编码区。全基因组关联研究（GWAS）报告了数十万种与复杂性状相关的变异，包括疾病和病理表型。大多数与疾病相关的变异存在于非编码DNA中。识别非编码变异的功能效应一直是GWAS之后的重大挑战，机器学习和深度学习在识别非编码变异及其效应方面也发挥着重要作用。基于SVM 的模型可用于对有害变异进行分类并对变异的致病性进行评分。Schubach等人使用不平衡感知方法来预测常见的疾病相关突变，这明显优于不平衡感知的机器学习方法。最近，Zhou等人利用深度学习方法开发了基于序列的DeepSEA，可用于预测非编码突变效应。然后，他们构建了一个基于深度学习的框架ExPecto，用于识别非编码突变并预测非编码突变对疾病的贡献。这些模型可以根据DNA序列准确预测组织特异性突变的转录效应，包括罕见或从未观察到的突变。</p><h2 id="ml-in-genome-editing">ML in Genome Editing</h2><p>在过去的十年中，基因组研究人员已经远远超出了查明现有基因组序列和相关修饰的范围，而是进入了一个前所未有的、在实现之前从未想象过的更具创新性的时代，即基因组编辑。基因组编辑是根据需要，通过添加或删除特定基因片段或插入、删除或替换特定碱基来改造目标基因的序列或功能。从更广泛的意义上讲，基因组编辑还包括表观遗传编辑，这是一种指导人工转录因子或染色质修饰剂来调节靶基因表达或改变染色质状态的新兴技术。成簇规则间隔短回文重复序列（CRISPR）是应用最广泛的基因组工程系统，因其在基因剪切和粘贴方面的高精度和灵活性而被应用于靶基因修复和基因表达调控。近年来，基因组编辑技术非常流行，不仅用于生物学和医学目的，还用于解决计算挑战。无论是基因编辑还是表观遗传修饰，CRISPR系统的关键是将编辑复合物精确引导至目标位置，这是通过精心设计的小引导RNA（sgRNA）来完成的。机器学习已积极应用于CRISPR 系统（1）根据基因组背景设计 sgRNA并预测切割趋势，（2）评估不同特征对 CRISPR效率和选择性的重要性，以及（3）评估脱靶倾向等等。所有这些机器学习方法可以分为三类：基于对齐的方法、假设驱动的方法和基于学习的方法。</p><p>sgRNA的设计和选择是基因组编辑的第一步，也是最重要的一步，它决定了剪刀在基因组中的位置，并评估剪刀切割基因组的效率。sgRNA设计还需要对另一面进行系统评估，即脱靶效应，即如果使用它，可能会严重破坏基因组的其余部分。一些sgRNA可以干扰细胞群中几乎所有的目标等位基因，而另一些则没有表现出明显的活性。然后，人们可以重新对指南进行排序，以便在裂解趋势和脱靶倾向之间进行权衡。目前，有许多通过机器学习或深度学习方法开发的sgRNA设计工具，例如SVM模型、CNN模型等，可以帮助研究人员获得高效的sgRNA。DeepCpf1 的开发是为了使用 sgRNA 序列特征和染色质可及性，基于 CNN 来预测Cpf1 的 sgRNA 编辑效率。 DeepCRISPR除了DNA序列特征之外还引入了四种表观遗传特征，并利用自动编码器的原理自动提取有效信息。建立了sgRNA 靶标切割和脱靶倾向预测等多种模型。 CNN_std仅使用序列特征来使用“XOR”编码设计构建二维输入矩阵，并利用 CNN进行预测。此外，Dimauro 等人。提出了一个名为 CRISPRLearner的模型，用于预测 sgRNA 靶向敲除活性。Song等人建立了基于深度学习的模型来预测任意目标序列中碱基编辑器定向编辑的效率和结果频率。</p><p>除了 sgRNA 设计之外，另一个活跃的主题是预测编辑的结果，即预测特定引导RNA的靶向位点将进行哪种类型的修复。可能性包括插入单个碱基对、小的缺失或称为微同源缺失的较大变化。序列修复的准确预测可以让研究人员通过计算来预测精确的引导RNA，从而重现精确的人类患者突变，从而开发出更好的研究模型来研究遗传疾病。在《自然》杂志上发表的一项研究中，Sherwood及其同事描述了他们如何训练一种名为 inDelphi 的机器学习算法来预测用 Cas9剪断的 DNA 的修复情况。该算法显示，在超过 50% 的编辑产品中，使用的指导RNA 中有 5-11%在人类基因组中诱导了单一的、可预测的修复基因型。另外，Felicity Allen和同事创建了一种名为 FORECasT（Cas9目标修复事件的有利结果）的算法来完成同样的事情。基于 41 630 个引导 RNA的文库以及修复前后目标位点的序列，该模型表明大多数修复是单碱基插入、小缺失或较长缺失（称为微同源介导的缺失），并且基于特定序列存在于Cas9 切割位点。</p><h2 id="ml-in-transcriptomics">ML in Transcriptomics</h2><p>转录组学是对转录组（由基因组产生的完整 RNA转录本）的研究。随着高通量方法的发展，研究人员可以全面、快速地获取物种在某种状态下特定组织或器官的几乎所有转录序列信息及其表达。然而，如此大量的转录组测序数据的分析和处理给传统的分析方法带来了挑战。机器学习和深度学习可以处理大规模高维数据。它已广泛应用于转录组数据分析，帮助全面研究基因表达、功能和结构，进而揭示发育过程和疾病进展中的特定生物过程和分子机制。</p><h3 id="基因表达的预测">基因表达的预测</h3><p>基因表达定义了基因组的哪些部分以多少量转录。基于转录测序（RNA-seq）的基因表达分析已成为转录组学研究的重要组成部分。机器学习具有很强的能力来学习此类数据集上的分层非线性模式，并且在根据遗传和表观遗传信息预测基因表达方面发挥着重要作用。基于深度学习，Chen等人提出了一种多任务多层前馈神经网络，称为D-GEX，用于基于标志基因的目标基因表达预测。组蛋白修饰是影响基因调控的另一个重要因素。从组蛋白修饰信号预测基因表达的计算方法对于理解它们在基因调控中的综合作用至关重要。研究人员使用深度卷积神经网络建立了一个称为DeepChrome的判别框架，以预测组蛋白修饰的基因表达。发现深度学习模型在基因表达预测任务中优于支持向量机、随机森林等最先进的模型。</p><h3 id="剪接的预测和分类">剪接的预测和分类</h3><p>剪接是转录的另一个方面，它定义了真核基因组的转录方式。选择性剪接可以增加转录组和蛋白质组的多样性，这是一种遗传和表观遗传调控的mRNA预处理。pre-mRNA的剪接高度准确，异常剪接可能导致疾病甚至癌症。许多研究表明深度学习可以准确预测剪接并对剪接类型进行分类。Leung等人使用 DNN 构建了一个模型，根据 RNAseq数据预测单个组织中的剪接模式。此外，研究人员还开发了基于 DNN的模型来预测不同背景下的剪接模式，包括使用基因组序列和表观遗传特征作为输入的发育和疾病。Jaganathan等人基于深度残差神经网络构建了 SpliceAI。该模型使用前 mRNA转录本的基因组序列作为输入。此外，其他研究人员还开发了一种仅使用局部 RNA序列基于人类内外显子选择性剪接行为的剪接分类方法 。</p><h3 id="转录因子结合位点的预测">转录因子结合位点的预测</h3><p>在基因表达中，转录因子（TF）发挥着重要作用。转录因子（TF）可以结合DNA 序列的特定区域并调节基因表达。 TF结合位点及其邻近突变对表达有很大影响，并可能导致一些复杂的疾病。 TF结合的详细分析对于进一步研究基因表达具有重要意义。近年来，机器学习尤其是深度学习方法在相关领域发挥了巨大的作用。先前基于位置权重矩阵（PWM）的模型存在诸如序列GC 偏差等问题。之后，基于机器学习和 ChIP-seq数据构建了一个模型来预测影响转录因子结合的调控变异。 Sherwood等人利用机器学习方法设计了 PIQ 模型，可以识别转录因子（TF）结合位点。DNase I超敏分析和测序（DNase-seq）实验可以确定700多个 TF结合位点，其准确性与 ChIP-seq相当。机器学习的应用极大地提高了模型的预测性能。此外，深度学习方法可以直接从海量数据中提取特征。基于CNN 开发的DeepBind 在预测 DNA 和 RNA结合蛋白的序列特异性方面具有更好的性能。然而，CNN训练的模型只关注当前状态，无法捕捉先前状态和未来状态对当前状态的影响，而RNN 可以有效地从时间序列数据中提取特征信息。Shen等提出了一种使用双向门控循环单元（GRU）网络的 KEGRU计算方法。该方法可以从 DNA 序列中提取特征信息，然后利用特征信息来预测 TF结合位点。该模型与基于 CNN的模型不同，可用于处理变长输入序列。此外，还使用机器学习方法开发了模型来描述TF 与染色质的特异性、活性和相互作用。</p><h3 id="使用转录组学进行辅助诊断">使用转录组学进行辅助诊断</h3><p>诊断是医疗的核心部分。医生在获取并解释患者信息后提供诊断结果。但这种手动方法费时费力，误诊概率较高。随着计算机辅助诊断（CAD）的出现，这些问题得到了显着改善。随后，机器学习算法特别是人工神经网络（ANN）的快速发展，极大地提高了诊断的准确性和效率。人工神经网络可以自学习、记忆和预测事件的发展。在疾病分类和诊断方面，人工神经网络比概率统计方法和数学模型等传统方法具有更好的性能。例如，如上所述，卷积神经网络（CNN）特别适合处理图像数据。此外，机器学习结合基因表达数据还可以对多种疾病起到很好的辅助诊断作用。例如，基于SVM开发的工具可以通过挖掘微阵列表达数据来预测肌病亚型。另一种深度学习模型可以根据基因表达数据系统、准确地预测药物性肝损伤。此外，它还被证明可以辅助诊断精神分裂症等精神疾病[98]和帕金森病（PD）等神经系统疾病。目前，已经从转录数据中鉴定出了一系列PD基因生物标志物。此外，大量研究证明，机器学习结合基因表达数据已广泛应用于癌症的辅助诊断。这些应用包括但不限于癌症分类、预测胰腺导管腺癌（PDAC）具有潜在治疗意义和预后的分子亚型、肝细胞癌的早期诊断和生存预测、预测癌症复发等。总之，机器学习不仅通过解释医学图像，还通过挖掘和分析基因表达数据来辅助疾病诊断。然而，数据不足仍然是一个常见的障碍。小规模数据训练的模型很难推广到其他项目。目前，许多数据库包含观察研究提供的遗传和临床信息。因此，开发新的机器学习方法来整合此类多队列数据可能很有价值。</p><h3 id="蛋白质组学中的机器学习">蛋白质组学中的机器学习</h3><p>蛋白质组学是指研究基因组中表达的所有蛋白质及其特性，主要包括蛋白质结构、蛋白质丰度、蛋白质活性、蛋白质修饰、蛋白质定位、蛋白质相互作用等。自人类基因组计划以来，各种生物医学数据量急剧增加。蛋白质组学的传统生化研究方法耗时耗力。基于机器学习的方法可以有效地处理大量蛋白质序列。然而，它很大程度上受到特征提取的影响。并且模型的最大性能是有限的。深度学习能够自动学习和分类抽象特征，为蛋白质组学的多领域研究提供了线索。</p><h3 id="在生物质光谱分析中的应用">在生物质光谱分析中的应用</h3><p>质谱（MS）是蛋白质组学中的一项关键技术，它利用样品离子的质荷比来分析蛋白质的成分和结构。质谱仪器发展迅速，但数据处理方式却没有跟上，这已成为蛋白质组学研究的瓶颈。机器学习，尤其是深度学习，可以解决蛋白质组数据的高维性和稀疏性。在生物量光谱中，机器学习在许多领域表现出色，例如从头测序、肽二级谱的碎片离子预测、肽性质预测、数据独立分析和质谱成像。传统上，发现未知序列的新蛋白质的一个重要方法是从头测序。但准确率仍不能令人满意。基于机器学习算法，DeepNovo被开发用于从头开始对肽进行测序。而且准确率远高于 de novo方法。此外，pDeep算法使用双向长短期记忆递归神经网络来预测肽片段的二级谱。另一方面，在液相色谱-质谱串联分析中，预测肽的保留时间具有重要价值。DeepRT就是为此目的基于深度学习而开发的。但特征识别的灵敏度并不是特别高。佐霍拉等人开发了基于卷积神经网络（CNN）的DeepIso，用于提取肽色谱和质谱特性。此外，CNN还用于质谱成像，对肺癌中的鳞状细胞癌和腺癌两组样本进行分类。综上所述，机器学习的应用对于提高肽数据检索具有重要意义。机器学习算法可以更准确地预测未知肽片段的序列。同时，深度学习算法在蛋白质组定量和表征过程中肽片段的质谱和色谱特性提取方面具有一定的应用潜力。</p><h3 id="筛选蛋白质生物标志物">筛选蛋白质生物标志物</h3><p>生物标志物在疾病筛查、监测、诊断、指导分子靶向治疗和评估疗效等方面发挥着重要作用。假设检验和回归分析等传统方法往往受到分类边界和变量相关性的限制。因此，它不适合现有的生物标志物发现策略。然而，机器学习方法没有这样的限制。在处理蛋白质生物标志物筛选任务时，无监督学习方法主要用于分析数据重复性、检查异常值、结果可视化以及检查标志物分离结果。而监督学习方法的主要功能是评估蛋白质生物标志物组合的分类效果。目前，质谱（MS）和机器学习等蛋白质组学技术的结合已被广泛使用，以进行完整的生物标志物筛选。An等人使用深度信念网络（DBN）来筛选阿尔茨海默病的蛋白质诊断标志物。他们最终获得的标记组包含20 个蛋白质，诊断准确率超过 90%。他们还发现 ACRP30蛋白与阿尔茨海默病有很强的相关性。Yan等人确定了三个关键生物标志物（乳酸脱氢酶、淋巴细胞和高敏 C反应蛋白）的水平，用于识别有 2019年冠状病毒病（COVID-19）风险的患者。然后他们建立了一个机器学习模型，可以至少提前10 天预测 COVID-19患者的个体死亡率。机器学习已被广泛用于评估蛋白质生物标志物分类的效果。但仍然存在一些挑战，如过拟合、黑盒、计算成本、模型选择等。但这并不妨碍机器学习在生物蛋白质生物标志物筛选中具有重要的应用前景。</p><h3 id="核酸结合蛋白预测">核酸结合蛋白预测</h3><p>核酸结合蛋白对多种生物过程具有重要意义。核酸结合蛋白的鉴定主要通过生化和标记技术进行。但识别的准确性和规模仍存在一定的局限性，且耗时长、成本高。最近，一些研究人员利用蛋白质特性（例如结构域序列）来预测核酸结合蛋白，但没有达到预期的准确性。目前，大规模数据集可以通过一些高通量测量获得，例如蛋白质结合微阵列（PBM）、指数富集配体的高通量系统进化（SELEX）和CHIP。由于高通量测量的可用性，机器学习在预测核酸结合蛋白方面比其他现有方法要准确得多。目前，已经开展了大量的计算研究，包括DNA结合域/蛋白质识别、DNA主题对发现、蛋白质-DNA或-RNA对接等。Alipanahi等人开发了基于 DeepBind的深度学习来预测蛋白质序列的核酸结合特性。同时，另一项研究发现支持向量机可以准确识别与核酸结合的残基，这有助于研究非特征蛋白与核酸之间的相互作用。然而，DNA和 RNA结合残基之间存在交叉预测的常见问题。未来需要付出更多努力来减少交叉预测，而机器学习必将为此提供强有力的支持。</p><h3 id="预测蛋白质-蛋白质相互作用">预测蛋白质-蛋白质相互作用</h3><p>近年来，蛋白质序列数据以惊人的速度积累，这有利于蛋白质-蛋白质相互作用（PPI）的检测。PPI 将有助于更好地了解蛋白质的功能和三维结构。PPI还为解释基因复制、转录、翻译、信号转导、细胞周期调节、免疫反应等一系列生理活动提供了有用的线索。然而，获得生物体中所有可能的二进制PPI 相关数据仍然很困难。一些公共数据库如 IntAct 和 BioGRID 可用于查询PPI，但大多数这些交互仍然不完整。而且，PPI 网络是动态的，实验 PPI数据缺乏组织特异性或特定条件（如健康和疾病状态），这使得 PPI相关研究变得更加复杂。最近，大多数研究人员认为使用计算方法来研究蛋白质-蛋白质相互作用非常有用。结合实验方法，通过机器学习可以有效地发现PPI。不同的数据通常需要不同的机器学习算法。用于PPI预测的两种流行的机器学习方法是随机森林分类器和支持向量机，而贝叶斯概率推理主要用于对高通量PPI数据集的置信度进行评分。基于机器学习，分层统计机械模型（HSM）被开发来预测PPI，它可以准确预测多个蛋白质家族之间肽结合域（PBD）-肽相互作用的亲和力。此外，深度学习也被广泛应用于PPI 预测。例如，基于领域的集成方法用于通过 DNN 预测 PPI。以及Hashemifar等人提出了一种深度学习模型DPPI，仅通过序列信息就可以对PPI进行建模和预测。</p><p>此外，蛋白质翻译后修饰（PTM）在蛋白质功能的调节中发挥着重要作用。PTM一般是指化学基团或蛋白质等各种官能团在氨基酸残基上的共价结合。作为生物功能调节的重要机制，PTM的识别和理解对于生物学和疾病研究至关重要。目前，已经开发了大量用于 PTM位点预测的机器学习方法。基于 SVM，Musite被提出来预测蛋白质磷酸化位点。它使用 K 最近邻 (KNN)评分、无序评分和氨基酸频率作为特征。此外，GlycoEP 可以通过 SVM 预测 N、O和 C 连接的糖基化位点。许多其他方法，例如 ModPred 和 DeepPhos 在预测 PTM位点方面也表现良好。此外，还开发了一些数据库来注释 PTM 位点，例如dbPTM、AWESOME 等。但是，大多数方法不支持大规模批量预测。Wang等人构建了一个新的网络服务器MusiteDeep，它以原始蛋白质序列作为输入，并使用 CNN 进行 PTM位点预测。它可以同时为多个 PTM提供预测和可视化。并且在准确性和速度方面都有一定的优势。此外，并非所有检测到的修改都是有效的。因此，研究人员很难确定应该在实验中进行哪些修改。最近，SAPH-ireTFx 被提出用于从大规模数据集中预测功能 PTM位点。对于确定具有生物学意义的 PTM 具有一定的指导意义。</p><h2 id="ml-in-metabolomics">ML in Metabolomics</h2><p>代谢组学是模仿基因组学和蛋白质组学的研究思路。它是定量分析生物体内所有代谢物，发现代谢物与生理、病理变化之间关系的研究方法。它也被认为是诊断高度异质性疾病的新技术。质谱、色谱和核磁共振获得的大规模代谢组数据的稀疏性往往对传统方法提出巨大挑战。机器学习算法适合处理此类数据并引起了人们的关注。各种机器学习算法[134]被用来解释这些大规模的代谢组数据。目前，机器学习在数据处理、代谢表型分层、代谢建模等一系列代谢组学研究中取得了令人瞩目的成就。</p><h3 id="数据处理与分析">数据处理与分析</h3><p>随着机器学习的快速发展和应用，代谢组学的数据处理和分析能力得到进一步提高。通过机器学习算法训练的模型用于模式识别（有时称为多元分类）。长期以来，偏最小二乘判别分析（PLS-DA）、极限学习、ANN、SVM等机器学习方法被应用于代谢组学数据分类。其中，PLS-DA一直占据主导地位。但目前的研究表明，SVM在代谢组学数据分类中的应用已经超越了PLSDA。SVM具有较高的预测精度和分类精度。与传统的基于回归的方法相比，人工神经网络开发的深度学习主要用于估计特定峰值的加权检测概率，并决定是否将其从最终数据集中排除[136]。研究表明深度学习还可以消除假阳性峰值。例如，DNN模型可以在不降低真阳性率的情况下消除 90%的假阳性峰值（噪声）。在代谢组学研究中，串联质谱 (MS/MS)通常用于识别“未知”代谢物（即参考光谱和/或结构信息不可用的代谢物）。然而，由于可用的MS/MS 参考光谱数量有限，仍然存在一些挑战。基于 DNN 的开源框架 DeepMASS可用于有效识别“未知”代谢物。此外，代谢组学数据处理需要足够的样本进行质量控制。并且数据处理和仪器性能的质量控制（QC）/质量保证（QA）也需要仔细检查。一般来说，深度学习方法可用于改进和自动化代谢组学分析的QC/QA 步骤。 DNN 可用于检测大规模非目标实验生成的稀疏MS。然后对数据进行归因并填补空白。然而，其准确性尚未得到评估，需要进一步努力。</p><h3 id="代谢表型的分层">代谢表型的分层</h3><p>代谢表型是指不同个体代谢过程和水平的特征。它可以通过代谢物的存在、代谢物的含量、代谢物之间的比例和代谢谱来描述。这一概念广泛应用于药物代谢组学。机器学习，尤其是深度学习，在代谢组学研究中表现出了优异的表现。它可以捕获代谢组学数据中复杂的代谢特征，从而提供有关受影响途径网络的推论。Inglese等人从人类结直肠癌活检中提取基于 MS 的成像数据集。然后他们使用 DNN结合参数化 t分布随机邻域嵌入来揭示癌症的代谢异质性。此外，一些研究表明，深度学习框架可用于基于代谢组学对乳腺癌雌激素受体（ER）状态进行分类。事实证明，预测精度高于其他机器学习方法，表明深度学习具有更好的揭示疾病生物学的能力。最近，提出了一种基于DNN的新方法。这种综合监督分类和回归技术可用于代谢表型分层和代谢物选择。而且，同样，该模型具有较高的分类精度。但其分类性能随样本量呈线性变化。其他人集成了多种DNN 分类器和统计方法，并开发了集成 DNN (EDNN)算法来提高分类和回归性能。</p><h3 id="代谢模型的基因组规模构建">代谢模型的基因组规模构建</h3><p>机器学习在代谢建模方面也取得了良好的发展和应用。主要用于建模过程中模型参数的确定、代谢网络分析和模型应用。基因组代谢模型（GEM）是一种数学模型，包括特定生物体或细胞基因组的代谢反应，以及酶和基因的关联。它为生物系统中的代谢通量建模提供了一个综合平台。并已广泛应用于人体新陈代谢的模拟。建模过程采用基于约束的定量建模方法。这种方法将生化和遗传信息结合在计算框架中。在建模过程中，机器学习经常用于优化参数并测试不同的输入条件。此外，越来越多的研究表明，机器学习与GSM相结合可以提高生物标志物（蛋白质/酶和代谢物等）的识别能力。这种组合还可用于量化代谢物通量、评估代谢物分泌、预测代谢基因和评估药物疗效。然而，机器学习在代谢组学中的应用远不止这些。应用还包括确定代谢相关药物副作用的预测因素、基于生成小分子碰撞截面（CCS）值的机器学习的预测以及疾病早期代谢标志物的识别。机器学习方法在代谢组学中具有巨大潜力。它可以捕获数据中复杂性状的代谢特征并完成分析和解释。然而，由于实验方法的限制、样本量小、缺乏可解释性以及普遍缺乏足够的参考，训练和验证数据仍然是挑战。</p><h2 id="ml-in-single-cell-omics">ML in Single-Cell Omics</h2><p>单细胞组学是在单细胞水平上研究基因组、转录组、蛋白质组和代谢组。通过这种单细胞水平的研究，研究人员可以更好地了解干细胞分化、免疫细胞功能、神经细胞发育和癌细胞病理等生物学和病理机制。此外，精准医学研究近年来已成为世界范围内的重点研究项目。单细胞研究重点解决生物异质性问题。因此可以在一定程度上指导疾病特别是一些重大和罕见病的治疗和预防和控制。因此，精准医疗发展的需求也需要加快单细胞技术的研发。单细胞转录组测序作为单细胞组学研究的关键技术之一，目前应用最为广泛。自唐等人以来。2009年完成了第一个单细胞RNA测序（scRNA-seq）工作，scRNA-seq不断得到改进。近年来，随着一些大规模商业平台的出现，scRNA-seq已经很容易实现。它已成为全细胞表达分析的常用方法。但scRNA-seq 数据的解读仍有很大的发展空间。目前，机器学习被广泛用于处理scRNA-seq 数据。例如，它可以执行去噪、丢失插补和批量效应校正。</p><h3 id="dropout-插补和批量效应校正">dropout 插补和批量效应校正</h3><p>单细胞 RNA 测序 (scRNA-seq)可以同时研究数万个单细胞转录本。因此，研究人员可以在单细胞水平上研究基因表达模式，从而更准确地研究细胞异质性。然而，由于当前技术的限制，scRNA-seq数据中往往存在大量噪声。在挖掘这些数据之前，通常需要进行去噪。机器学习方法在去噪方面表现良好。目前，已经开发了许多机器学习方法来对scRNA-seq数据进行去噪，并且更多的方法正在开发中。除了噪声之外，scRNA-seq数据通常包含过多的零，由于 RNA捕获率较低，这些零大多是"false"零。这种情况通常被称为dropout。另一个不可避免的混杂因素是由于样本之间技术操作的差异而产生的批次效应，例如测序仪的类型，甚至运行样本的技术人员[157]。在处理scRNAseq 数据时，dropout事件和批次效应给下游分析带来很大障碍。因此，dropout 插补和批次效应校正在scRNAseq 数据分析中几乎是不可或缺的。传统方法，如 scImpute 和svaseq，在丢失插补和批量效应校正方面是有效的。近年来，深度学习和机器学习算法已被有效地用于dropout插补和批量效应校正，并且具有更好的性能。Eraslan等人开发了基于深度学习的深度计数自动编码器（DCA）。 DCA 实现了一系列scRNA 序列特异性噪声模型来解决 scRNA-seq 数据中的插补任务。 LIGER是一种用于联合分析多个 scRNA-seq数据集的集成算法，可以有效消除批次效应。最近，我们的小组开发了一种基于生成对抗网络（GAN）的插补方法，称为scIGAN。我们将每个单个细胞的转录组重塑为灰色图像，并将丢失插补转换为图像修复。scIGAN 对于丢失插补非常有效，并增强了各种下游分析。 ScIGAN对于具有低表达和/或细胞间差异的基因很少的小型数据集具有鲁棒性。我们通过多种方式证明了scIGAN 不仅是 GAN 在组学数据中的应用，而且还代表了 scRNA-seq数据的一种竞争性插补方法。目前，有多种方法用于dropout插补和批量效应校正。然而，由于scRNA-seq数据集的不断扩展，许多数据集并没有提供完整的信息。另外，在校正批量效应时，可能会校正过度，从而丢失真正的底层数据结构。因此，我们期望更有效、更准确的dropout插补和批量效应校正方法。</p><h3 id="细胞聚类和轨迹推断">细胞聚类和轨迹推断</h3><p>scRNA-seq数据分析大致可分为细胞水平和基因水平，其中核心步骤是细胞聚类，以识别细胞类型。如上所述，许多机器学习算法在scRNA-seq数据的预处理中表现良好。同时，它们在下游分析中也很受欢迎。许多研究人员已经使用机器学习来实现细胞聚类和轨迹推断。Abdelaal等人使用 27 个公开的 scRNA-seq 数据集评估了 22个分类器，这些数据集具有不同的大小、复杂性、技术和物种。他们发现大多数分类器在各种数据集上都表现良好。特别是，SVM分类器总体表现最佳。此外，一些研究人员使用标记的单细胞数据集来训练 ANN模型。然后他们提出了一种具有良好性能的细胞聚类监督分类器技术框架。此外，聚类和批量效应消除是相互关联的。理想的消除批次效应有利于获得更好的细胞聚类。基于DNN，提出了一种无监督算法DESC。与Seurat3.0、scVI、BERMUDA等一系列先进的scRNA-seq 分析方法相比，DESC可以消除复杂的批次效应并保留细胞聚类中的生物学变异。</p><p>细胞类型识别后，轨迹推断将有助于我们研究不同细胞的动态分化过程。轨迹推断就是通过数据挖掘出细胞的连续状态，从而计算推断细胞的发育轨迹。它为研究细胞周期、细胞分化和细胞激活等细胞动态过程提供了新的机会。在过去的几年里，基于不同算法的大量轨迹推理方法被开发出来，例如Monocle、Waterfall、CellTree、scTDA 和URD。可以根据数据的特征，例如数据集的大小和轨迹拓扑，选择最合适的轨迹推断方法。而且，这些现有方法具有很强的互补性。我们可以使用多种方法来确认轨迹假设和相关的下游分析。然而，大多数这些工具经常低估或高估底层拓扑的复杂性。最近，机器学习在数据预处理后的轨迹推断方面取得了良好的效果。结合潜在的分层混合模型和变分自动编码器，提出了一种概率方法VITAE（Variational Inference for Trajectory byAutoEncoder），用于从后验近似推断轨迹。该模型可以调整混杂协变量以集成多个数据集。</p><p>除了单细胞转录组测序数据的处理和分析之外，机器学习还可以与单细胞质谱结合。它可以有效预测细胞表型，例如耐药癌细胞的表型。此外，机器学习在流成像技术和微流控芯片方面也得到了很好的发展。机器学习与单细胞组学研究的结合可以提供更全面、更准确的生理病理机制。可以预见，这将推动精准医疗的蓬勃发展。</p><h2 id="ml-in-radiomics">ML in Radiomics</h2><p>放射组学是指通过放射线照相技术（例如计算机断层扫描（CT）、磁共振成像（MRI）和正电子发射断层扫描（PET））高通量采集大量医学图像。随着医疗和信息技术的不断提高，各种医疗数据也在迅速积累。尤其是医学图像的输出量极高。图像数据包含大量反映人类健康的潜在信息。然而，这些数据的人工分析和处理效率低下，并且容易受到主观因素的干扰。功能强大的机器学习可以有效缓解这些缺点。它可以提取更高级的特征，同时消除主观因素的影响。深度学习方法，尤其是卷积神经网络（CNN），在图像数据方面表现出特殊的能力[180]。目前，机器学习算法与医学图像（如CT和PET）相结合已广泛应用于医学图像，如疾病诊断、疾病分类和病灶识别。</p><h3 id="疾病的诊断和分类">疾病的诊断和分类</h3><h3 id="病灶识别">病灶识别</h3><h2 id="ml-in-multi-omics">ML in Multi-Omics</h2>]]></content>
    
    
    <categories>
      
      <category>Bioinformatics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Bioinformatics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文研读-A Comprehensive Survey of Few-shot Learning Evolution, Applications, Challenges, and Opportunities</title>
    <link href="/2023/11/08/lun-wen-yan-du-a-comprehensive-survey-of-few-shot-learning-evolution-applications-challenges-and-opportunities/"/>
    <url>/2023/11/08/lun-wen-yan-du-a-comprehensive-survey-of-few-shot-learning-evolution-applications-challenges-and-opportunities/</url>
    
    <content type="html"><![CDATA[<p>这周主要是看的有关 Few-shot learning 和 Meta-Learning有关的文章，对《A Comprehensive Survey of Few-shot Learning: Evolution,Applications, Challenges, and Opportunities》做一下总结。</p><h2 id="few-shot-learning">Few-shot Learning</h2><p>把包括大量样本的类别称作 seen classes，记作 <spanclass="math inline">\(\mathcal{Y}_s\)</span>，把仅包括少量 few-shot样本的类别称作 unseen classes，记作 <spanclass="math inline">\(\mathcal{Y}_u\)</span> 。</p><p>给定一个训练集 <span class="math inline">\(\mathcal{D}_{tr}=\{(x,y)|x\in \mathcal{X}_s, y \in \mathcal{Y}_s \}\)</span>，其中 <spanclass="math inline">\(\mathcal{X}_s\)</span> 和 <spanclass="math inline">\(\mathcal{Y}_s\)</span>分别是训练样本的输入和类别，一个 few-shot 样本集 <spanclass="math inline">\(\mathcal{D}_{few}=\{(x,y)|x \in \mathcal{X}_{few},y \in \mathcal{Y}_u \}\)</span>， 其中 <spanclass="math inline">\(\mathcal{Y}_u \bigcap \mathcal{Y}_s =\emptyset\)</span>，在 <spanclass="math inline">\(\mathcal{Y}_s\)</span> 中的每一个类别在 <spanclass="math inline">\(\mathcal{D}_{tr}\)</span>中都有大量的样本，每一个在 <spanclass="math inline">\(\mathcal{Y}_{u}\)</span> 的类别仅在 <spanclass="math inline">\(\mathcal{D}_{few}\)</span> 有少量样本。 FSL 用<span class="math inline">\(\mathcal{D}_{tr}\)</span> 和 <spanclass="math inline">\(\mathcal{D}_{few}\)</span> 训练分类器 <spanclass="math inline">\(f\)</span> 预测在测试集 <spanclass="math inline">\(\mathcal{D}_{te}=\{(x,y)|x \in \mathcal{X}_{u}, y\in \mathcal{Y}_u \}\)</span> 中的样本，其中 <spanclass="math inline">\(\mathcal{X}_u \bigcap \mathcal{X}_{few} =\emptyset\)</span>。</p><p><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20231110154140824.png" alt="image-20231110154140824" style="zoom:30%;" /></p><p><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20231110154237671.png" alt="image-20231110154237671" style="zoom:30%;" /></p><p>FSL 任务通常用 N-way-K-shot 表示。分为 Meta-training 和 Meta-testing两个过程。每类的样本数量对模型性能有显着影响。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20231110160920290.png"alt="image-20231110160920290" /><figcaption aria-hidden="true">image-20231110160920290</figcaption></figure><p>先在 Testing data中选择几个类，每个类别都包括很多样本，然后从选择的每个类中选出 K+X个样本，其中 K 个样本用作 support set，X 个样本被用作 queryset，这样就得到了 Meta-testing 过程中需要用到的两个数据集。N-way k-shot中的 N 和 k 是由 Testing data决定的，与 Training data 无关。同理，可得到Meta-training 过程中的两个数据集 S 和 Q，但从每个类中也不必非要选 k个样本作为 Support set，现在可以选任意个。接下来依据该数据集划分进行训练，计算损失，更新参数。在 Meta-training的过程中，Support set + Query set 就是一个 mini-batch。因此，一个episode 的训练过程实际上就是一个 batch 的训练过程。而这种基于 episode的训练方法即被称为 Episodic Training。</p><h3 id="与-transfer-learning-的关系">与 Transfer Learning 的关系</h3><p>与 Transfer Learning 相比，有限的训练数据、领域变化和任务修改使得 FSL更具挑战性。 FSL 有很多变体，包括 One-shot Learning（OSL）、Zero-shotLearning（ZSL）和 Cross-domain Few-shotLearning。这些变体在样本大小和领域学习方面被认为是迁移学习的特殊情况。</p><ul><li><p>One-shot Learning：One-shot Learning 在 support datasets中每个类都有一个可用样本。模型只需要根据 support set回答是或否。事实上，One-shot Learning并没有专门对数据进行分类，只是简单地做一个聚类来学习相似度度量函数。</p></li><li><p>Zero-shot Learning：Zero-shot Learning 考虑了扩展 FSL的更极端情况。在没有任何 support 样本的情况下，Zero-shot Learning完全依赖语义特征作为桥梁来推断未见过的样本。零样本学习使用高维语义特征而不是低维像素特征来表示原始数据。One-shot Learning 和 FSL 可以看作是一种特殊的 Zero-shotLearning。</p></li><li><p>Cross-domain Few-shot Learning：Cross-domain Few-shot Learning结合了 Transfer Learning 和 FSL 的挑战。由于域 gap的存在，源域和目标域中的类的交集为空。此外，目标域的样本数量极其稀缺。</p></li></ul><h3 id="与-meta-learning-的关系">与 Meta Learning 的关系</h3><p>Few-shot learning 是 meta learning 的一种。</p><p>Few-shot learning更加关注<strong>如何利用已有的信息</strong>来完成新任务的学习，而 Metalearning更加关注如何设计一种<strong>可以快速适应新任务</strong>的算法或模型。</p><p>两者之间的关系十分混杂，暂时这样理解。</p><p>Meta Learning 是提供 episodic training的通用范例。它的重点是在先验知识的帮助下提高未见过的任务的泛化能力。如果使用先验知识来辅助模型学习特定任务，那么元学习可以被视为FSL 的变体。Meta Learning 并不等同于 FSL。 FSL更多的是一个最终目标，旨在在不依赖大规模数据集的情况下实现稳健的表示。通过数据和任务空间的双重采样，MetaLearning 能够构建大量与未见过的任务相关的辅助任务。即使有些工作不涉及Meta Learning，如果可以考虑 episodictraining，也可能会提高性能，例如元强化学习、元视频检测等。</p><p>元学习有其自身的局限性：当训练和测试任务存在明显的领域差距时，元学习很少初始化参数权重。此外，元学习高度依赖于网络结构，需要针对广泛不同的任务进行重新设计。尽管如此，元学习仍然是解决FSL 问题最有效的方法之一。</p><h2 id="数据集">数据集</h2><h2 id="few-shot-learning-分类">Few-shot Learning 分类</h2><p>Few-shotlearning（FSL）已成为一种有效的学习方法，并显示出巨大的潜力。尽管最近在处理FSL任务方面取得了创造性的成果，但从少量甚至零样本中快速学习有效信息仍然是一个严峻的挑战。作者调研了200+ FSL 的文章，根据对先验知识的抽象层面将 FSL 的工作分为数据级（datalevel），特征级（feature level），任务级（tasklevel）和多模态级（multimodal level）。</p><ul><li>数据级<ul><li>定义：数据级别主要通过增加特征或样本的数量来评估真实的数据分布。最直接的方法是基于语义空间生成附加数据或标记类似的辅助标记数据集。对于半监督数据集，对比学习和潜在增强是有效的表示框架；</li><li>数据分布评估不准确挑战：在FSL中，由于成本、伦理、法律或其他原因，很难对大量半监督数据集进行标记。仅依赖少数样本可能会在估计真实数据分布时产生偏差，这对某些任务是有害的。在数据层面，最大限度地探索稀缺信息的数据分布是最重大的挑战。</li></ul></li><li>特征级<ul><li>定义：特征级别主要用于构建从支持集到查询集的数据到标签的映射。良好的特征嵌入对于提取判别性表示至关重要。大多数预训练和微调工作需要通过有效的正则化来应用。另外，如果最大化保留预训练模型的参数，那么提示调优通过给模型一些手动提示，很大程度上打破了数据的约束；</li><li>特征重用敏感性挑战：通过积累大规模数据标签领域知识，迁移学习可以利用预训练的模型轻松获得特征级先验知识。然而，就跨域FSL而言，预训练的特征提取器没有足够的泛化性，这导致了对未见过的任务的误导。</li></ul></li><li>任务级<ul><li>定义：任务级主要用于细化任务空间中的参数，包括模型参数和元学习参数。与多任务学习不同，元学习试图从相关任务中学习先验知识，而不扩展学习参数或牺牲推理效率。元学习、度量学习和图神经网络是此处理过程中的主要方法；</li><li>未来任务的通用性挑战：以元学习和情景训练为代表，任务级方法可以对数据和任务进行双重采样，并尽快从已见的任务映射到未见的任务。然而，最近的研究表明，只有当训练和测试任务足够相似时，元学习才有效。此外，元学习高度依赖于现实世界各个领域下的网络结构。</li></ul></li><li>多模态级<ul><li>定义：多模态级别使文本、视觉效果和其他消息能够以最小的损失嵌入到同一空间中。在语言模型的帮助下，图像也可以以补丁的形式嵌入。多模态学习包含丰富的知识，导致FSL进入多模态大模型+小样本领域；</li><li>多模态信息融合的有效性挑战：多模态学习可以实现与图像、文本和音频等环境的交互。它结合了图像嵌入和文本向量，表现出了很强的通用性。参数化多模态预训练模型可以轻松解决各种下游任务。然而，如何以最小的损失将文本、图像和其他消息嵌入到同一空间中仍然是多模态FSL 的核心挑战。</li></ul></li></ul><h2 id="数据级">数据级</h2><p>FSL 旨在每类仅使用少量甚至零样本来学习新类别。在现实世界的 FSL任务中，由于隐私、收集和标签成本的原因，用于训练的样本数量始终受到限制。为了解决数据稀缺的问题，一个直接的方法是增加可用样本的数量。根据先验知识级别的抽象，将数据增强细分为数据扩展（Dataexpansion）和特征增强（Feature Augmentation）。</p><h3 id="数据扩展">数据扩展</h3><p>数据扩展主要是在原始训练数据集中添加新的可用数据，或者作为基于未标记数据生成的伪标记数据或对原始数据集进行变换。</p><p>其中应用最广泛的方法是对图像进行各种规则变换，涉及翻转、高斯模糊、裁剪、缩放等。然而，像素信息对FSL的改进有限。一些工作尝试叠加语义信息、补充擦除区域以及转换类间或类内变化。早期的工作是在类似的类别中使用Hausdorff距离度量。后来，使用自动编码器来捕获类属性的线性或非线性变化，例如空间和外观。就自然语言处理而言，数据扩展主要包括简单的回译、单词替换、释义和信念状态注释。无论图像还是文本，相应的标签都会分配原始标签。最近，反转标签也可以发现在特殊任务上的性能会得到提高。</p><p>与在原始数据集上生成更多样的样本相比，更好的方向是引入大量未标记的数据集。在手势识别方面，在一个大型的未标记手势库中共同训练相似的手势。在许多其他情况下，模型无法立即完成对未标记数据的选择。整个过程需要不断的训练、推理、增加。然而，如果未标记数据和标记数据之间存在域转移，则生成的伪标签可能会带来来自目标类的噪声。许多工作被用来生成更好的伪标签，包括融合数据分布、对比学习和嵌入增强。</p><p>除了未标记的数据集之外，query sets 也可以在 FSL中一起训练，这被称为直推式少样本学习（transductive few-shotlearning）。通过在训练阶段添加测试数据，可以获得更具辨别力的全局表示特征。STARTUP提出在未标记的 query datasets上训练具有对比损失函数的模型，并将模型结果的聚类作为这些未标记的查询数据集的标签。如果域间差异很大，那么使用固定的预训练模型可能不是最佳选择。Islam等人放弃对比度损失函数并使用动态蒸馏来学习目标域的更好表示。目前，基于转导学习的FSL与元学习任务紧密结合。</p><h3 id="特征增强">特征增强</h3><p>特征增强是更高级别的数据增强，专注于一系列特征转换。在特征增强中，样本通过特征提取器，有效特征被进一步压缩为纹理、位置或属性。Laso是一个早期且具有代表性的工作。由于每个样本具有多个标签，通过定义特征空间的交集和差集，可以在hidden 空间中生成更多相关样本。PFEMed将预训练的一般特征与特定特征连接起来，以进一步增强 FSL特征的语义信息。Chen等人建议添加一组参考图像，其中包含许多同一类别的图像对。在嵌入空间中减去或添加参考对的特征。它不仅丰富了多样性，还引入了参考特征。此外，AFHN探索使用条件生成对抗网络来产生更多种类的识别特征。最近，提出了一个假设，当基类和新类在语义上相似时，均值和方差可以在很大程度上共享。基类的均值和方差可以校正新类的数据分布。此外，Xu等人提出了一种分解数据集中方差的框架，其中一个表示类内方差，其他表示判别信息的嵌入。通过重复采样，可以将类内方差添加到判别性特征中。这样就可以最大程度地学习到特征，同时保持较大的类内方差。</p><h2 id="特征级">特征级</h2><p>深度学习模型表征特征的能力远远超出了人类，拥有大规模的数据集和强大的计算能力。FSL利用特征知识来不同程度地共享预训练参数并重新定义下游任务。在本节中，按照先验知识的方向，将相关工作总结为迁移学习（transferlearning）和多任务学习（multitask learning）。</p><h3 id="transfer-learning">Transfer Learning</h3><p>从迁移学习的角度来看，FSL可以被视为跨领域的学习任务。FSL中的标准迁移学习也可以分为两个阶段：在源任务上学习和在目标任务上迁移。</p><h4 id="pre-training">Pre-training</h4><p>FSL 利用预训练模型的特征表示能力来很大程度上减少类内variation，这使模型专注于更具辨别力的区域。实验证明，监督训练方法会扭曲不同类的实例并忽略语义信息。相反，无监督训练侧重于调整下游任务的特征。</p><h4 id="fine-tuning">Fine-tuning</h4><p>经验表明，仅仅重新随机化顶层参数和相同架构知识的自蒸馏也对 FSL有利。最近，许多作品探索了有用的技巧，包括自监督学习、注意力模块和掩模生成模块。显然，P&gt; M &gt; F总结了预训练、元学习和微调的训练过程。它在多个基准数据集上达到了 FSL的最先进水平。其中，特征提取 backbone 是 FSL 性能的主导因素。</p><p>同样，在自然语言处理中，预训练和微调也大大加速了 FSL的发展。其中，BERT 主要用于语言理解，GPT用于文本生成。语言模型在大型语料库上训练不同的任务，因此它具有理解甚至生成目标文本的能力。Zhang等人探索微调BERT 的主要技巧，包括纠错、权重初始化、L2 正则化和冻结。然而，Lee等人微调 GPT-2 并在 automatic patent assertion任务上实现最先进的技术。除了性能之外，微调的最大优点是不需要任务级模型结构设计。</p><h4 id="prompt-learning">Prompt Learning</h4><p>根据 prompt 的位置，Prompt Learning 可分为前缀提示（prefixprompt）和完形填空式提示（cloze-styleprompt），具体来说，前缀提示表示后续文本与不同任务对应的前缀词组合。完形提示是指在句子的空白处填入合适的单词。不同的任务可以有不同的空白位置来测试模型是否学习了相应的语义知识。</p><p>手动提示模板的一个显着问题是 prompt的质量直接影响模型的性能。Jiang等人在大量文本预期中收集一些转换作为模板。新的任务可以在海量的经验中寻找模板。另一种方法是prompt paraphrasing。在 seed prompt的基础上，通过回译和关键词替换将其扩展到更多的提示。最具代表性的工作是AutoPrompt，它屏蔽模板中的单词，自动用其他合适的单词替换它们，以最大化标记的概率。Gao等人使用 T5模型进行训练和比较，生成空白的每个位置。整个过程的计算量相对较大。</p><p>更进一步，一种更抽象的提示类型是提示隐藏空间。输入可以是嵌入向量，输出不再是具体的单词。最有代表性的方法是PT，它在正常输入的前面添加了几个前缀ID。除初始化参数外，其他参数只有在调整前缀时才会被冻结和更新。受这个想法的启发，CP-Tuning使用连续的提示嵌入来代替提示模板的端到端手动设计。随后，一些作品开始探索如何在隐藏空间中初始化提示。PPT指出，通过在预训练阶段添加软提示可以获得更好的初始化。另一种方法是使用提示作为初始状态，并在微调过程中找到更合适的潜在提示。</p><h3 id="multitask-learning">Multitask Learning</h3><p>与 Transfer Learning 相比，Multitask Learning需要多个损失函数来优化模型。扩展任务倾向于在嵌入空间中向不同方向进行更新，这在一定程度上抵消了一些噪声。在FSL多任务工作中，参数更新可以分为硬参数共享和软参数共享。</p><p>据我们所知，FSL Multitask Learning最初应用于视频事件检测任务。在特征表示中，通过控制参数更新的位置，多任务学习可以最大程度地混淆共享层参数。这种分离特征的操作最大化了通用特征和判别特征的学习之间的平衡。通过添加几个判别性属性、消除与属性无关的特征以及使用与属性相关的特征进行指控预测。多任务学习也可用于数据增强。FGVC提出在预训练任务中添加样本选择任务。每个训练分类器共享一个基础网络以获得更好的结果。</p><p>遵循这个过程，FSL Multitask Learning很长一段时间都与自监督学习联系在一起。具体来说，BF3S 将 FSL分类任务与预测图像的旋转角度相结合。同样，预任务也用于对图像进行着色并预测每个补丁的相对位置，局部Fisher 判别式。大量实验表明，预测图像块的旋转角度和相对位置是 FSL更有效的方法。 PSST 进一步结合了 FSL分类、图像位置预测和角度预测以实现最先进的技术。直觉上，通过解决这些任务，backbone网络提取更多语义特征，而不仅仅是相关标签。由于 Meta Learning与精心设计的 Multitask Learning相结合，可以获得有竞争力的结果并将训练时间减少 10 倍。</p><h2 id="任务级">任务级</h2><p>任务级别与数据级别和特征级别不同，它对数据和任务进行双重采样以提取元知识。Meta-knowledge独立于具体问题，在任务空间中搜索最优参数。从广义上讲，任务级别学习优化参数、生成度量函数并总结知识迁移。其中，学习优化参数还包括优化元学习参数和优化现有模型参数；生成度量算法包括特征嵌入、基于外部memory、度量学习、图神经网络和其他基于相似性的算法。</p><h3 id="learning-optimization-meta-learned-parameters">LearningOptimization Meta-learned Parameters</h3><p>学习元学习参数的关键思想是搜索对未见过的任务通用的全局初始化状态。传统的初始化方法，如均匀分布、正态分布等，很容易陷入局部最优。考虑到FSL的特殊性，MAML（模型无关元学习）提出了一种基于episodic training 的新方法。 MAML是典型的两阶段参数模型。第一阶段是更新每个任务，即本地更新。第二次更新是每批次任务查询损失的平均值，即全局更新。计算量是最大的问题。为此，大量工作旨在改进MAML，包括simplifying、动量更新，甚至忽略二阶导数，使用进化算法来避免反向传播以在CPU 并考虑梯度更新方向和学习率。</p><p>另外，相关的初始化参数也可以由其他模型生成。 AWGIM基于低维引入互信息和注意力，生成包含更多 query 信息的权重。此外，MAML与概率分布相结合，得出 LLAMA、PLATIPUS、贝叶斯 MAML、DKT 和ABML。在扩大参数空间方面，MT-NET 添加了变换矩阵和二值掩码矩阵； TAML引入正则化条件； OOD-MAML 利用虚拟样本； ARML 构建元知识图； WGD-MAML在学习层之间放置非线性激活函数。最近，METADOCK通过动态选择内核来压缩元模型。它可以轻松部署在边缘设备上。</p><p>除了优化初始化参数之外，另一个重要方向是 model-based 机制。它需要外部memory 以 key-value 对的形式存储先验知识。key 存储模型嵌入的输出，value存储各种标签。据我们所知，MANN 是最早使用外部 memory 来显式存储每个episode 特征嵌入的工作。Kaiser 等人扩展 key-value存储模块使用三元组进行保存。额外的向量用于选择一个长时间没有更新的向量。APL进一步将基于模型的知识总结为概率分布。与应用静态内存矩阵的现有工作不同，DMIN使用动态内存路由来学习动态内存模块。在一些极端跨域的场景中，memory模块中的 key 不能简单地推广到语义，而应该是多层的。</p><p>受 MAML 和 Reptile的启发，网络架构搜索也可以在几轮更新中实现快速收敛。共享或随机选择的surpernet 权重是早期工作被称为 one-shot NAS。但 one-shot NAS的性能仍然相对落后于传统 NAS。随后，Zhao 等人提出了 few-shotNAS。其核心思想是将 surper-networks 划分为多个 sub-supernets来搜索待搜索空间的不同区域。由于 surpernets 数量略有增加，few-shot NAS的准确率得到了大幅提升。 MetaNAS 是第一个将元学习和传统 NAS完全结合的方法。它取代了 DARTS的加权求和以减少不同的操作。经验表明，MetaNAS 更适合下游学习任务。</p><h3 id="learning-metric-algorithm">Learning Metric Algorithm</h3><p>学习度量算法旨在学习计算 support-query对之间的抽象距离的映射。一般来说，度量学习也可以被视为整合了学习学习思想的相似性度量的学习。</p><p>孪生神经网络（Siamese neural network）是比较早期的模型。输入由一组正对或负对组成，输出只是一个二元分类问题。基于孪生神经网络，更多的工作考虑添加锚样本，甚至更多样本，并选择硬样本。孪生神经网络是一个通用框架。当backbone 模型不共享权重时，孪生神经网络将成为伪孪生神经网络。</p><p>与孪生神经网络相比，基于特征平均的原型网络（Prototypenetwork）实现了真正意义上的分类。但简单地平均特征很容易受到噪声的干扰。一些工作提出了相似类别和不相似类别之间不同类型的自适应边界距离。除此之外，选择图像的特定部分来强制高阶语义特征的交互可以指导模型学习新度量空间中的原型表示。APLCNE 是最具代表性的工作，它用胶囊网络（Capsule network）代替 CNN来编码空间位置信息。原型是使用加权求和来计算的。如果样本更加稀疏，那么可以使用正则化来约束不同模态的原型长度，并引入注意力来重建原型表示。</p><p>匹配网络（MatchingNetwork）结合了参数和非参数算法来对距离分布进行建模。最常用的度量包括欧几里德距离、曼哈顿距离和余弦相似度。最近，DEEPEMD首次提出了一种与 Earth’s Mover距离的交叉引用机制。它使用节点特征与另一个结构中的平均节点特征的点积来生成相关性分数作为权重值。DEEPEMD 可以应用于遥感场景。深布朗距离协方差（Deep Brownian DistanceCovariance）是另一种较低计算方法，它通过测量嵌入特征的联合分布与边缘分布的乘积之间的差异来学习图像表示。实验表明，布朗距离协方差在FSL 度量学习中具有巨大的潜力和广泛的应用。</p><p>关系网络（Relationnetwork）与上述模型的不同之处在于，其相似度是使用余弦相似度、注意力模块和图神经网络相结合的神经网络来计算的。其中，图神经网络被用来解决一大类FSL 问题。最早的工作之一是GNN-FSL，它将样本的向量表示输入到图神经网络中。在此基础上，通过设计不同的嵌入节点和标签传播机制，图神经网络可以很好地对support-query 对之间的关系进行建模。</p><h2 id="多模态级">多模态级</h2><p>尽管一些工作为 FSL 的特定任务提供了出色的结果，但 FSL仍然很难利用更广义的先验知识取得重大突破，例如 BSCDFSL。如今，由于前缀tuning 和提示 tuning 的有效性，FSL 在 Zero-shot learning中表现得令人惊讶。</p><p>CLIP 给出了多模态训练的基本思想：文本 backbone 和视觉 backbone的联合训练。基于 CLIP，CADA-VAE 使用 VAE作为主干，将图像特征和标签文本映射到相同的潜在空间。Wang等人将图像分解为原始图像、前景图像和背景图像，通过 CNN backbone获得融合的视觉特征。Li 等人应用无监督聚类来获取层级语义信息，Schwartz等人使用多个 MLP 层通过文本 backbone 提取带有标签向量的语义原型。Peng等人利用 GCN将类别的语义信息生成相应的分类权重，并利用视觉分类权重结合语义生成权重完成FSL 分类器。Xing等人将文本向量与视觉特征一起添加到原型网络中以训练自适应分类。Pahde等人使用语义生成来辅助视觉特征。 CCAM通过对上下文原型进行编码来替换真实标签，并通过比较各个原型之间的距离来对它们进行分类。使用各个领域的backbone网络嵌入训练起来很简单，但是良好的语言模型和视觉模型共同训练会在很大程度上损害语言模型的性能。</p><p>Multimodal prompt learning允许视觉嵌入通过提示或添加前缀来一致地适应语言模型，这在很大程度上保留了语言模型强大的特征表示能力。MCNLG的提示非常直观，它将多模态序列作为前缀提示放置在输入序列之前，然后依次解码共享的多模态信息。视觉backbone 是 ResNet-152，文本 backbone 是嵌入网络。 ActionCLIP 是 CLIP在动作识别场景中的应用。整个训练过程可以表述为预训练、提示学习和微调，其中提示用于保留更多多模态预训练模型的稳健表征性能。具体来说，文本提示包括前缀提示、完形填空提示和后缀提示。ALPRO 遵循 CLIP手动模板，其主要贡献是使用对比学习来进一步调整嵌入空间中的各个模态。 CPT是一种颜色提示调优，通过构造相应的提示模板，使用不同的颜色模块来区分实体。CoOp与之前的手动提示模板不同。标签嵌入可以选择放置在生成的自动化虚拟模板的中间、前面和后面。最近的一项里程碑式的工作帮助语言模型理解图像，其核心思想是将视觉特征与文本特征对齐。语言模型参数在训练阶段完全冻结，仅在反向传播期间在视觉编码器中更新。冻结的语言模型可以看作是前缀提示。</p><h2 id="fsl的未来方向和机遇">FSL的未来方向和机遇</h2><h3 id="更好地评估数据分布">更好地评估数据分布</h3><p>机器学习很难从极少数的样本中训练出具有优异泛化能力的模型。如果算法是在带有偏差的数据上进行训练的，那么它将破坏模型的泛化能力。Yang等人在这个方向上做出了第一次有意义的尝试。它假设基类数据和新类数据都服从正态分布。通过计算基类的均值和方差，可以将值转移到新类。如果这个假设足够准确，那么它可以在一定程度上弥合FSL和传统机器学习之间的差距。然而，这仍然是对数据分布的一个相对严格的假设。如果基类和新类之间存在较大差距，则需要使用关系网络等辅助模块来探索足够复杂的校正规则。未来，这是一个令人兴奋的方向，考虑放宽假设并探索更多的泛化方法。</p><p>另一个问题是主流的FSL基准数据集或多或少存在一些问题。例如，mini-ImageNet数据集获得了一些对于模型评估来说太困难的不合适的样本，例如实体遮挡和多个对象。然而，其他简单的数据集，例如Omniglot、cifar10、cifar100 等，已经得到了很大程度的解决。 FSL需要更具挑战性的数据集，例如BSCD-FSL。同样，自然语言处理和多模态领域也需要更多像 FEWCLUE、FEWGlue这样与研究和应用热点紧密结合的基准数据集。但到目前为止，还没有基准数据集来评估模型在细粒度和全场景级别的泛化能力。</p><h3 id="提高数据到标签映射的稳健性">提高数据到标签映射的稳健性</h3><p>BSCD-FSL 基准的出现给 FSL 带来了新的挑战。它探索并揭示了当前 FSL对于跨领域学习的局限性。最近的研究已经产生了精心设计的模型、更复杂的超参数调整、额外的辅助数据集以及对FSL有效的与领域无关的特征的提取。目前，微调在迁移学习和元学习的交叉点上已经表现得非常强劲。预训练可以看作是学习多类任务；这是单任务学习。然而，元学习是一种多任务学习方法。是否有更好的模型能够将元学习和微调结合起来，最大限度地提高模型的性能，同时降低元学习过程中的计算复杂度，值得探索。</p><p>如今，特征层面对于参数空间优化仍然具有很大的潜力。 P &gt; M &gt; F探索了这个简单的流程，包括预训练、元学习和微调。它在 mini-ImageNet上实现了最先进的 FSL 分类任务。后续工作继续尝试替换预训练的backbone，在微调中添加各种技巧，或者引入多任务以充分利用微调和元学习的性能。</p><h3id="从历史任务中更有效地学习元知识">从历史任务中更有效地学习元知识</h3><p>元学习是一个非常普遍的概念。元学习与其他方法结合使用可以进一步提高FSL任务的性能。然而，元学习仅限于定义的网络结构下的特定任务空间。对于分类任务，目前只考虑相关的分类任务。是否存在一个同时考虑分类、检测、预测和生成的框架？这将使元学习在某种程度上与任务概念分离。最近的工作正在尝试对每个批次进行整体优化。在这种情况下，如何优化内循环将是一个重要的方向。未来，元学习和微调的结合将成为解决FSL的主流算法。</p><p>请注意，元学习仍在探索任务之间的相关性。目前尚未出现相关理论来解释元学习背后的因果关系。未来，随着因果理论框架的发展，元学习可能会成为一个更通用的框架。</p><h3 id="多模态信息全面融合">多模态信息全面融合</h3><p>多模态学习是目前解决FSL问题的新兴方法，它自动从没有监督标签的边缘场景中学习异构数据，并快速转移到不同的下游任务。多模态学习被广泛认为是从有限领域的弱人工智能到通用人工智能的探索。多模态FSL的出现，让FSL进入了预训练多模态模型+小样本微调的时代。从技术上讲，具有足够数量参数的预训练多模态模型可以解决促销方面的任何下游任务。现阶段，鼓励研究人员尝试更多模态融合学习，例如语音和视频。另一个方向是如何在多种信息融合的情况下量化每类信息的重要性，以便在训练阶段获得不同的权重。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Few-shot Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（三）晏几道篇</title>
    <link href="/2023/09/24/song-ci-san-yan-ji-dao-pian/"/>
    <url>/2023/09/24/song-ci-san-yan-ji-dao-pian/</url>
    
    <content type="html"><![CDATA[<center>临江仙</center><center>梦后楼台高锁，酒醒帘幕低垂。去年春恨却来时，落花人独立，微雨燕双飞。</center><center>记得小初见，两重心字罗衣。琵琶弦上说相思，当时明月在，曾照彩云归。</center><p><br></p>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex &amp; Markdown 空格</title>
    <link href="/2023/09/17/latex-markdown-kong-ge/"/>
    <url>/2023/09/17/latex-markdown-kong-ge/</url>
    
    <content type="html"><![CDATA[<table><thead><tr class="header"><th>空格类型</th><th>示例</th><th>文章中空格绝对大小</th></tr></thead><tbody><tr class="odd"><td>两个 quad 空格</td><td>a b</td><td>两个 m 的宽度</td></tr><tr class="even"><td>quad 空格</td><td>a b</td><td>一个 m 的宽度</td></tr><tr class="odd"><td>大空格</td><td>a b</td><td>1/3 m 宽度</td></tr><tr class="even"><td>中等空格</td><td>a;b</td><td>2/7 m 宽度</td></tr><tr class="odd"><td>小空格</td><td>a,b</td><td>1/6 m 宽度</td></tr><tr class="even"><td>没有空格</td><td>ab</td><td></td></tr><tr class="odd"><td>紧贴</td><td>a!b</td><td>缩进 1/6 m 宽度</td></tr></tbody></table><h2 id="参考">参考</h2><p>[1] <ahref="https://blog.csdn.net/kangkanglhb88008/article/details/125547204">latex如何打空格</a></p>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex &amp; Markdown 论文写作篇</title>
    <link href="/2023/09/17/latex-markdown-lun-wen-xie-zuo-pian/"/>
    <url>/2023/09/17/latex-markdown-lun-wen-xie-zuo-pian/</url>
    
    <content type="html"><![CDATA[<h2 id="伪代码">伪代码</h2><p>在 latex 生成伪代码的方式有 algorithmic, algpseudocode, algorithm2e,algorithmicx 等几种方法，这里主要介绍前两种。</p><h3 id="algorithmic">algorithmic</h3><p>主要的命令有如下几个：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\STATE</span> &lt;text&gt;<br> <span class="hljs-keyword">\IF</span>&#123;&lt;condition&gt;&#125; <span class="hljs-keyword">\STATE</span> &#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ELSE</span> <span class="hljs-keyword">\STATE</span>&#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ENDIF</span><br> <span class="hljs-keyword">\IF</span>&#123;&lt;condition&gt;&#125; <span class="hljs-keyword">\STATE</span> &#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ELSIF</span>&#123;&lt;condition&gt;&#125; <span class="hljs-keyword">\STATE</span>&#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ENDIF</span><br> <span class="hljs-keyword">\FOR</span>&#123;&lt;condition&gt;&#125; <span class="hljs-keyword">\STATE</span> &#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ENDFOR</span><br> <span class="hljs-keyword">\FOR</span>&#123;&lt;condition&gt; <span class="hljs-keyword">\TO</span> &lt;condition&gt; &#125; <span class="hljs-keyword">\STATE</span> &#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ENDFOR</span><br> <span class="hljs-keyword">\FORALL</span>&#123;&lt;condition&gt;&#125; <span class="hljs-keyword">\STATE</span>&#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ENDFOR</span><br> <span class="hljs-keyword">\WHILE</span>&#123;&lt;condition&gt;&#125; <span class="hljs-keyword">\STATE</span>&#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ENDWHILE</span><br> <span class="hljs-keyword">\REPEAT</span> <span class="hljs-keyword">\STATE</span>&#123;&lt;text&gt;&#125; <span class="hljs-keyword">\UNTIL</span>&#123;&lt;condition&gt;&#125;<br> <span class="hljs-keyword">\LOOP</span> <span class="hljs-keyword">\STATE</span>&#123;&lt;text&gt;&#125; <span class="hljs-keyword">\ENDLOOP</span><br> <span class="hljs-keyword">\REQUIRE</span> &lt;text&gt;<br> <span class="hljs-keyword">\ENSURE</span> &lt;text&gt;<br> <span class="hljs-keyword">\RETURN</span> &lt;text&gt;<br> <span class="hljs-keyword">\PRINT</span> &lt;text&gt;<br> <span class="hljs-keyword">\COMMENT</span>&#123;&lt;text&gt;&#125;<br> <span class="hljs-keyword">\AND</span>, <span class="hljs-keyword">\OR</span>, <span class="hljs-keyword">\XOR</span>, <span class="hljs-keyword">\NOT</span>, <span class="hljs-keyword">\TO</span>, <span class="hljs-keyword">\TRUE</span>, <span class="hljs-keyword">\FALSE</span><br></code></pre></td></tr></table></figure><p>示例：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\documentclass</span>&#123;article&#125;<br><span class="hljs-keyword">\usepackage</span>&#123;algcompatible&#125;<br><span class="hljs-comment">% OR \usepackage&#123;algorithmic&#125;</span><br><span class="hljs-keyword">\begin</span>&#123;document&#125;<br><span class="hljs-keyword">\begin</span>&#123;algorithmic&#125;<br><span class="hljs-keyword">\STATE</span> <span class="hljs-built_in">$</span>i<span class="hljs-keyword">\gets</span> 10<span class="hljs-built_in">$</span><br><span class="hljs-keyword">\IF</span> &#123;<span class="hljs-built_in">$</span>i<span class="hljs-keyword">\geq</span> 5<span class="hljs-built_in">$</span>&#125; <br>  <span class="hljs-keyword">\STATE</span> <span class="hljs-built_in">$</span>i<span class="hljs-keyword">\gets</span> i-1<span class="hljs-built_in">$</span><br><span class="hljs-keyword">\ELSE</span><br>  <span class="hljs-keyword">\IF</span> &#123;<span class="hljs-built_in">$</span>i<span class="hljs-keyword">\leq</span> 3<span class="hljs-built_in">$</span>&#125;<br>    <span class="hljs-keyword">\STATE</span> <span class="hljs-built_in">$</span>i<span class="hljs-keyword">\gets</span> i+2<span class="hljs-built_in">$</span><br>  <span class="hljs-keyword">\ENDIF</span><br><span class="hljs-keyword">\ENDIF</span> <br><span class="hljs-keyword">\end</span>&#123;algorithmic&#125;<br><br><span class="hljs-keyword">\end</span>&#123;document&#125;<br></code></pre></td></tr></table></figure><h3 id="algpseudocode">algpseudocode</h3>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex &amp; Markdown 字母符号篇</title>
    <link href="/2023/09/05/latex-markdown-zi-mu-pian/"/>
    <url>/2023/09/05/latex-markdown-zi-mu-pian/</url>
    
    <content type="html"><![CDATA[<!-- 让表格居中显示的风格 --><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><h2 id="希腊字母">希腊字母</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">名称</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">alpha</td><td style="text-align: center;"><spanclass="math inline">\(\alpha\)</span></td><td style="text-align: center;">\alpha</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">beta</td><td style="text-align: center;"><spanclass="math inline">\(\beta\)</span></td><td style="text-align: center;">\beta</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">gamma</td><td style="text-align: center;"><spanclass="math inline">\(\gamma\)</span></td><td style="text-align: center;">\gamma</td><td style="text-align: center;"><spanclass="math inline">\(\Gamma\)</span></td><td style="text-align: center;">\Gamma</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">delta</td><td style="text-align: center;"><spanclass="math inline">\(\delta\)</span></td><td style="text-align: center;">\delta</td><td style="text-align: center;"><spanclass="math inline">\(\Delta\)</span></td><td style="text-align: center;">\Delta</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">epsilon</td><td style="text-align: center;"><spanclass="math inline">\(\epsilon\)</span></td><td style="text-align: center;">\epsilon</td><td style="text-align: center;"><spanclass="math inline">\(\varepsilon\)</span></td><td style="text-align: center;">\varepsilon</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">zeta</td><td style="text-align: center;"><spanclass="math inline">\(\zeta\)</span></td><td style="text-align: center;">\zeta</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">eta</td><td style="text-align: center;"><spanclass="math inline">\(\eta\)</span></td><td style="text-align: center;">\eta</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">theta</td><td style="text-align: center;"><spanclass="math inline">\(\theta\)</span></td><td style="text-align: center;">\theta</td><td style="text-align: center;"><spanclass="math inline">\(\Theta\)</span></td><td style="text-align: center;">\Theta</td><td style="text-align: center;"><spanclass="math inline">\(\vartheta\)</span></td><td style="text-align: center;">\vartheta</td></tr><tr class="odd"><td style="text-align: center;">iota</td><td style="text-align: center;"><spanclass="math inline">\(\iota\)</span></td><td style="text-align: center;">\iota</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">kappa</td><td style="text-align: center;"><spanclass="math inline">\(\kappa\)</span></td><td style="text-align: center;">\kappa</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">lambda</td><td style="text-align: center;"><spanclass="math inline">\(\lambda\)</span></td><td style="text-align: center;">\lambda</td><td style="text-align: center;"><spanclass="math inline">\(\Lambda\)</span></td><td style="text-align: center;">\Lambda</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">mu</td><td style="text-align: center;"><spanclass="math inline">\(\mu\)</span></td><td style="text-align: center;">\mu</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">nu</td><td style="text-align: center;"><spanclass="math inline">\(\nu\)</span></td><td style="text-align: center;">\nu</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">xi</td><td style="text-align: center;"><spanclass="math inline">\(\xi\)</span></td><td style="text-align: center;">\xi</td><td style="text-align: center;"><spanclass="math inline">\(\Xi\)</span></td><td style="text-align: center;">\Xi</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">pi</td><td style="text-align: center;"><spanclass="math inline">\(\pi\)</span></td><td style="text-align: center;">\pi</td><td style="text-align: center;"><spanclass="math inline">\(\Pi\)</span></td><td style="text-align: center;">\Pi</td><td style="text-align: center;"><spanclass="math inline">\(\varpi\)</span></td><td style="text-align: center;">\varpi</td></tr><tr class="even"><td style="text-align: center;">rho</td><td style="text-align: center;"><spanclass="math inline">\(\rho\)</span></td><td style="text-align: center;">\rho</td><td style="text-align: center;"><spanclass="math inline">\(\varrho\)</span></td><td style="text-align: center;">\varrho</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">sigma</td><td style="text-align: center;"><spanclass="math inline">\(\sigma\)</span></td><td style="text-align: center;">\sigma</td><td style="text-align: center;"><spanclass="math inline">\(\varsigma\)</span></td><td style="text-align: center;">\varsigma</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">tau</td><td style="text-align: center;"><spanclass="math inline">\(\tau\)</span></td><td style="text-align: center;">\tau</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">upsilon</td><td style="text-align: center;"><spanclass="math inline">\(\upsilon\)</span></td><td style="text-align: center;">\upsilon</td><td style="text-align: center;"><spanclass="math inline">\(\Upsilon\)</span></td><td style="text-align: center;">\Upsilon</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">phi</td><td style="text-align: center;"><spanclass="math inline">\(\phi\)</span></td><td style="text-align: center;">\phi</td><td style="text-align: center;"><spanclass="math inline">\(\Phi\)</span></td><td style="text-align: center;">\Phi</td><td style="text-align: center;"><spanclass="math inline">\(\varphi\)</span></td><td style="text-align: center;">\varphi</td></tr><tr class="odd"><td style="text-align: center;">chi</td><td style="text-align: center;"><spanclass="math inline">\(\chi\)</span></td><td style="text-align: center;">\chi</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">psi</td><td style="text-align: center;"><spanclass="math inline">\(\psi\)</span></td><td style="text-align: center;">\psi</td><td style="text-align: center;"><spanclass="math inline">\(\Psi\)</span></td><td style="text-align: center;">\Psi</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">omega</td><td style="text-align: center;"><spanclass="math inline">\(\omega\)</span></td><td style="text-align: center;">\omega</td><td style="text-align: center;"><spanclass="math inline">\(\Omega\)</span></td><td style="text-align: center;">\Omega</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table></div><h2 id="戴帽符号">戴帽符号</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\hat{y}\)</span></td><td style="text-align: center;">\hat{y}</td><td style="text-align: center;"><spanclass="math inline">\(\widetilde{A}\)</span></td><td style="text-align: center;">\widetilde{A}</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\check{y}\)</span></td><td style="text-align: center;">\check{y}</td><td style="text-align: center;"><spanclass="math inline">\(\vec{a}\)</span></td><td style="text-align: center;">\vec{a}</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\breve{y}\)</span></td><td style="text-align: center;">\breve{y}</td><td style="text-align: center;"><spanclass="math inline">\(\dot{a}\)</span></td><td style="text-align: center;">\dot{a}</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\bar{a}\)</span></td><td style="text-align: center;">\bar{a}</td><td style="text-align: center;"><spanclass="math inline">\(\ddot{a}\)</span></td><td style="text-align: center;">\ddot{a}</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\tilde{a}\)</span></td><td style="text-align: center;">\tilde{a}</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table></div><h2 id="连线符号">连线符号</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\overline{a+b+c+d}\)</span></td><td style="text-align: center;">\overline{a+b+c+d}</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\underline{a+b+c+d}\)</span></td><td style="text-align: center;">\underline{a+b+c+d}</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\overbrace{a+\underbrace{b+c}{1.0}+d}^{2.0}\)</span></td><tdstyle="text-align: center;">\overbrace{a+\underbrace{b+c}{1.0}+d}^{2.0}</td></tr></tbody></table></div><h2 id="箭头符号">箭头符号</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\uparrow\)</span></td><td style="text-align: center;">\uparrow</td><td style="text-align: center;"><spanclass="math inline">\(\Rightarrow\)</span></td><td style="text-align: center;">\Rightarrow</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\downarrow\)</span></td><td style="text-align: center;">\downarrow</td><td style="text-align: center;"><spanclass="math inline">\(\Leftarrow\)</span></td><td style="text-align: center;">\Leftarrow</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\Uparrow\)</span></td><td style="text-align: center;">\Uparrow</td><td style="text-align: center;"><spanclass="math inline">\(\longrightarrow\)</span></td><td style="text-align: center;">\longrightarrow</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\Downarrow\)</span></td><td style="text-align: center;">\Downarrow</td><td style="text-align: center;"><spanclass="math inline">\(\longleftarrow\)</span></td><td style="text-align: center;">\longleftarrow</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\rightarrow\)</span></td><td style="text-align: center;">\rightarrow</td><td style="text-align: center;"><spanclass="math inline">\(\Longrightarrow\)</span></td><td style="text-align: center;">\Longrightarrow</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\leftarrow\)</span></td><td style="text-align: center;">\leftarrow</td><td style="text-align: center;"><spanclass="math inline">\(\Longleftarrow\)</span></td><td style="text-align: center;">\Longleftarrow</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\leftrightarrow\)</span></td><td style="text-align: center;">\leftrightarrow</td><td style="text-align: center;"><spanclass="math inline">\(\Longleftrightarrow\)</span></td><td style="text-align: center;">\Longleftrightarrow</td></tr></tbody></table></div><h2 id="字母类符号">字母类符号</h2><p>Caligraphic letters: <code>$\mathcal&#123;A&#125;$</code> etc.: <spanclass="math inline">\(\mathcal{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Mathbb letters: <code>$\mathbb&#123;A&#125;$</code>etc.: <spanclass="math inline">\(\mathbb{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Mathfrak letters: <code>$\mathfrak&#123;A&#125;$</code>etc.: <spanclass="math inline">\(\mathfrak{A B C D E F G H I J K L M N O P Q I S TU V W X Y Z}\)</span></p><p>Math Sans serif letters: <code>$\mathsf&#123;A&#125;$</code>etc.: <spanclass="math inline">\(\mathsf{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Math bold letters: <code>$\mathbf&#123;A&#125;$</code>etc.: <spanclass="math inline">\(\mathbf{A B C D E F G H I J K L M N O P Q I S T UV W X Y Z}\)</span></p><p>Math bold italic letters: define<code>\def\mathbi#1&#123;\textbf&#123;\em #1&#125;&#125;</code> then use<code>$\mathbi&#123;A&#125;$</code></p>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex &amp; Markdown 运算符篇</title>
    <link href="/2023/09/05/latex-markdown-shu-xue-fu-hao-pian/"/>
    <url>/2023/09/05/latex-markdown-shu-xue-fu-hao-pian/</url>
    
    <content type="html"><![CDATA[<!-- 让表格居中显示的风格 --><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;}</style><div class="center"><h2 id="关系运算符">关系运算符</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\pm\)</span></td><td style="text-align: center;">\pm</td><td style="text-align: center;"><spanclass="math inline">\(\bigoplus\)</span></td><td style="text-align: center;">\bigoplus</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\times\)</span></td><td style="text-align: center;">\times</td><td style="text-align: center;"><spanclass="math inline">\(\leq\)</span></td><td style="text-align: center;">\leq</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\div\)</span></td><td style="text-align: center;">\div</td><td style="text-align: center;"><spanclass="math inline">\(\geq\)</span></td><td style="text-align: center;">\geq</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\mid\)</span></td><td style="text-align: center;">\mid</td><td style="text-align: center;"><spanclass="math inline">\(\neq\)</span></td><td style="text-align: center;">\neq</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\nmid\)</span></td><td style="text-align: center;">\nmid</td><td style="text-align: center;"><spanclass="math inline">\(\approx\)</span></td><td style="text-align: center;">\approx</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\cdot\)</span></td><td style="text-align: center;">\cdot</td><td style="text-align: center;"><spanclass="math inline">\(\equiv\)</span></td><td style="text-align: center;">\equiv</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\circ\)</span></td><td style="text-align: center;">\circ</td><td style="text-align: center;"><spanclass="math inline">\(\sum\)</span></td><td style="text-align: center;">\sum</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\ast\)</span></td><td style="text-align: center;">\ast</td><td style="text-align: center;"><spanclass="math inline">\(\prod\)</span></td><td style="text-align: center;">\prod</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\bigodot\)</span></td><td style="text-align: center;">\bigodot</td><td style="text-align: center;"><spanclass="math inline">\(\coprod\)</span></td><td style="text-align: center;">\coprod</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\bigotimes\)</span></td><td style="text-align: center;">\bigotimes</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table></div><h2 id="集合运算符">集合运算符</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\emptyset\)</span></td><td style="text-align: center;">\emptyset</td><td style="text-align: center;"><spanclass="math inline">\(\bigcap\)</span></td><td style="text-align: center;">\bigcap</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\in\)</span></td><td style="text-align: center;">\in</td><td style="text-align: center;"><spanclass="math inline">\(\bigcup\)</span></td><td style="text-align: center;">\bigcup</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\notin\)</span></td><td style="text-align: center;">\notin</td><td style="text-align: center;"><spanclass="math inline">\(\bigvee\)</span></td><td style="text-align: center;">\bigvee</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\subset\)</span></td><td style="text-align: center;">\subset</td><td style="text-align: center;"><spanclass="math inline">\(\bigwedge\)</span></td><td style="text-align: center;">\bigwedge</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\supset\)</span></td><td style="text-align: center;">\supset</td><td style="text-align: center;"><spanclass="math inline">\(\biguplus\)</span></td><td style="text-align: center;">\biguplus</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\subseteq\)</span></td><td style="text-align: center;">\subseteq</td><td style="text-align: center;"><spanclass="math inline">\(\bigsqcup\)</span></td><td style="text-align: center;">\bigsqcup</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\supseteq\)</span></td><td style="text-align: center;">\supseteq</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table></div><h2 id="对数运算符">对数运算符</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\log\)</span></td><td style="text-align: center;">\log</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\lg\)</span></td><td style="text-align: center;">\lg</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\ln\)</span></td><td style="text-align: center;">\ln</td></tr></tbody></table></div><h2 id="三角运算符">三角运算符</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\bot\)</span></td><td style="text-align: center;">\bot</td><td style="text-align: center;"><spanclass="math inline">\(\tan\)</span></td><td style="text-align: center;">\tan</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\angle\)</span></td><td style="text-align: center;">\angle</td><td style="text-align: center;"><spanclass="math inline">\(\cot\)</span></td><td style="text-align: center;">\cot</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(30^\circ\)</span></td><td style="text-align: center;">30^\circ</td><td style="text-align: center;"><spanclass="math inline">\(\sec\)</span></td><td style="text-align: center;">\sec</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\sin\)</span></td><td style="text-align: center;">\sin</td><td style="text-align: center;"><spanclass="math inline">\(\csc\)</span></td><td style="text-align: center;">\csc</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\cos\)</span></td><td style="text-align: center;">\cos</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table></div><h2 id="微积分运算符">微积分运算符</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\prime\)</span></td><td style="text-align: center;">\prime</td><td style="text-align: center;"><spanclass="math inline">\(\oint\)</span></td><td style="text-align: center;">\oint</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\int\)</span></td><td style="text-align: center;">\int</td><td style="text-align: center;"><spanclass="math inline">\(\lim\)</span></td><td style="text-align: center;">\lim</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\iint\)</span></td><td style="text-align: center;">\iint</td><td style="text-align: center;"><spanclass="math inline">\(\infty\)</span></td><td style="text-align: center;">\infty</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\iiint\)</span></td><td style="text-align: center;">\iiint</td><td style="text-align: center;"><spanclass="math inline">\(\nabla\)</span></td><td style="text-align: center;">\nabla</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\iiiint\)</span></td><td style="text-align: center;">\iiiint</td><td style="text-align: center;"><spanclass="math inline">\(\partial\)</span></td><td style="text-align: center;">\partial</td></tr></tbody></table></div><h2 id="逻辑运算符">逻辑运算符</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th><th style="text-align: center;">原符号</th><th style="text-align: center;">markdown</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\because\)</span></td><td style="text-align: center;">\because</td><td style="text-align: center;"><spanclass="math inline">\(\not=\)</span></td><td style="text-align: center;">\not=</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\therefore\)</span></td><td style="text-align: center;">\therefore</td><td style="text-align: center;"><spanclass="math inline">\(\not&gt;\)</span></td><td style="text-align: center;">\not&gt;</td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(\forall\)</span></td><td style="text-align: center;">\forall</td><td style="text-align: center;"><spanclass="math inline">\(\not\subset\)</span></td><td style="text-align: center;">\not\subset</td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(\exists\)</span></td><td style="text-align: center;">\exists</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数学习日记（一）</title>
    <link href="/2023/08/26/xian-xing-dai-shu-xue-xi-ri-ji-yi/"/>
    <url>/2023/08/26/xian-xing-dai-shu-xue-xi-ri-ji-yi/</url>
    
    <content type="html"><![CDATA[<p>线性代数的核心问题是解决等式的一系列问题，这些等式是线性的，即未知数仅被数字乘，不会出现两个未知数<span class="math inline">\(x\)</span> 乘 <spanclass="math inline">\(y\)</span> 的情况。</p><h2 id="introduction-to-vectors">Introduction to Vectors</h2><p>向量 <span class="math inline">\(cv\)</span> 在一条线上，当 <spanclass="math inline">\(w\)</span> 不在这条线上时，<spanclass="math inline">\(cv + dw\)</span> 构成了一个平面。</p><p>线性代数建立在 <span class="math inline">\(v + w\)</span>，<spanclass="math inline">\(cv\)</span> 和 <spanclass="math inline">\(dw\)</span>这些运算上，也就是向量加法和与标量相乘。</p><h2 id="lengths-and-dot-products">Lengths and Dot Products</h2><p><span class="math inline">\(v = (v_1,v_2)\)</span> 和 <spanclass="math inline">\(w = (w_1, w_2)\)</span> 的点积或内积是标量 <spanclass="math inline">\(v \cdot w\)</span> ： <spanclass="math display">\[v \cdot w = v_1w_1 + v_2w_2\]</span> 向量 <span class="math inline">\(v\)</span> 的长度 <spanclass="math inline">\(||v||\)</span> 是 <span class="math inline">\(v\cdot v\)</span> 的平方根： <span class="math display">\[length = ||v|| = \sqrt{v \cdot v} = (v_1^2 + v_2^2 + \cdots +v_n^2)^{1/2}\]</span> 单位向量是长度为 1 的向量，即 <span class="math inline">\(u\cdot u = 1\)</span>。</p><p>v 的单位向量与 v 在同一方向为 <span class="math inline">\(u = v /||v||\)</span>。</p><p>当 v 和 w 垂直时，他们的点积 <span class="math inline">\(v \cdot w =0\)</span>。</p><p>如果 v 和 w 是非零向量，那么 <span class="math inline">\(\frac{v\cdot w}{||v|| ||w||} = cos \theta\)</span></p><p>schwarz inequality <span class="math inline">\(|v \cdot w| \leq ||v||\ ||w||\)</span></p><p>triangle inequality <span class="math inline">\(|v + w| \leq ||v|| +||w||\)</span></p><h2 id="matrices">Matrices</h2><h3 id="列角度">列角度</h3><p>矩阵 A 与向量 x 相乘是矩阵 A 的各列以 x 的方式组合 <spanclass="math display">\[Ax =\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   -1 &amp; 1 &amp; 0 \\   0 &amp; -1 &amp; 1\end{array}\right]\left[\begin{array}{}   x_1 \\   x_2 \\   x_3\end{array}\right] =\left[\begin{array}{}   x_1 \\   x_2 - x_1 \\   x_3 - x_2\end{array}\right] =\left[\begin{array}{}   b_1 \\   b_2 \\   b_3\end{array}\right] =b\]</span> 这个 A 称作 diffenence matrix，因为 b 包括了输入向量 x的所有差值。</p><h3 id="行角度">行角度</h3><p>矩阵 A 与向量 x 相乘也可以看作是行的点积 <spanclass="math display">\[Ax =\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   -1 &amp; 1 &amp; 0 \\   0 &amp; -1 &amp; 1\end{array}\right]\left[\begin{array}{}   x_1 \\   x_2 \\   x_3\end{array}\right] =\left[\begin{array}{}   (1, 0, 0) \cdot (x_1, x_2, x_3) \\   (-1, 1, 0) \cdot (x_1, x_2, x_3) \\   (0, -1, 1) \cdot (x_1, x_2, x_3)\end{array}\right]\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Math</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高效能人士的七个习惯 学习日记（一）</title>
    <link href="/2023/08/26/gao-xiao-neng-ren-shi-de-qi-ge-xi-guan-xue-xi-ri-ji-yi/"/>
    <url>/2023/08/26/gao-xiao-neng-ren-shi-de-qi-ge-xi-guan-xue-xi-ri-ji-yi/</url>
    
    <content type="html"><![CDATA[<h2 id="section"></h2><p>习惯一：积极主动</p><p>习惯二：以终为始</p><p>习惯三：要事第一</p><p>习惯四：双赢思维</p><p>习惯五：知彼解己</p><p>习惯六：统合综效</p><p>习惯七：不断更新</p>]]></content>
    
    
    <categories>
      
      <category>心理学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>心理学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性代数学习日记（二）</title>
    <link href="/2023/08/23/xian-xing-dai-shu-xue-xi-ri-ji-er/"/>
    <url>/2023/08/23/xian-xing-dai-shu-xue-xi-ri-ji-er/</url>
    
    <content type="html"><![CDATA[<h2 id="elimination-using-matrices">Elimination Using Matrices</h2><p>通过与<strong>消元矩阵</strong> <spanclass="math inline">\(E\)</span> 相乘从第 2 行中减去第 1 行的 2 倍。<span class="math display">\[\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   \mathbf{-2} &amp; \mathbf{1} &amp; 0 \\   0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{}   \mathbf{2} \\   \mathbf{8} \\   10\end{array}\right]\left[\begin{array}{}   2 \\   \mathbf{4} \\   10\end{array}\right] \quad \quad\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   \mathbf{-2} &amp; \mathbf{1} &amp; 0 \\   0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{}   \mathbf{b_1} \\   \mathbf{b_2} \\   b_3\end{array}\right]\left[\begin{array}{}   b_1 \\   \mathbf{b_2-2b_1} \\   b_3\end{array}\right]\]</span> 通过与<strong>置换矩阵</strong>相乘交换矩阵中的行 <spanclass="math display">\[\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   0 &amp; 0 &amp; 1 \\   0 &amp; 2 &amp; 0\end{array}\right]\left[\begin{array}{}   2 &amp; 4 &amp; 1 \\   \mathbf{0} &amp; \mathbf{0} &amp; \mathbf{3} \\   0 &amp; 6 &amp; 5\end{array}\right] =\left[\begin{array}{}   2 &amp; 4 &amp; 1 \\   0 &amp; 6 &amp; 5 \\   \mathbf{0} &amp; \mathbf{0} &amp; \mathbf{3}\end{array}\right]\]</span> 除 A 矩阵外包含 b作为额外列的矩阵称作<strong>增广矩阵</strong> <spanclass="math display">\[\left[\begin{array}{}   A &amp; b\end{array}\right] =\left[\begin{array}{}   2 &amp; 4 &amp; -2 &amp; \mathbf{2}\\   4 &amp; 9 &amp; -3 &amp; \mathbf{8}\\   -2 &amp; -3 &amp; 7 &amp; \mathbf{10}\end{array}\right]\]</span> 在增广矩阵上进行消元操作 <span class="math display">\[\left[\begin{array}{}   1 &amp; 0 &amp; 0 \\   -2 &amp; 1 &amp; 0 \\   0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{}   2 &amp; 4 &amp; -2 &amp; \mathbf{2}\\   4 &amp; 9 &amp; -3 &amp; \mathbf{8}\\   -2 &amp; -3 &amp; 7 &amp; \mathbf{10}\end{array}\right] =\left[\begin{array}{}   2 &amp; 4 &amp; -2 &amp; \mathbf{2}\\   0 &amp; 1 &amp; 1 &amp; \mathbf{4}\\   -2 &amp; -3 &amp; 7 &amp; \mathbf{10}\end{array}\right]\]</span></p><h2 id="rules-for-matrix-operations">Rules for Matrix Operations</h2><h3 id="矩阵加法">矩阵加法</h3><p>两个大小相同矩阵相加 <span class="math inline">\(A + B\)</span>为对应位置元素相加： <span class="math display">\[\left[\begin{matrix}   1 &amp; 2 \\   3 &amp; 4 \\   0 &amp; 0  \end{matrix}  \right] +  \left[  \begin{matrix}  2 &amp; 2 \\  4 &amp; 4 \\  9 &amp; 9  \end{matrix}  \right] =  \left[  \begin{matrix}  3 &amp; 4 \\  7 &amp; 8 \\  9 &amp; 9  \end{matrix}  \right]\]</span> 常数和矩阵相乘 <span class="math inline">\(cA\)</span>为矩阵每个元素乘常数： <span class="math display">\[2\left[\begin{matrix}   1 &amp; 2 \\   3 &amp; 4 \\   0 &amp; 0  \end{matrix}  \right] =  \left[\begin{matrix}   2 &amp; 4 \\   6 &amp; 8 \\   0 &amp; 0  \end{matrix}  \right]\]</span></p><h3 id="矩阵乘法">矩阵乘法</h3><h4 id="点积视角">点积视角</h4><p><span class="math inline">\(m \times n\)</span> 的矩阵 A 和 <spanclass="math inline">\(n \times p\)</span> 的矩阵 B 相乘后维度为 <spanclass="math inline">\(m \times p\)</span> <span class="math display">\[(m \times n)(n \times p) = (m \times p) \\left[\begin{matrix}   m \ rows \\   n \ columns  \end{matrix}\right]\left[\begin{matrix}   n \ rows \\   p \ columns  \end{matrix}\right] =\left[\begin{matrix}   m \ rows \\   p \ columns  \end{matrix}\right]\]</span> 矩阵 AB 第 i 行 第 j 列元素为 A 的第 i 行和 B 的第 j列的点积。 <span class="math display">\[\left[\begin{matrix}   * \\   a_{i1} &amp; a_{i2} &amp; \cdots &amp; a_{i5} \\   * \\   *  \end{matrix}\right]\left[\begin{matrix}   * &amp; * &amp; b_{1j} &amp; * &amp; * &amp; *\\    &amp;  &amp; b_{2j} &amp;  &amp;  &amp; \\    &amp;  &amp; \vdots &amp;  &amp;  &amp; \\    &amp;  &amp; b_{5j} &amp;  &amp;  &amp;  \end{matrix}\right] =\left[\begin{matrix}     &amp;   &amp; * &amp;   &amp;   &amp;  \\   * &amp; * &amp; AB_{ij} &amp; * &amp; * &amp; *\\    &amp;  &amp; * &amp;  &amp;  &amp; \\    &amp;  &amp; * &amp;  &amp;  &amp;  \end{matrix}\right]\]</span>一行乘一列得到一个元素称作内积，也叫点积，一列乘一行得到一个矩阵称作外积。</p><h4 id="列的组合">列的组合</h4><p>矩阵 AB 的每一列是 A 的列的组合 <span class="math display">\[A[b_1, \cdots, b_p] = [Ab_1, \cdots, Ab_p]\]</span></p><h4 id="行的组合">行的组合</h4><p>矩阵 AB 的每一行是 B 的行的组合 <span class="math display">\[[row \ i \ of \ A] \\left[\begin{matrix}   1 &amp; 2 &amp; 3 \\   4 &amp; 5 &amp; 6\\   7 &amp; 8 &amp; 9  \end{matrix}\right] =[row \ i \ of \ AB]\]</span></p><h4 id="列乘行">列乘行</h4><p>矩阵 A 的列 i 与矩阵 B 的行 j 相乘之后相加 <spanclass="math display">\[\left[\begin{matrix}   col\ 1 &amp; col\ 2 &amp; col\ 3 \\   \cdot &amp; \cdot &amp; \cdot \\   \cdot &amp; \cdot &amp; \cdot  \end{matrix}\right]\left[\begin{matrix}   row\ 1 &amp; \cdot &amp; \cdot \\   row\ 2 &amp; \cdot &amp; \cdot \\   row\ 3 &amp; \cdot &amp; \cdot  \end{matrix}\right] =(col \ 1)(row \ 1) + (col \ 2)(row \ 2) + (col \ 3)(row \ 3)\]</span></p><h3 id="矩阵乘法规则">矩阵乘法规则</h3><p>矩阵相乘遵循的规则： <span class="math display">\[\begin{array}{}   AB \neq BA \\A(B+C) = AB + AC \\(A+B)C = AC + BC \\(AB)C=A(BC) \\A_p = AAA\cdots A(p \ factors) \quad (A^p)(A^q) = A^{p+q} \quad (A^p)^q= A^{pq}\end{array}\]</span></p><h3 id="块矩阵及其乘法">块矩阵及其乘法</h3><p><span class="math inline">\(4 \times 6\)</span> 的矩阵可以切割为6个<span class="math inline">\(2 \times 2\)</span> 的矩阵。 <spanclass="math display">\[\left[\begin{array}{cc|cc|cc}   1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\   0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\   \hline   1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 \\   0 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\end{array}\right] =\left[\begin{array}{}   I &amp; I &amp; I \\   I &amp; I &amp; I \\\end{array}\right]\]</span> 如果 A 的块和 B 的块能够相乘，那么 AB 成立。AB 为 A的列的切片乘 B 的行的切片。 <span class="math display">\[\left[\begin{array}{}   A_{11} &amp; A_{12} \\   A_{21} &amp; A_{22} \\\end{array}\right]\left[\begin{array}{}   B_{11} \\   B_{21} \\\end{array}\right] =\left[\begin{array}{}   A_{11}B_{11} &amp; A_{12}B_{21} \\   A_{21}B_{11} &amp; A_{22}B_{21} \\\end{array}\right]\]</span></p><h2 id="inverse-matrices">Inverse Matrices</h2><p>矩阵 A 可逆，如果有一个矩阵 <spanclass="math inline">\(A^{-1}\)</span>： <span class="math display">\[A^{-1} A = I \quad \quad AA^{-1}=I\]</span> 不是所有的矩阵都有逆矩阵，有以下几个条件或性质：</p><ul><li><p>Note 1: 矩阵的逆存在当且仅当消元能产生 n 个主元。</p></li><li><p>Note 2: 矩阵 A 没有两个不同的逆。</p></li><li><p>Note 3: 如果 A 可逆，<span class="math inline">\(Ax=b\)</span>唯一的解是 <span class="math inline">\(x=A^{-1}b\)</span>.</p></li><li><p>Note 4: x 为非零向量，<spanclass="math inline">\(Ax=0\)</span>，那么 A 没有逆。</p></li><li><p>Note 5: <span class="math inline">\(2 \times 2\)</span>的矩阵可逆当且仅当 <span class="math inline">\(ad-bc\)</span>非零。</p></li></ul><p><span class="math display">\[\left[\begin{array}{}   a &amp; b \\   c &amp; d \\\end{array}\right]^{-1} =\frac{1}{ad-bc}\left[\begin{array}{}   d &amp; -b \\   -c &amp; a \\\end{array}\right]\]</span></p><ul><li>Note 6: 对角矩阵有逆，该逆矩阵的对角线元素没有 0。</li></ul><p>A 和 B 都可逆，他们的乘积 AB 是： <span class="math display">\[(AB)^{-1}=B^{-1}A^{-1}\]</span></p><h3 id="gauss-jordan-method">Gauss-Jordan method</h3><p><span class="math inline">\(A^{-1}\)</span> 乘 <spanclass="math inline">\([A \quad I]\)</span> 得到 <spanclass="math inline">\([A \quad I]\)</span>. <spanclass="math display">\[\begin{aligned}    \left[\begin{array}{}        K &amp; e_1 &amp; e_2 &amp; e_3 \\    \end{array}\right] &amp;=    \left[\begin{array}{}       2 &amp; -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\       -1 &amp; 2 &amp; -1 &amp; 0 &amp; 1 &amp; 0 \\       0 &amp; -1 &amp; 2 &amp; 0 &amp; 0 &amp; 1 \\    \end{array}\right] \\ &amp;\rightarrow    \left[\begin{array}{}       2 &amp; -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\       0 &amp; \frac{3}{2} &amp; -1 &amp; \frac{1}{2} &amp; 1 &amp; 0 \\       0 &amp; -1 &amp; 2 &amp; 0 &amp; 0 &amp; 1 \\    \end{array}\right] \\ &amp;\rightarrow    \left[\begin{array}{}       2 &amp; -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\       0 &amp; \frac{3}{2} &amp; -1 &amp; \frac{1}{2} &amp; 1 &amp; 0 \\       0 &amp; 0 &amp; \frac{4}{3} &amp; \frac{1}{3} &amp; \frac{2}{3}&amp; 1 \\    \end{array}\right] \\ &amp;\rightarrow    \left[\begin{array}{}       2 &amp; -1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\       0 &amp; \frac{3}{2} &amp; 0 &amp; \frac{3}{4} &amp; \frac{3}{2}&amp; \frac{3}{4} \\       0 &amp; 0 &amp; \frac{4}{3} &amp; \frac{1}{3} &amp; \frac{2}{3}&amp; 1 \\    \end{array}\right] \\ &amp;\rightarrow    \left[\begin{array}{}       2 &amp; 0 &amp; 0 &amp; \frac{3}{2} &amp; 1 &amp; \frac{1}{2} \\       0 &amp; \frac{3}{2} &amp; 0 &amp; \frac{3}{4} &amp; \frac{3}{2}&amp; \frac{3}{4} \\       0 &amp; 0 &amp; \frac{4}{3} &amp; \frac{1}{3} &amp; \frac{2}{3}&amp; 1 \\    \end{array}\right] \\ &amp;\rightarrow    \left[\begin{array}{}       1 &amp; 0 &amp; 0 &amp; \frac{3}{4} &amp; \frac{1}{2} &amp;\frac{1}{4} \\       0 &amp; 1 &amp; 0 &amp; \frac{1}{2} &amp; 1 &amp; \frac{1}{2} \\       0 &amp; 0 &amp; 1 &amp; \frac{1}{4} &amp; \frac{1}{2} &amp;\frac{3}{4} \\    \end{array}\right]\end{aligned}\]</span> 关于 K，我们可以观察到以下几个现象：</p><ul><li>K 是沿着对角线对称，<span class="math inline">\(K^{-1}\)</span>也对称。</li><li>K 是对角的（只有对角线三个非零元素），但是 <spanclass="math inline">\(K^{-1}\)</span> 是没有零元素的稠密矩阵。</li><li>枢轴元素之积是 <span class="math inline">\(2(\frac{3}{2})(\frac{4}{3}) = 4\)</span>. 4 是 K 的行列式。</li></ul><p><span class="math inline">\(K^{-1}\)</span> 包括了被 K 的行列式的除：<span class="math display">\[K^{-1} = \frac{1}{4}\left[\begin{array}{}   3 &amp; 2 &amp; 1 \\   2 &amp; 4 &amp; 2 \\   1 &amp; 2 &amp; 3\end{array}\right]\]</span> 消元为方阵的可逆性提供了检验，A 有 n 个主元，<spanclass="math inline">\(A^{-1}\)</span> 存在的情况下： <spanclass="math display">\[If \quad AC=I \quad then \quad CA=I \quad and \quad C=A^{-1}\]</span></p><h2 id="elimination-factorization-a-lu">Elimination = Factorization: A =LU</h2><h2 id="transposes-and-permutations">Transposes and Permutations</h2>]]></content>
    
    
    <categories>
      
      <category>Math</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>算法笔记——树篇</title>
    <link href="/2023/08/12/suan-fa-bi-ji-shu-pian/"/>
    <url>/2023/08/12/suan-fa-bi-ji-shu-pian/</url>
    
    <content type="html"><![CDATA[<h2 id="二叉树的存储结构与基本操作">二叉树的存储结构与基本操作</h2><p>结点的存储结构</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">node</span>&#123;<br><span class="hljs-keyword">typename</span> data; <span class="hljs-comment">//数据域</span><br>    node* lchild; <span class="hljs-comment">//指向左子树根结点的指针</span><br>    node* rchild; <span class="hljs-comment">//指向右子树根结点的指针</span><br>&#125;<br><br>node* root = <span class="hljs-literal">NULL</span>; <span class="hljs-comment">//根结点的初始化</span><br></code></pre></td></tr></table></figure><p>新建结点</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">node* <span class="hljs-title">newNode</span><span class="hljs-params">(<span class="hljs-type">int</span> v)</span></span>&#123;<br>    node* Node = <span class="hljs-keyword">new</span> node;<br>    Node-&gt;data = v;<br>    Node-&gt;lchild = Node-&gt;rchild = <span class="hljs-literal">NULL</span>;<br>    <span class="hljs-keyword">return</span> Node; <span class="hljs-comment">//返回新建结点的地址</span><br>&#125;<br></code></pre></td></tr></table></figure><p>二叉树结点的查找和修改</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">search</span><span class="hljs-params">(node* root, <span class="hljs-type">int</span> x, <span class="hljs-type">int</span> newdata)</span></span>&#123;<br><span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>)&#123;<br>        <span class="hljs-keyword">return</span>; <span class="hljs-comment">//空树，死胡同（递归边界）</span><br>    &#125;<br>    <span class="hljs-keyword">if</span>(root-&gt;data == x)&#123; <span class="hljs-comment">//找到数据域为 x 的结点，把它修改成 newdata</span><br>        root-&gt;data = newdata;<br>    &#125;<br>    <span class="hljs-built_in">search</span>(root-&gt;lchild, x, newdata); <span class="hljs-comment">//往左子树搜索 x (递归式)</span><br>    <span class="hljs-built_in">search</span>(root-&gt;rchild, x, newdata); <span class="hljs-comment">//往右子树搜索 x (递归式)</span><br>&#125;<br></code></pre></td></tr></table></figure><p>二叉树结点的插入</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//insert 函数将在二叉树中插入一个数据域为 x 的新结点</span><br><span class="hljs-comment">//注意根结点指针 root 要使用引用，否则插入不会成功</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">insert</span><span class="hljs-params">(node* &amp;root, <span class="hljs-type">int</span> x)</span></span>&#123;<br><span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>)&#123;<br>        root = <span class="hljs-built_in">newNode</span>(x); <span class="hljs-comment">//空树，说明查找失败，也即插入位置（递归边界）</span><br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-keyword">if</span>(由二叉树的性质，x 应插在左子树)&#123;<br>        <span class="hljs-built_in">insert</span>(root-&gt;lchild, x); <span class="hljs-comment">//往左子树搜索(递归式)</span><br>    &#125; <span class="hljs-keyword">else</span>&#123;<br>        <span class="hljs-built_in">insert</span>(root-&gt;rchild, x); <span class="hljs-comment">//往右子树搜索(递归式)</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>二叉树的创建</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//二叉树的建立</span><br><span class="hljs-function">node* <span class="hljs-title">Create</span><span class="hljs-params">(<span class="hljs-type">int</span> data[], <span class="hljs-type">int</span> n)</span></span>&#123;<br>    node* root = <span class="hljs-literal">NULL</span>; <span class="hljs-comment">//新建空根结点 root</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-built_in">insert</span>(root, data[i]); <span class="hljs-comment">//将 data[0]~data[n-1]插入二叉树中</span><br>    &#125;<br>    <span class="hljs-keyword">return</span> root; <span class="hljs-comment">//返回根结点</span><br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>算法笔记——搜索篇</title>
    <link href="/2023/08/12/suan-fa-bi-ji-sou-suo-pian/"/>
    <url>/2023/08/12/suan-fa-bi-ji-sou-suo-pian/</url>
    
    <content type="html"><![CDATA[<h2 id="深度优先搜索dfs">深度优先搜索（DFS）</h2><p><strong>背包问题</strong></p><p>有 n 件物品，每件物品的重量为 w[i], 价值为c[i]。现在需要选出若干件物品放入一个容量为 V的背包中，使得在选入背包的物品重量和不超过容量 V的前提下，让背包中物品的价值之和最大，求最大价值。（1&lt;= n &lt;=20）</p><p>输入数据：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-number">5</span> <span class="hljs-number">8</span> <span class="hljs-comment">//5 件物品，背包容量为 8</span><br><span class="hljs-number">3</span> <span class="hljs-number">5</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-comment">//重量分别为 3 5 1 2 2</span><br><span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">3</span> <span class="hljs-comment">//价值分别为 4 5 2 1 3</span><br></code></pre></td></tr></table></figure><p>输出数据：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>实现代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstido&gt;</span></span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> maxn = <span class="hljs-number">30</span>;<br><span class="hljs-type">int</span> n, V, maxValue = <span class="hljs-number">0</span>; <span class="hljs-comment">//物品件数 n，背包容量 V，最大价值 maxValue</span><br><span class="hljs-type">int</span> w[maxn], c[maxn]; <span class="hljs-comment">//w[i] 为每件物品的重量，c[i] 为每件物品的价值</span><br><span class="hljs-comment">// DFS, index 为当前处理的物品编号</span><br><span class="hljs-comment">// sumW 和 sumC 分别为当前总重量和当前总价值</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">DFS</span><span class="hljs-params">(<span class="hljs-type">int</span> index, <span class="hljs-type">int</span> sumW, <span class="hljs-type">int</span> sumC)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(index == n)&#123; <span class="hljs-comment">//已经完成对 n 件物品的选择（死胡同）</span><br>        <span class="hljs-keyword">if</span>(sumW &lt;= V &amp;&amp; sumC &gt; maxValue)&#123;<br>            maxValue = sumC; <span class="hljs-comment">//不超过背包容量时更新最大价值 maxValue</span><br>        &#125;<br>        <span class="hljs-keyword">return</span>;<br>    &#125;<br>    <span class="hljs-comment">//岔道口</span><br>    <span class="hljs-built_in">DFS</span>(index + <span class="hljs-number">1</span>, sumW, sumC); <span class="hljs-comment">//不选第 index 件物品</span><br>    <span class="hljs-built_in">DFS</span>(index + <span class="hljs-number">1</span>, sumW + w[index], sumC + c[index]); <span class="hljs-comment">//选第 index 件物品</span><br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d%d&quot;</span>, &amp;n, &amp;V);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d&quot;</span>, &amp;w[i]); <span class="hljs-comment">//每件物品的重量</span><br>    &#125;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;<br>        <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d&quot;</span>, &amp;c[i]); <span class="hljs-comment">//每件物品的价值</span><br>    &#125;<br>    <span class="hljs-built_in">DFS</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>); <span class="hljs-comment">//初始时为第 0 件物品、当前总重量和总价值均为 0</span><br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d\n&quot;</span>, maxValue);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="广度优先搜索bfs">广度优先搜索（BFS）</h2><p>给出一个 m * n 的矩阵，矩阵中的元素为 0 或 1。称位置 (x, y)与其上下左右四个位置 (x, y + 1)、(x, y - 1)、(x + 1, y)、(x - 1, y)是相邻的。如果矩阵中有若干个 1 是相邻的（不必两两相邻），那么称这些 1构成了一个 “块”。求给定的矩阵中 “块” 的个数。</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">0 </span><span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span><br><span class="hljs-symbol">0 </span><span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">0 </span><span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">0 </span><span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">1 </span><span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br><span class="hljs-symbol">1 </span><span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>上面的 6 * 7 的矩阵中，“块”的个数为 4。</p><p>实现代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstido&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> maxn = <span class="hljs-number">100</span>;<br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">node</span>&#123;<br>    <span class="hljs-type">int</span> x, y;<br>&#125; Node;<br><br><span class="hljs-type">int</span> n, m;<br><span class="hljs-type">int</span> matrix[maxn][maxn];<br><span class="hljs-type">bool</span> inq[maxn][maxn] = &#123;<span class="hljs-literal">false</span>&#125;;<br><span class="hljs-type">int</span> X[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>&#125;;<br><span class="hljs-type">int</span> Y[<span class="hljs-number">4</span>] = &#123;<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>&#125;;<br><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">judge</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(x &gt;= n || x &lt; <span class="hljs-number">0</span> || y &gt;= m || y &lt; <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    <span class="hljs-comment">//</span><br>    <span class="hljs-keyword">if</span>(matrix[x][y] == <span class="hljs-number">0</span> || inq[x][y] == <span class="hljs-literal">true</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    <span class="hljs-comment">//</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>&#125;<br><span class="hljs-comment">//</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">BFS</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span></span>&#123;<br>    queue&lt;node&gt; Q;<br>    Node.x = x, Node.y = y;<br>    Q.<span class="hljs-built_in">push</span>(Node); <span class="hljs-comment">//将结点 Node 入队</span><br>    inq[x][y] = <span class="hljs-literal">true</span>; <span class="hljs-comment">//设置(x, y) 已入过队</span><br>    <span class="hljs-keyword">while</span>(!Q.<span class="hljs-built_in">empty</span>())&#123;<br>        node top = Q.<span class="hljs-built_in">front</span>(); <span class="hljs-comment">//取出队首元素</span><br>        Q.<span class="hljs-built_in">pop</span>(); <span class="hljs-comment">//队首元素出队</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++)&#123; <span class="hljs-comment">//循环 4 次，得到 4 个相邻位置</span><br>            <span class="hljs-type">int</span> newX = top.x + X[i];<br>            <span class="hljs-type">int</span> newY = top.y + Y[i];<br>            <span class="hljs-keyword">if</span>(<span class="hljs-built_in">judge</span>(newX, newY))&#123; <span class="hljs-comment">//如果新位置 (newX, newY) 需要访问</span><br>                <span class="hljs-comment">//设置 Node 的坐标为 (newX, newY)</span><br>                Node.x = newX, Node.y = newY;<br>                Q.<span class="hljs-built_in">push</span>(Node); <span class="hljs-comment">//将结点 Node 加入队列</span><br>                inq[newX][newY] = <span class="hljs-literal">true</span>; <span class="hljs-comment">//设置位置 (newX, newY) 已入过队</span><br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d%d&quot;</span>, &amp;n, &amp;m);<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> x = <span class="hljs-number">0</span>; x &lt; n; x++)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">0</span>; y &lt; m; y++)&#123;<br>            <span class="hljs-built_in">scanf</span>(<span class="hljs-string">&quot;%d&quot;</span>, &amp;matrix[x][y]); <span class="hljs-comment">//读入 01 矩阵</span><br>        &#125;<br>    &#125;<br>    <span class="hljs-type">int</span> ans = <span class="hljs-number">0</span>; <span class="hljs-comment">//存放块数</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> x = <span class="hljs-number">0</span>; x &lt; n; x++)&#123; <span class="hljs-comment">//枚举每一个位置</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> y = <span class="hljs-number">0</span>; y &lt; m; y++)&#123;<br>            <span class="hljs-keyword">if</span>(matrix[x][y] == <span class="hljs-number">1</span> &amp;&amp; inq[x][y] == <span class="hljs-literal">false</span>)&#123;<br>                ans++; <span class="hljs-comment">//块数加 1</span><br>                <span class="hljs-built_in">BFS</span>(x, y); <span class="hljs-comment">//访问整个块，将该块所有 &quot;1&quot; 的 inq 都标记为 true</span><br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d\n&quot;</span>, ans);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>算法</category>
      
    </categories>
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gene Ontology (Go) overview</title>
    <link href="/2023/08/09/gene-ontology-go-overview/"/>
    <url>/2023/08/09/gene-ontology-go-overview/</url>
    
    <content type="html"><![CDATA[<p>本体（Ontology）是给定领域内知识体系的形式表示。本体通常由一组类（或术语或概念）组成，这些类之间具有运作的关系。基因本体论（GeneOntology GO）从三个方面描述了我们对生物领域的知识：</p><table><thead><tr class="header"><th>术语</th><th>含义</th></tr></thead><tbody><tr class="odd"><td>分子功能（Molecular Function）</td><td>基因产物进行的分子水平活动。分子功能术语描述在分子水平上发生的活动，例如“催化”或“运输”。GO分子功能术语代表活动而不是执行动作的实体（分子或复合物），并且不指定动作发生的地点、时间或背景。分子功能通常对应于单个基因产物（即蛋白质或RNA）可以执行的活动，但某些活动是由多个基因产物组成的分子复合物执行的。广义功能术语的例子有催化活性和转运蛋白活性；较窄的功能术语的例子是腺苷酸环化酶活性或Toll样受体结合。为了避免基因产物名称与其分子功能之间的混淆，GO分子功能通常附加“活性”一词（蛋白激酶将具有GO 分子功能蛋白激酶活性）。</td></tr><tr class="even"><td>细胞成分（Cellular Component）</td><td>基因产物发挥功能的相对于细胞结构的位置，可以是细胞区室（例如线粒体），也可以是它们所属的稳定大分子复合物（例如核糖体）。与GO 的其他方面不同，细胞成分类别不是指过程，而是指细胞解剖结构。</td></tr><tr class="odd"><td>生物过程（Biological Process）</td><td>更广的过程，或由多种分子活动完成的“生物程序”。广泛的生物过程术语的例子有DNA修复或信号转导。更具体的术语的例子是嘧啶核碱基生物合成过程或葡萄糖跨膜转运。请注意，生物过程并不等同于pathway。目前，GO 并未尝试表示完整描述 pathway 所需的动态或依赖性。</td></tr></tbody></table><p>在GO注释的一个例子中，人类“细胞色素c”可以通过<strong>分子功能</strong>氧化还原酶活性、<strong>生物过程</strong>氧化磷酸化和<strong>细胞成分</strong>线粒体膜间隙来描述。</p><p>GO词汇表设计为与物种无关，包括适用于原核生物和真核生物以及单细胞和多细胞生物的术语。</p><p>GO 类由定义、标签、唯一标识符和其他几个元素组成。</p><p>GO的结构可以用图来描述，其中每个GO项是一个节点，项之间的关系是节点之间的边。GO的层次结构松散，“子”术语比“父”术语更专业，但与严格的层次结构不同，一个术语可能有多个父术语。例如，生物过程术语己糖生物合成过程有两个父体：己糖代谢过程和单糖生物合成过程。这反映了生物合成过程是代谢过程的亚型并且己糖是单糖的亚型这一事实。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedhexose-biosynthetic-process.png"alt="hexose-biosynthetic-process" /><figcaption aria-hidden="true">hexose-biosynthetic-process</figcaption></figure><p>如上图所示，三个 GO域（细胞成分、生物过程和分子功能）各自由一个单独的根本体术语表示。域中的所有术语都可以将其起源追溯到根术语，尽管可能存在通过不同数量的中间术语到达本体根的许多不同路径。三个根节点互不相关，没有共同的父节点，因此GO是三个本体。一些基于图形的软件可能需要单个根节点；在这些情况下，可以添加“假”术语作为三个现有根节点的父节点。</p><p>这三个 GO本体是不相交的，这意味着来自不同本体的术语之间没有关系。然而，其他关系（例如部分关系和规范关系）确实在GO本体之间运行。例如，分子功能术语“细胞周期蛋白依赖性蛋白激酶活性”是生物过程“细胞周期”的一部分。</p><p>GO旨在代表生物学知识的当前状态，因此随着生物学知识的积累而不断修订和扩展。每周都会进行更改（大多数相对较小）。本体的修订由在生物学和计算知识表示方面拥有丰富经验的本体编辑团队管理。这些更新是GOC 本体团队和请求更新的科学家之间合作完成的。大多数请求来自进行 GO注释的科学家（这些请求通常只影响几个术语），以及来自特定生物学领域的领域专家（这些专家通常会修改包含许多术语和关系的本体论的整个“分支”）。</p><p><strong>参考文献</strong></p><p>[1] <ahref="https://geneontology.github.io/docs/ontology-documentation/">GeneOntology overview</a></p>]]></content>
    
    
    <categories>
      
      <category>Bioinformatics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Bioinformatics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>优化器指南</title>
    <link href="/2023/08/08/you-hua-qi-zhi-nan/"/>
    <url>/2023/08/08/you-hua-qi-zhi-nan/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（四）余篇</title>
    <link href="/2023/08/05/song-ci-si-yu-pian/"/>
    <url>/2023/08/05/song-ci-si-yu-pian/</url>
    
    <content type="html"><![CDATA[<center>木兰花</center><center>钱惟演</center><center>城上风光莺语乱，城下烟波春拍岸。绿杨芳草几时休，泪眼愁肠先已断。</center><center>情怀渐觉成衰晚，鸾镜朱颜惊暗换。昔年多病厌芳尊，今日芳尊惟恐浅。</center><p><br></p><center>木兰花 春景</center><center>宋祁</center><center>东城渐觉风光好，縠皱波纹迎客掉。绿杨烟外晓云轻，红杏枝头春意闹。</center><center>浮生长恨欢娱少，肯爱千金轻一笑？为君持酒劝斜阳，且向花间留晚照。</center><p><br></p><center>踏莎行</center><center>寇准</center><center>春色将阑，莺声渐老，红英落尽青梅小。画堂人静雨蒙蒙，屏山半掩余香袅。</center><center>密约沉沉，离情杳杳，菱花尘满慵将照。倚楼无语欲销魂，长空暗淡逢芳草。</center><p><br></p>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（五）图神经网络（GNN）的理论动机</title>
    <link href="/2023/08/05/tu-shen-jing-wang-luo-xue-xi-ri-ji-wu-tu-shen-jing-wang-luo-gnn-de-li-lun-dong-ji/"/>
    <url>/2023/08/05/tu-shen-jing-wang-luo-xue-xi-ri-ji-wu-tu-shen-jing-wang-luo-gnn-de-li-lun-dong-ji/</url>
    
    <content type="html"><![CDATA[<h2 id="gnn-与图信号">GNN 与图信号</h2><h2 id="gnn-与概率图模型">GNN 与概率图模型</h2><h2 id="gnn-与图同构">GNN 与图同构</h2>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文研读-Deep Learning with Differential Privacy</title>
    <link href="/2023/07/25/lun-wen-yan-du-deep-learning-with-differential-privacy/"/>
    <url>/2023/07/25/lun-wen-yan-du-deep-learning-with-differential-privacy/</url>
    
    <content type="html"><![CDATA[<figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230725160345731.png"alt="image-20230725160345731" /><figcaption aria-hidden="true">image-20230725160345731</figcaption></figure><p>其中，对于高斯机制，<span class="math inline">\(\sigma\)</span>如下式： <span class="math display">\[\sigma^2=\frac{2s^2\log(1.25/\delta)}{\epsilon^2}\]</span></p><p>通过实验作者观察到，模型精度对训练参数（例如批量大小和噪声水平）比神经网络的结构更敏感。非凸学习本质上不太稳定，聚合时需要更大的batch size，更大的 batch size带来了更大的隐私消耗，合理的权衡是使每个时期的批次数量与所需的时期数量具有相同的数量级。从相对较大的学习率开始，然后在几个时期内将其线性衰减到较小的值，并在之后保持恒定，有一定的好处。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Differential Privacy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2023 深度单细胞分析</title>
    <link href="/2023/07/11/shen-du-dan-xi-bao-fen-xi/"/>
    <url>/2023/07/11/shen-du-dan-xi-bao-fen-xi/</url>
    
    <content type="html"><![CDATA[<h2 id="section">2023</h2><table><thead><tr class="header"><th>Paper</th><th style="text-align: left;">Function</th><th>Underlying deep models</th><th>Accesses</th></tr></thead><tbody><tr class="odd"><td>ScCCL: Single-Cell Data Clustering Based on Self-SupervisedContrastive Learning</td><td style="text-align: left;">Clustering</td><td></td><td>https://github.com/LuckyxiaoLin/ScCCL.git</td></tr><tr class="even"><td>Deep Multi-Constraint Soft Clustering Analysis for Single-CellRNA-Seq Data via Zero-Inflated Autoencoder Embedding</td><td style="text-align: left;"></td><td></td><td>https://github.com/leaf233/scMCKC</td></tr><tr class="odd"><td>CosTaL: an accurate and scalable graph-based clustering algorithmfor high-dimensional single-cell data analysis</td><td style="text-align: left;"></td><td></td><td>https://github.com/li000678/CosTaL</td></tr><tr class="even"><td>Deep generative modeling and clustering of single cell Hi-Cdata</td><td style="text-align: left;"></td><td></td><td>https://github.com/zhoujt1994/scHiCluster</td></tr><tr class="odd"><td>IsoCell: An Approach to Enhance Single Cell Clustering byIntegrating Isoform-Level Expression Through Orthogonal Projection</td><td style="text-align: left;"></td><td></td><td>https://github.com/genemine/IsoCell</td></tr><tr class="even"><td>Clustering Single-Cell RNA Sequence Data Using Information Maximizedand Noise-Invariant Representations</td><td style="text-align: left;"></td><td></td><td>https://github.com/arnabkmondal/sc-INDC</td></tr><tr class="odd"><td>SSNMDI: a novel joint learning model of semi-supervised non-negativematrix factorization and data imputation for clustering of single-cellRNA-seq data</td><td style="text-align: left;"></td><td></td><td>https://github.com/yushanqiu/SSNMDI</td></tr><tr class="even"><td>scENT for Revealing Gene Clusters From Single-Cell RNA-Seq Data</td><td style="text-align: left;"></td><td></td><td>None</td></tr><tr class="odd"><td>Denoising adaptive deep clustering with self-attention mechanism onsingle-cell sequencing data</td><td style="text-align: left;"></td><td></td><td>https://github.com/LRX2022/scDASFK</td></tr><tr class="even"><td>scDCCA: deep contrastive clustering for single-cell RNA-seq databased on auto-encoder network</td><td style="text-align: left;"></td><td></td><td>https://github.com/WJ319/scDCCA</td></tr><tr class="odd"><td>scBKAP: A Clustering Model for Single-Cell RNA-Seq Data Based onBisecting K-Means</td><td style="text-align: left;"></td><td></td><td>https://github.com/YuBinLab-QUST/scBKAP/</td></tr><tr class="even"><td>scBGEDA: deep single-cell clustering analysis via a dual denoisingautoencoder with bipartite graph ensemble clustering</td><td style="text-align: left;"></td><td></td><td>https://github.com/wangyh082/scBGEDA</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Bioinformatics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Bioinformatics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（四）Fluid 主题</title>
    <link href="/2023/07/11/github-pages-ge-ren-bo-ke-da-jian-si-fluid-zhu-ti/"/>
    <url>/2023/07/11/github-pages-ge-ren-bo-ke-da-jian-si-fluid-zhu-ti/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率论（一） 贝叶斯公式</title>
    <link href="/2023/06/16/gai-lu-lun-yi-bei-xie-si-ding-li/"/>
    <url>/2023/06/16/gai-lu-lun-yi-bei-xie-si-ding-li/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>概率</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>WandB 学习日记（一）Tutorials</title>
    <link href="/2023/06/12/wandb-xue-xi-ri-ji-yi-doc/"/>
    <url>/2023/06/12/wandb-xue-xi-ri-ji-yi-doc/</url>
    
    <content type="html"><![CDATA[<h2 id="track-experiments">Track experiments</h2><p>快速实验是机器学习的基础。在本教程中，我们使用 W&amp;B来跟踪和可视化实验，以便我们可以快速迭代和理解我们的结果。</p><h3 id="a-shared-dashboard-for-your-experiments">A shared dashboard foryour experiments</h3><p>只需几行代码，您就可以获得丰富的、交互式的、可共享的仪表板，您可以在这里看到<ahref="https://wandb.ai/wandb/wandb_example?_gl=1*1ycseye*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MDE0OC41MC4wLjA.">自己的dashboard</a>。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedPell4Oo.png"alt="dashboard" /><figcaption aria-hidden="true">dashboard</figcaption></figure><h3 id="data-privacy">Data &amp; Privacy</h3><p>我们非常重视安全性，我们的云托管 dashboard使用行业标准最佳加密实践。如果您正在使用无法离开企业集群的数据集，我们可以提供<ahref="https://docs.wandb.com/self-hosted">本地安装</a>。</p><p>下载所有数据并将其导出到其他工具也很容易——例如在 Jupyter笔记本中进行自定义分析。下面是关于我们 <ahref="https://docs.wandb.com/library/api">API</a> 的更多信息。</p><h3 id="install-wandb-library-and-login">Install <code>wandb</code>library and login</h3><p>首先安装库并登录到您的免费帐户。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">!pip install wandb -qU<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Log in to your W&amp;B account</span><br><span class="hljs-keyword">import</span> wandb<br>wandb.login()<br></code></pre></td></tr></table></figure><h3 id="run-an-experiment">Run an experiment</h3><p>1️⃣. 开始新的运行并传入超参数进行跟踪</p><p>2️⃣. 训练或评估的日志指标</p><p>3️⃣. 在 dashboard 中可视化结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><br><span class="hljs-comment"># Launch 5 simulated experiments</span><br>total_runs = <span class="hljs-number">5</span><br><span class="hljs-keyword">for</span> run <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total_runs):<br>  <span class="hljs-comment"># 🐝 1️⃣ Start a new run to track this script</span><br>  wandb.init(<br>      <span class="hljs-comment"># Set the project where this run will be logged</span><br>      project=<span class="hljs-string">&quot;basic-intro&quot;</span>, <br>      <span class="hljs-comment"># We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)</span><br>      name=<span class="hljs-string">f&quot;experiment_<span class="hljs-subst">&#123;run&#125;</span>&quot;</span>, <br>      <span class="hljs-comment"># Track hyperparameters and run metadata</span><br>      config=&#123;<br>      <span class="hljs-string">&quot;learning_rate&quot;</span>: <span class="hljs-number">0.02</span>,<br>      <span class="hljs-string">&quot;architecture&quot;</span>: <span class="hljs-string">&quot;CNN&quot;</span>,<br>      <span class="hljs-string">&quot;dataset&quot;</span>: <span class="hljs-string">&quot;CIFAR-100&quot;</span>,<br>      <span class="hljs-string">&quot;epochs&quot;</span>: <span class="hljs-number">10</span>,<br>      &#125;)<br>  <br>  <span class="hljs-comment"># This simple block simulates a training loop logging metrics</span><br>  epochs = <span class="hljs-number">10</span><br>  offset = random.random() / <span class="hljs-number">5</span><br>  <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, epochs):<br>      acc = <span class="hljs-number">1</span> - <span class="hljs-number">2</span> ** -epoch - random.random() / epoch - offset<br>      loss = <span class="hljs-number">2</span> ** -epoch + random.random() / epoch + offset<br>      <br>      <span class="hljs-comment"># 🐝 2️⃣ Log metrics from your script to W&amp;B</span><br>      wandb.log(&#123;<span class="hljs-string">&quot;acc&quot;</span>: acc, <span class="hljs-string">&quot;loss&quot;</span>: loss&#125;)<br>      <br>  <span class="hljs-comment"># Mark the run as finished</span><br>  wandb.finish()<br></code></pre></td></tr></table></figure><p>3️⃣ 当您运行此代码时，您可以通过单击上面的任何 👆 wandb链接找到您的交互式 dashboard。</p><h3 id="simple-pytorch-neural-network">Simple Pytorch NeuralNetwork</h3><p>运行此模型以训练一个简单的 MNIST分类器，然后单击项目页面链接以实时查看您的结果流到 W&amp;B 项目。</p><p>wandb 中的任何运行都会自动记录 <ahref="https://docs.wandb.ai/ref/app/pages/run-page#charts-tab">metrics</a>,<a href="https://docs.wandb.ai/ref/app/pages/run-page#system-tab">systeminformation</a>, <ahref="https://docs.wandb.ai/ref/app/pages/run-page#overview-tab">hyperparameters</a>,<a href="https://docs.wandb.ai/ref/app/pages/run-page#logs-tab">terminaloutput</a> ，您将看到一个包含模型输入和输出的交互式表格。</p><h4 id="set-up-dataloader">Set up Dataloader</h4><p>要运行此示例，我们需要安装 PyTorch。如果您使用的是 GoogleColab，则它已经预装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">!pip install torch torchvision<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> wandb<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch, torchvision<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T<br><br>device = <span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_dataloader</span>(<span class="hljs-params">is_train, batch_size, <span class="hljs-built_in">slice</span>=<span class="hljs-number">5</span></span>):<br>    <span class="hljs-string">&quot;Get a training dataloader&quot;</span><br>    full_dataset = torchvision.datasets.MNIST(root=<span class="hljs-string">&quot;.&quot;</span>, train=is_train, transform=T.ToTensor(), download=<span class="hljs-literal">True</span>)<br>    sub_dataset = torch.utils.data.Subset(full_dataset, indices=<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(full_dataset), <span class="hljs-built_in">slice</span>))<br>    loader = torch.utils.data.DataLoader(dataset=sub_dataset, <br>                                         batch_size=batch_size, <br>                                         shuffle=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>, <br>                                         pin_memory=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> loader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>(<span class="hljs-params">dropout</span>):<br>    <span class="hljs-string">&quot;A simple model&quot;</span><br>    model = nn.Sequential(nn.Flatten(),<br>                         nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">256</span>),<br>                         nn.BatchNorm1d(<span class="hljs-number">256</span>),<br>                         nn.ReLU(),<br>                         nn.Dropout(dropout),<br>                         nn.Linear(<span class="hljs-number">256</span>,<span class="hljs-number">10</span>)).to(device)<br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">validate_model</span>(<span class="hljs-params">model, valid_dl, loss_func, log_images=<span class="hljs-literal">False</span>, batch_idx=<span class="hljs-number">0</span></span>):<br>    <span class="hljs-string">&quot;Compute performance of the model on the validation dataset and log a wandb.Table&quot;</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    val_loss = <span class="hljs-number">0.</span><br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        correct = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_dl):<br>            images, labels = images.to(device), labels.to(device)<br><br>            <span class="hljs-comment"># Forward pass ➡</span><br>            outputs = model(images)<br>            val_loss += loss_func(outputs, labels)*labels.size(<span class="hljs-number">0</span>)<br><br>            <span class="hljs-comment"># Compute accuracy and accumulate</span><br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>            <span class="hljs-comment"># Log one batch of images to the dashboard, always same batch_idx.</span><br>            <span class="hljs-keyword">if</span> i==batch_idx <span class="hljs-keyword">and</span> log_images:<br>                log_image_table(images, predicted, labels, outputs.softmax(dim=<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> val_loss / <span class="hljs-built_in">len</span>(valid_dl.dataset), correct / <span class="hljs-built_in">len</span>(valid_dl.dataset)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_image_table</span>(<span class="hljs-params">images, predicted, labels, probs</span>):<br>    <span class="hljs-string">&quot;Log a wandb.Table with (img, pred, target, scores)&quot;</span><br>    <span class="hljs-comment"># 🐝 Create a wandb Table to log images, labels and predictions to</span><br>    table = wandb.Table(columns=[<span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;pred&quot;</span>, <span class="hljs-string">&quot;target&quot;</span>]+[<span class="hljs-string">f&quot;score_<span class="hljs-subst">&#123;i&#125;</span>&quot;</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)])<br>    <span class="hljs-keyword">for</span> img, pred, targ, prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(images.to(<span class="hljs-string">&quot;cpu&quot;</span>), predicted.to(<span class="hljs-string">&quot;cpu&quot;</span>), labels.to(<span class="hljs-string">&quot;cpu&quot;</span>), probs.to(<span class="hljs-string">&quot;cpu&quot;</span>)):<br>        table.add_data(wandb.Image(img[<span class="hljs-number">0</span>].numpy()*<span class="hljs-number">255</span>), pred, targ, *prob.numpy())<br>    wandb.log(&#123;<span class="hljs-string">&quot;predictions_table&quot;</span>:table&#125;, commit=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><h4 id="train-your-model">Train Your Model</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Launch 5 experiments, trying different dropout rates</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    <span class="hljs-comment"># 🐝 initialise a wandb run</span><br>    wandb.init(<br>        project=<span class="hljs-string">&quot;pytorch-intro&quot;</span>,<br>        config=&#123;<br>            <span class="hljs-string">&quot;epochs&quot;</span>: <span class="hljs-number">10</span>,<br>            <span class="hljs-string">&quot;batch_size&quot;</span>: <span class="hljs-number">128</span>,<br>            <span class="hljs-string">&quot;lr&quot;</span>: <span class="hljs-number">1e-3</span>,<br>            <span class="hljs-string">&quot;dropout&quot;</span>: random.uniform(<span class="hljs-number">0.01</span>, <span class="hljs-number">0.80</span>),<br>            &#125;)<br>    <br>    <span class="hljs-comment"># Copy your config </span><br>    config = wandb.config<br><br>    <span class="hljs-comment"># Get the data</span><br>    train_dl = get_dataloader(is_train=<span class="hljs-literal">True</span>, batch_size=config.batch_size)<br>    valid_dl = get_dataloader(is_train=<span class="hljs-literal">False</span>, batch_size=<span class="hljs-number">2</span>*config.batch_size)<br>    n_steps_per_epoch = math.ceil(<span class="hljs-built_in">len</span>(train_dl.dataset) / config.batch_size)<br>    <br>    <span class="hljs-comment"># A simple MLP model</span><br>    model = get_model(config.dropout)<br><br>    <span class="hljs-comment"># Make the loss and optimizer</span><br>    loss_func = nn.CrossEntropyLoss()<br>    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)<br><br>   <span class="hljs-comment"># Training</span><br>    example_ct = <span class="hljs-number">0</span><br>    step_ct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>        model.train()<br>        <span class="hljs-keyword">for</span> step, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dl):<br>            images, labels = images.to(device), labels.to(device)<br><br>            outputs = model(images)<br>            train_loss = loss_func(outputs, labels)<br>            optimizer.zero_grad()<br>            train_loss.backward()<br>            optimizer.step()<br>            <br>            example_ct += <span class="hljs-built_in">len</span>(images)<br>            metrics = &#123;<span class="hljs-string">&quot;train/train_loss&quot;</span>: train_loss, <br>                       <span class="hljs-string">&quot;train/epoch&quot;</span>: (step + <span class="hljs-number">1</span> + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, <br>                       <span class="hljs-string">&quot;train/example_ct&quot;</span>: example_ct&#125;<br>            <br>            <span class="hljs-keyword">if</span> step + <span class="hljs-number">1</span> &lt; n_steps_per_epoch:<br>                <span class="hljs-comment"># 🐝 Log train metrics to wandb </span><br>                wandb.log(metrics)<br>                <br>            step_ct += <span class="hljs-number">1</span><br><br>        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-<span class="hljs-number">1</span>)))<br><br>        <span class="hljs-comment"># 🐝 Log train and validation metrics to wandb</span><br>        val_metrics = &#123;<span class="hljs-string">&quot;val/val_loss&quot;</span>: val_loss, <br>                       <span class="hljs-string">&quot;val/val_accuracy&quot;</span>: accuracy&#125;<br>        wandb.log(&#123;**metrics, **val_metrics&#125;)<br>        <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Train Loss: <span class="hljs-subst">&#123;train_loss:<span class="hljs-number">.3</span>f&#125;</span>, Valid Loss: <span class="hljs-subst">&#123;val_loss:3f&#125;</span>, Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.2</span>f&#125;</span>&quot;</span>)<br><br>    <span class="hljs-comment"># If you had a test set, this is how you could log it as a Summary metric</span><br>    wandb.summary[<span class="hljs-string">&#x27;test_accuracy&#x27;</span>] = <span class="hljs-number">0.8</span><br><br>    <span class="hljs-comment"># 🐝 Close your wandb run </span><br>    wandb.finish()<br></code></pre></td></tr></table></figure><p>您现在已经使用 wandb 训练了您的第一个模型！ 👆 单击上面的 wandb链接查看您的指标</p><h3 id="try-wb-alerts">Try W&amp;B Alerts</h3><p><strong><a href="https://docs.wandb.ai/guides/track/alert">W&amp;BAlerts</a></strong> 允许您将从 Python 代码触发的警报发送到您的 Slack或电子邮件。第一次发送 Slack 或电子邮件警报时，需要执行 2个步骤，这些警报由您的代码触发：</p><p>1) 在你的 W&amp;B <a href="https://wandb.ai/settings">UserSettings</a> 开启警报</p><p>2) 添加 <code>wandb.alert()</code> 到你的代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">wandb.alert(<br>    title=<span class="hljs-string">&quot;Low accuracy&quot;</span>, <br>    text=<span class="hljs-string">f&quot;Accuracy is below the acceptable threshold&quot;</span><br>)<br></code></pre></td></tr></table></figure><p>请参阅下面的最小示例以了解如何使用 wandb.alert，您可以在此处找到 <ahref="https://docs.wandb.ai/guides/track/alert">W&amp;BAlerts</a>的完整文档</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Start a wandb run</span><br>wandb.init(project=<span class="hljs-string">&quot;pytorch-intro&quot;</span>)<br><br><span class="hljs-comment"># Simulating a model training loop</span><br>acc_threshold = <span class="hljs-number">0.3</span><br><span class="hljs-keyword">for</span> training_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<br><br>    <span class="hljs-comment"># Generate a random number for accuracy</span><br>    accuracy = <span class="hljs-built_in">round</span>(random.random() + random.random(), <span class="hljs-number">3</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Accuracy is: <span class="hljs-subst">&#123;accuracy&#125;</span>, <span class="hljs-subst">&#123;acc_threshold&#125;</span>&#x27;</span>)<br>    <br>    <span class="hljs-comment"># 🐝 Log accuracy to wandb</span><br>    wandb.log(&#123;<span class="hljs-string">&quot;Accuracy&quot;</span>: accuracy&#125;)<br><br>    <span class="hljs-comment"># 🔔 If the accuracy is below the threshold, fire a W&amp;B Alert and stop the run</span><br>    <span class="hljs-keyword">if</span> accuracy &lt;= acc_threshold:<br>        <span class="hljs-comment"># 🐝 Send the wandb Alert</span><br>        wandb.alert(<br>            title=<span class="hljs-string">&#x27;Low Accuracy&#x27;</span>,<br>            text=<span class="hljs-string">f&#x27;Accuracy <span class="hljs-subst">&#123;accuracy&#125;</span> at step <span class="hljs-subst">&#123;training_step&#125;</span> is below the acceptable theshold, <span class="hljs-subst">&#123;acc_threshold&#125;</span>&#x27;</span>,<br>        )<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Alert triggered&#x27;</span>)<br>        <span class="hljs-keyword">break</span><br><br><span class="hljs-comment"># Mark the run as finished (useful in Jupyter notebooks)</span><br>wandb.finish()<br></code></pre></td></tr></table></figure><h2 id="visualize-predictions">Visualize predictions</h2><p>这包括如何在训练过程中使用 PyTorch 对 MNIST数据进行跟踪、可视化和比较模型预测。</p><p>你将学到如何：</p><ol type="1"><li>在模型训练或评估期间将指标、图像、文本等记录到<code>wandb.Table()</code></li><li>查看、排序、筛选、分组、加入、交互式查询和探索这些表</li><li>比较模型预测或结果：动态地跨越特定图像、超参数/模型版本或时间步长。</li></ol><h3 id="examples">Examples</h3><h4 id="compare-predicted-scores-for-specific-images">Compare predictedscores for specific images</h4><p><ahref="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*6z1980*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#compare-predictions-after-1-vs-5-epochs">实例：比较1 和 5 个训练周期后的预测</a></p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedNMme6Qj.png"alt="1 epoch vs 5 epochs of training" /><figcaption aria-hidden="true">1 epoch vs 5 epochs oftraining</figcaption></figure><p>直方图比较了两个模型之间的每类分数。每个直方图中顶部的绿色条代表模型“CNN-2,1 epoch”（id 0），它只训练了 1 个 epoch。底部的紫色条代表模型“CNN-2, 5epochs” (id 1)，它训练了 5 个epochs。图像被过滤到模型不一致的情况。例如，在第一行中，“4”在 1个时期后在所有可能的数字中获得高分，但在 5个时期后，它在正确标签上得分最高，而在其余部分得分非常低。</p><h4 id="focus-on-top-errors-over-time">Focus on top errors overtime</h4><p><ahref="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*1nxbzl7*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#top-errors-over-time">实例→</a></p><p>查看完整测试数据的不正确预测（过滤 "guess" != "truth"的行）。请注意，在 1 个训练时期后有 229 个错误猜测，但在 5 个时期后只有98 个。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined7g8nodn.png"alt="side by side, 1 vs 5 epochs of training" /><figcaption aria-hidden="true">side by side, 1 vs 5 epochs oftraining</figcaption></figure><h4 id="compare-model-performance-and-find-patterns">Compare modelperformance and find patterns</h4><p><ahref="https://wandb.ai/stacey/table-quickstart/reports/CNN-2-Progress-over-Training-Time--Vmlldzo3NDY5ODU?_gl=1*8r828v*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjU0ODg1NC40LjEuMTY4NjU1MTg0Ni4zOC4wLjA.#false-positives-grouped-by-guess">查看实例中的完整详细信息→</a></p><p>过滤出正确答案，然后按猜测分组，以查看错误分类图像的示例和真实标签的基本分布——并排显示两个模型。具有2X layer sizes和学习率的模型变体在左侧，基线在右侧。请注意，对于每个猜测的类，基线都会犯更多的错误。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedi5PP9AE.png"alt="grouped errors for baseline vs double variant" /><figcaption aria-hidden="true">grouped errors for baseline vs doublevariant</figcaption></figure><h3 id="sign-up-or-login">Sign up or login</h3><p><a href="https://wandb.ai/login">Sign up or login</a> W&amp;B以在浏览器中查看您的实验并与之互动。</p><p>在此示例中，我们使用 Google Colab作为方便的托管环境，但您可以从任何地方运行自己的训练脚本，并使用 W&amp;B的实验跟踪工具可视化指标。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">!pip install wandb -qqq<br></code></pre></td></tr></table></figure><p>登录您的帐户</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> wandb<br>wandb.login()<br><br>WANDB_PROJECT = <span class="hljs-string">&quot;mnist-viz&quot;</span><br></code></pre></td></tr></table></figure><h3 id="setup">0. Setup</h3><p>安装依赖项，下载 MNIST，并使用 PyTorch 创建训练和测试数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> T <br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br>device = <span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br><br><span class="hljs-comment"># create train and test dataloaders</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_dataloader</span>(<span class="hljs-params">is_train, batch_size, <span class="hljs-built_in">slice</span>=<span class="hljs-number">5</span></span>):<br>    <span class="hljs-string">&quot;Get a training dataloader&quot;</span><br>    ds = torchvision.datasets.MNIST(root=<span class="hljs-string">&quot;.&quot;</span>, train=is_train, transform=T.ToTensor(), download=<span class="hljs-literal">True</span>)<br>    loader = torch.utils.data.DataLoader(dataset=ds, <br>                                         batch_size=batch_size, <br>                                         shuffle=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>, <br>                                         pin_memory=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> loader<br></code></pre></td></tr></table></figure><h3 id="define-the-model-and-training-schedule">1. Define the model andtraining schedule</h3><ul><li>设置要运行的纪元数，其中每个纪元包含一个训练步骤和一个验证（测试）步骤。（可选）配置每个测试步骤要记录的数据量。这里要可视化的批次数和每批次的图像数设置得较低，以简化演示。</li><li>定义一个简单的卷积神经网络（遵循 <ahref="https://github.com/yunjey/pytorch-tutorial">pytorch-tutorial</a>代码）。</li><li>使用 PyTorch 加载训练集和测试集</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Number of epochs to run</span><br><span class="hljs-comment"># Each epoch includes a training step and a test step, so this sets</span><br><span class="hljs-comment"># the number of tables of test predictions to log</span><br>EPOCHS = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># Number of batches to log from the test data for each test step</span><br><span class="hljs-comment"># (default set low to simplify demo)</span><br>NUM_BATCHES_TO_LOG = <span class="hljs-number">10</span> <span class="hljs-comment">#79</span><br><br><span class="hljs-comment"># Number of images to log per test batch</span><br><span class="hljs-comment"># (default set low to simplify demo)</span><br>NUM_IMAGES_PER_BATCH = <span class="hljs-number">32</span> <span class="hljs-comment">#128</span><br><br><span class="hljs-comment"># training configuration and hyperparameters</span><br>NUM_CLASSES = <span class="hljs-number">10</span><br>BATCH_SIZE = <span class="hljs-number">32</span><br>LEARNING_RATE = <span class="hljs-number">0.001</span><br>L1_SIZE = <span class="hljs-number">32</span><br>L2_SIZE = <span class="hljs-number">64</span><br><span class="hljs-comment"># changing this may require changing the shape of adjacent layers</span><br>CONV_KERNEL_SIZE = <span class="hljs-number">5</span><br><br><span class="hljs-comment"># define a two-layer convolutional neural network</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br>        self.layer1 = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, L1_SIZE, CONV_KERNEL_SIZE, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(L1_SIZE),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.layer2 = nn.Sequential(<br>            nn.Conv2d(L1_SIZE, L2_SIZE, CONV_KERNEL_SIZE, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.BatchNorm2d(L2_SIZE),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>        self.fc = nn.Linear(<span class="hljs-number">7</span>*<span class="hljs-number">7</span>*L2_SIZE, NUM_CLASSES)<br>        self.softmax = nn.Softmax(NUM_CLASSES)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># uncomment to see the shape of a given layer:</span><br>        <span class="hljs-comment">#print(&quot;x: &quot;, x.size())</span><br>        out = self.layer1(x)<br>        out = self.layer2(out)<br>        out = out.reshape(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        out = self.fc(out)<br>        <span class="hljs-keyword">return</span> out<br><br>train_loader = get_dataloader(is_train=<span class="hljs-literal">True</span>, batch_size=BATCH_SIZE)<br>test_loader = get_dataloader(is_train=<span class="hljs-literal">False</span>, batch_size=<span class="hljs-number">2</span>*BATCH_SIZE)<br><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="run-training-and-log-test-predictions">2. Run training and logtest predictions</h3><p>对于每个时期，运行一个训练步骤和一个测试步骤。对于每个测试步骤，创建一个wandb.Table()来存储测试预测。这些可以在您的浏览器中进行可视化、动态查询和并排比较。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ✨ W&amp;B: Initialize a new run to track this model&#x27;s training</span><br>wandb.init(project=<span class="hljs-string">&quot;table-quickstart&quot;</span>)<br><br><span class="hljs-comment"># ✨ W&amp;B: Log hyperparameters using config</span><br>cfg = wandb.config<br>cfg.update(&#123;<span class="hljs-string">&quot;epochs&quot;</span> : EPOCHS, <span class="hljs-string">&quot;batch_size&quot;</span>: BATCH_SIZE, <span class="hljs-string">&quot;lr&quot;</span> : LEARNING_RATE,<br>            <span class="hljs-string">&quot;l1_size&quot;</span> : L1_SIZE, <span class="hljs-string">&quot;l2_size&quot;</span>: L2_SIZE,<br>            <span class="hljs-string">&quot;conv_kernel&quot;</span> : CONV_KERNEL_SIZE,<br>            <span class="hljs-string">&quot;img_count&quot;</span> : <span class="hljs-built_in">min</span>(<span class="hljs-number">10000</span>, NUM_IMAGES_PER_BATCH*NUM_BATCHES_TO_LOG)&#125;)<br><br><span class="hljs-comment"># define model, loss, and optimizer</span><br>model = ConvNet(NUM_CLASSES).to(device)<br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)<br><br><span class="hljs-comment"># convenience funtion to log predictions for a batch of test images</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">log_test_predictions</span>(<span class="hljs-params">images, labels, outputs, predicted, test_table, log_counter</span>):<br>  <span class="hljs-comment"># obtain confidence scores for all classes</span><br>  scores = F.softmax(outputs.data, dim=<span class="hljs-number">1</span>)<br>  log_scores = scores.cpu().numpy()<br>  log_images = images.cpu().numpy()<br>  log_labels = labels.cpu().numpy()<br>  log_preds = predicted.cpu().numpy()<br>  <span class="hljs-comment"># adding ids based on the order of the images</span><br>  _<span class="hljs-built_in">id</span> = <span class="hljs-number">0</span><br>  <span class="hljs-keyword">for</span> i, l, p, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(log_images, log_labels, log_preds, log_scores):<br>    <span class="hljs-comment"># add required info to data table:</span><br>    <span class="hljs-comment"># id, image pixels, model&#x27;s guess, true label, scores for all classes</span><br>    img_id = <span class="hljs-built_in">str</span>(_<span class="hljs-built_in">id</span>) + <span class="hljs-string">&quot;_&quot;</span> + <span class="hljs-built_in">str</span>(log_counter)<br>    test_table.add_data(img_id, wandb.Image(i), p, l, *s)<br>    _<span class="hljs-built_in">id</span> += <span class="hljs-number">1</span><br>    <span class="hljs-keyword">if</span> _<span class="hljs-built_in">id</span> == NUM_IMAGES_PER_BATCH:<br>      <span class="hljs-keyword">break</span><br><br><span class="hljs-comment"># train the model</span><br>total_step = <span class="hljs-built_in">len</span>(train_loader)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):<br>    <span class="hljs-comment"># training step</span><br>    <span class="hljs-keyword">for</span> i, (images, labels) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        images = images.to(device)<br>        labels = labels.to(device)<br>        <span class="hljs-comment"># forward pass</span><br>        outputs = model(images)<br>        loss = criterion(outputs, labels)<br>        <span class="hljs-comment"># backward and optimize</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>  <br>        <span class="hljs-comment"># ✨ W&amp;B: Log loss over training steps, visualized in the UI live</span><br>        wandb.log(&#123;<span class="hljs-string">&quot;loss&quot;</span> : loss&#125;)<br>        <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;&#x27;</span><br>                .<span class="hljs-built_in">format</span>(epoch+<span class="hljs-number">1</span>, EPOCHS, i+<span class="hljs-number">1</span>, total_step, loss.item()))<br>            <br><br>    <span class="hljs-comment"># ✨ W&amp;B: Create a Table to store predictions for each test step</span><br>    columns=[<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;guess&quot;</span>, <span class="hljs-string">&quot;truth&quot;</span>]<br>    <span class="hljs-keyword">for</span> digit <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>      columns.append(<span class="hljs-string">&quot;score_&quot;</span> + <span class="hljs-built_in">str</span>(digit))<br>    test_table = wandb.Table(columns=columns)<br><br>    <span class="hljs-comment"># test the model</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    log_counter = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        correct = <span class="hljs-number">0</span><br>        total = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> test_loader:<br>            images = images.to(device)<br>            labels = labels.to(device)<br>            outputs = model(images)<br>            _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> log_counter &lt; NUM_BATCHES_TO_LOG:<br>              log_test_predictions(images, labels, outputs, predicted, test_table, log_counter)<br>              log_counter += <span class="hljs-number">1</span><br>            total += labels.size(<span class="hljs-number">0</span>)<br>            correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()<br><br>        acc = <span class="hljs-number">100</span> * correct / total<br>        <span class="hljs-comment"># ✨ W&amp;B: Log accuracy across training epochs, to visualize in the UI</span><br>        wandb.log(&#123;<span class="hljs-string">&quot;epoch&quot;</span> : epoch, <span class="hljs-string">&quot;acc&quot;</span> : acc&#125;)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test Accuracy of the model on the 10000 test images: &#123;&#125; %&#x27;</span>.<span class="hljs-built_in">format</span>(acc))<br><br>    <span class="hljs-comment"># ✨ W&amp;B: Log predictions table to wandb</span><br>    wandb.log(&#123;<span class="hljs-string">&quot;test_predictions&quot;</span> : test_table&#125;)<br><br><span class="hljs-comment"># ✨ W&amp;B: Mark the run as complete (useful for multi-cell notebook)</span><br>wandb.finish()<br></code></pre></td></tr></table></figure><h2 id="tune-hyperparameters">Tune hyperparameters</h2><p><ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W&amp;B.ipynb">在此处试用Colab Notebook →</a></p><p>在高维超参数空间中搜索以找到性能最高的模型可能会很快变得笨拙。超参数扫描提供了一种有组织且高效的方式来进行模型大逃杀并选择最准确的模型。他们通过自动搜索超参数值的组合（例如learning rate, batch size, number of hidden layers, optimizertype）来找到最佳值来实现这一点。</p><p>在本教程中，我们将了解如何使用 Weights &amp; Biases 通过 3个简单步骤运行复杂的超参数扫描。</p><h3 id="follow-along-with-a-video-tutorial">Follow along with a <ahref="http://wandb.me/sweeps-video">video tutorial</a>!</h3><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedWVKkMWw.png"alt="tune hyperparameters" /><figcaption aria-hidden="true">tune hyperparameters</figcaption></figure><h3 id="setup-1">Setup</h3><p>首先安装实验跟踪库并设置您的免费 W&amp;B 帐户：</p><ol type="1"><li>使用 <code>pip install</code> 安装</li><li><code>import</code> Python 所需依赖</li><li><code>.login()</code> 这样您就可以将指标记录到您的项目中</li></ol><p>如果您以前从未使用过 Weights &amp;Biases，登录电话会给您一个注册帐户的链接。 W&amp;B可免费用于个人和学术项目！</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">!pip install wandb -Uq<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> wandb<br><br>wandb.login()<br></code></pre></td></tr></table></figure><h3 id="step-1.-define-the-sweep">Step 1️⃣. Define the Sweep</h3><p>从根本上说，Sweep将尝试一堆超参数值的策略与评估它们的代码结合在一起。您只需要以<ahref="https://docs.wandb.com/sweeps/configuration">配置</a>的形式定义您的策略。</p><p>当您像这样在笔记本中设置 Sweep时，该配置对象是一个嵌套字典。当您通过命令行运行 Sweep时，配置对象是一个 <ahref="https://docs.wandb.com/sweeps/quickstart#2-sweep-config">YAMLfile</a>。</p><p>让我们一起了解 Sweep配置的定义。我们会慢慢来，这样我们就有机会解释每个组件。在典型的 Sweep管道中，此步骤将在单个分配中完成。</p><h4 id="pick-a-method">Pick a <code>method</code></h4><p>我们需要定义的第一件事是选择新参数值的 <code>method</code>。</p><p>我们提供以下搜索 <code>methods</code>：</p><ul><li><strong><code>grid</code> Search </strong>–迭代超参数值的每个组合。非常有效，但计算量大。</li><li><strong><code>random</code> Search</strong> – 根据提供的<code>distribution</code> 随机选择每个新组合。出乎意料的有效！</li><li><strong><code>bayesian</code> Search</strong> –创建一个度量分数作为超参数函数的概率模型，并选择具有提高度量的高概率的参数。适用于少量连续参数但扩展性差。</li></ul><p><code>random</code> 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">sweep_config = &#123;<br>    <span class="hljs-string">&#x27;method&#x27;</span>: <span class="hljs-string">&#x27;random&#x27;</span><br>    &#125;<br></code></pre></td></tr></table></figure><p>对于 <code>bayesian</code> Sweeps，您还需要告诉我们一些关于您的metric的信息。我们需要知道它的名称，以便我们可以在模型输出中找到它，我们需要知道您的目标是最小化它（例如，如果它是squared error）还是最大化它（例如，如果它是 accuracy）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">metric = &#123;<br>    <span class="hljs-string">&#x27;name&#x27;</span>: <span class="hljs-string">&#x27;loss&#x27;</span>,<br>    <span class="hljs-string">&#x27;goal&#x27;</span>: <span class="hljs-string">&#x27;minimize&#x27;</span>   <br>    &#125;<br><br>sweep_config[<span class="hljs-string">&#x27;metric&#x27;</span>] = metric<br></code></pre></td></tr></table></figure><p>如果您没有运行 <code>bayesian</code>Sweep，则不必这样做，但无论如何将其包含在您的 <code>sweep_config</code>中并不是一个坏主意，以防您以后改变主意。记录这样的事情也是很好的再现性实践，以防万一您或其他人在6 个月或 6 年后回到您的 Sweep 并且不知道 <code>val_G_batch</code>应该是高还是低。</p><h4 id="name-the-hyperparameters">Name thehyper<code>parameters</code></h4><p>一旦您选择了一种 <code>method</code>来尝试超参数的新值，您需要定义这些 <code>parameters</code>是什么</p><p>大多数时候，这一步很简单：您只需为 <code>parameter</code>命名并指定参数的合法 <code>values</code> 列表。</p><p>例如，当我们为我们的网络选择 <code>optimizer</code>时，只有有限数量的选项。在这里，我们坚持使用两个最受欢迎的选择，<code>adam</code>和<code>sgd</code>。即使对于具有潜在无限选项的超参数，通常也只尝试几个选择<code>values</code> 才有意义，就像我们在此处对隐藏层<code>layer_size</code> 和 <code>dropout</code> 所做的那样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">parameters_dict = &#123;<br>    <span class="hljs-string">&#x27;optimizer&#x27;</span>: &#123;<br>        <span class="hljs-string">&#x27;values&#x27;</span>: [<span class="hljs-string">&#x27;adam&#x27;</span>, <span class="hljs-string">&#x27;sgd&#x27;</span>]<br>        &#125;,<br>    <span class="hljs-string">&#x27;fc_layer_size&#x27;</span>: &#123;<br>        <span class="hljs-string">&#x27;values&#x27;</span>: [<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>]<br>        &#125;,<br>    <span class="hljs-string">&#x27;dropout&#x27;</span>: &#123;<br>          <span class="hljs-string">&#x27;values&#x27;</span>: [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>]<br>        &#125;,<br>    &#125;<br><br>sweep_config[<span class="hljs-string">&#x27;parameters&#x27;</span>] = parameters_dict<br></code></pre></td></tr></table></figure><p>通常情况下，有些超参数我们不想在此 Sweep 中改变，但我们仍希望在我们的<code>sweep_config</code> 中设置它们。</p><p>在那种情况下，我们直接设置 <code>value</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">parameters_dict.update(&#123;<br>    <span class="hljs-string">&#x27;epochs&#x27;</span>: &#123;<br>        <span class="hljs-string">&#x27;value&#x27;</span>: <span class="hljs-number">1</span>&#125;<br>    &#125;)<br></code></pre></td></tr></table></figure><p>对于 <code>grid</code> 搜索，这就是您所需要的。</p><p>对于 <code>random</code> 搜索，参数的所有 <code>values</code>在给定运行中被选择的可能性相同。</p><p>如果这样做不行，您可以改为指定命名 <code>distribution</code>及其参数，例如 <code>normal</code> 分布的均值 <code>mu</code> 和标准差<code>sigma</code>。</p><p>在<ahref="https://docs.wandb.com/sweeps/configuration#distributions">此处</a>查看有关如何设置随机变量分布的更多信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">parameters_dict.update(&#123;<br>    <span class="hljs-string">&#x27;learning_rate&#x27;</span>: &#123;<br>        <span class="hljs-comment"># a flat distribution between 0 and 0.1</span><br>        <span class="hljs-string">&#x27;distribution&#x27;</span>: <span class="hljs-string">&#x27;uniform&#x27;</span>,<br>        <span class="hljs-string">&#x27;min&#x27;</span>: <span class="hljs-number">0</span>,<br>        <span class="hljs-string">&#x27;max&#x27;</span>: <span class="hljs-number">0.1</span><br>      &#125;,<br>    <span class="hljs-string">&#x27;batch_size&#x27;</span>: &#123;<br>        <span class="hljs-comment"># integers between 32 and 256</span><br>        <span class="hljs-comment"># with evenly-distributed logarithms </span><br>        <span class="hljs-string">&#x27;distribution&#x27;</span>: <span class="hljs-string">&#x27;q_log_uniform_values&#x27;</span>,<br>        <span class="hljs-string">&#x27;q&#x27;</span>: <span class="hljs-number">8</span>,<br>        <span class="hljs-string">&#x27;min&#x27;</span>: <span class="hljs-number">32</span>,<br>        <span class="hljs-string">&#x27;max&#x27;</span>: <span class="hljs-number">256</span>,<br>      &#125;<br>    &#125;)<br></code></pre></td></tr></table></figure><p>当我们完成后，<code>sweep_config</code>是一个嵌套的字典，它准确地指定了我们有兴趣尝试哪些<code>parameters</code> 以及我们将使用什么 <code>method</code>来尝试它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pprint<br><br>pprint.pprint(sweep_config)<br></code></pre></td></tr></table></figure><p>但这不是所有的配置选项！</p><p>例如，我们还提供了使用 <ahref="https://arxiv.org/pdf/1603.06560.pdf">HyperBand</a> 调度算法<code>early_terminate</code> 运行的选项。在<ahref="https://docs.wandb.com/sweeps/configuration#stopping-criteria">这里</a>查看更多。</p><p>您可以在<ahref="https://docs.wandb.com/library/sweeps/configuration">此处</a>找到所有配置选项的列表，并在<ahref="https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion">此处</a>找到大量YAML 格式的示例。</p><h3 id="step-2.-initialize-the-sweep">Step 2️⃣. Initialize the Sweep</h3><p>一旦您定义了搜索策略，就该设置一些东西来实现它了。</p><p>负责我们 Sweep 的 clockwork taskmaster 被称为 <em>SweepController</em>。每次运行完成时，它将发出一组新的指令来描述要执行的新运行。这些指令由实际执行运行的<em>agents</em> 获取。</p><p>在典型的 Sweep 中，Controller位于我们的机器上，而完成运行的代理位于您的机器上，如下图所示。这种分工使得只需添加更多机器来运行代理就可以非常容易地扩展Sweeps！</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedzlbw3vQ.png"alt="sweeps-diagram" /><figcaption aria-hidden="true">sweeps-diagram</figcaption></figure><p>我们可以通过使用适当的 <code>sweep_config</code> 和<code>project</code> 名称调用 <code>wandb.sweep</code> 来结束 SweepController。</p><p>此函数返回一个 <code>sweep_id</code>，我们稍后将使用它来将 agents分配给此 Controller。</p><p>旁注：在命令行上，此功能被替换为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wandb sweep config.yaml<br></code></pre></td></tr></table></figure><p><ahref="https://docs.wandb.com/sweeps/quickstart">了解更多关于在命令行中使用Sweeps ➡</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sweep_id = wandb.sweep(sweep_config, project=<span class="hljs-string">&quot;pytorch-sweeps-demo&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="step-3.-run-the-sweep-agent">Step 3️⃣. Run the Sweep agent</h3><h4 id="define-your-training-procedure">Define Your TrainingProcedure</h4><p>在我们实际执行 sweep 之前，我们需要定义使用这些值的训练过程。</p><p>在下面的函数中，我们在 PyTorch中定义了一个简单的全连接神经网络，并添加了以下 <code>wandb</code>工具来记录模型指标、可视化性能和输出并跟踪我们的实验：</p><ul><li><ahref="https://docs.wandb.com/library/init"><strong><code>wandb.init()</code></strong></a>– 初始化新的 W&amp;B 运行。每次运行都是训练功能的一次执行。</li><li><ahref="https://docs.wandb.com/library/config"><strong><code>wandb.config</code></strong></a>– 将所有超参数保存在配置对象中，以便记录它们。在<ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb">此处</a>阅读有关如何使用<code>wandb.config</code> 的更多信息。</li><li><ahref="https://docs.wandb.com/library/log"><strong><code>wandb.log()</code></strong></a>– 将模型行为记录到 W&amp;B。在这里，我们只记录性能；有关可以使用<code>wandb.log</code> 记录的所有其他富媒体，请参阅此 <ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_(Almost)_Anything_with_W%26B_Media.ipynb">Colab</a>。</li></ul><p>有关使用 PyTorch 检测 W&amp;B 的更多详细信息，请参阅此 <ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb">Colab</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">config=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-comment"># Initialize a new wandb run</span><br>    <span class="hljs-keyword">with</span> wandb.init(config=config):<br>        <span class="hljs-comment"># If called by wandb.agent, as below,</span><br>        <span class="hljs-comment"># this config will be set by Sweep Controller</span><br>        config = wandb.config<br><br>        loader = build_dataset(config.batch_size)<br>        network = build_network(config.fc_layer_size, config.dropout)<br>        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)<br><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>            avg_loss = train_epoch(network, loader, optimizer)<br>            wandb.log(&#123;<span class="hljs-string">&quot;loss&quot;</span>: avg_loss, <span class="hljs-string">&quot;epoch&quot;</span>: epoch&#125;)           <br></code></pre></td></tr></table></figure><p>这个单元格定义了我们训练过程的四个部分：<code>build_dataset</code>,<code>build_network</code>, <code>build_optimizer</code>和<code>train_epoch</code>.</p><p>所有这些都是基本 PyTorch 管道的标准部分，它们的实现不受使用 W&amp;B的影响，因此我们不会对它们发表评论。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_dataset</span>(<span class="hljs-params">batch_size</span>):<br>   <br>    transform = transforms.Compose(<br>        [transforms.ToTensor(),<br>         transforms.Normalize((<span class="hljs-number">0.1307</span>,), (<span class="hljs-number">0.3081</span>,))])<br>    <span class="hljs-comment"># download MNIST training dataset</span><br>    dataset = datasets.MNIST(<span class="hljs-string">&quot;.&quot;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>,<br>                             transform=transform)<br>    sub_dataset = torch.utils.data.Subset(<br>        dataset, indices=<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(dataset), <span class="hljs-number">5</span>))<br>    loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size)<br><br>    <span class="hljs-keyword">return</span> loader<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_network</span>(<span class="hljs-params">fc_layer_size, dropout</span>):<br>    network = nn.Sequential(  <span class="hljs-comment"># fully-connected, single hidden layer</span><br>        nn.Flatten(),<br>        nn.Linear(<span class="hljs-number">784</span>, fc_layer_size), nn.ReLU(),<br>        nn.Dropout(dropout),<br>        nn.Linear(fc_layer_size, <span class="hljs-number">10</span>),<br>        nn.LogSoftmax(dim=<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">return</span> network.to(device)<br>        <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_optimizer</span>(<span class="hljs-params">network, optimizer, learning_rate</span>):<br>    <span class="hljs-keyword">if</span> optimizer == <span class="hljs-string">&quot;sgd&quot;</span>:<br>        optimizer = optim.SGD(network.parameters(),<br>                              lr=learning_rate, momentum=<span class="hljs-number">0.9</span>)<br>    <span class="hljs-keyword">elif</span> optimizer == <span class="hljs-string">&quot;adam&quot;</span>:<br>        optimizer = optim.Adam(network.parameters(),<br>                               lr=learning_rate)<br>    <span class="hljs-keyword">return</span> optimizer<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch</span>(<span class="hljs-params">network, loader, optimizer</span>):<br>    cumu_loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> _, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(loader):<br>        data, target = data.to(device), target.to(device)<br>        optimizer.zero_grad()<br><br>        <span class="hljs-comment"># ➡ Forward pass</span><br>        loss = F.nll_loss(network(data), target)<br>        cumu_loss += loss.item()<br><br>        <span class="hljs-comment"># ⬅ Backward pass + weight update</span><br>        loss.backward()<br>        optimizer.step()<br><br>        wandb.log(&#123;<span class="hljs-string">&quot;batch loss&quot;</span>: loss.item()&#125;)<br><br>    <span class="hljs-keyword">return</span> cumu_loss / <span class="hljs-built_in">len</span>(loader)<br></code></pre></td></tr></table></figure><p>现在，我们准备开始 sweeping 了！</p><p>Sweep Controllers，就像我们通过运行 <code>wandb.sweep</code>制作的控制器一样，坐等有人要求他们提供 <code>config</code>来试用。</p><p>某人是 <code>agent</code>，他们是用 <code>wandb.agent</code>创建的。要开始，agent 只需要知道</p><ol type="1"><li>它是 (<code>sweep_id</code>) 的一部分</li><li>它应该运行哪个函数（这里是 <code>train</code>）</li><li>（可选）有多少配置要求控制器（<code>count</code>）</li></ol><p>仅供参考，您可以在不同的计算资源上启动具有相同 <code>sweep_id</code>的多个 <code>agent</code>，Controller 将确保它们根据<code>sweep_config</code>中制定的策略协同工作。这使得在尽可能多的节点上扩展 Sweeps变得轻而易举！</p><p>旁注：在命令行上，此功能被替换为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wandb agent sweep_id<br></code></pre></td></tr></table></figure><p><ahref="https://docs.wandb.com/sweeps/quickstart">了解更多关于在命令行中使用Sweeps ➡</a></p><p>下面的单元格将启动一个运行 <code>train</code> 5 次的<code>agent</code>，使用 Sweep Controller返回的随机生成的超参数值。执行时间不到 5 分钟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">wandb.agent(sweep_id, train, count=<span class="hljs-number">5</span>)<br></code></pre></td></tr></table></figure><h3 id="visualize-sweep-results">Visualize Sweep Results</h3><h4 id="parallel-coordinates-plot">Parallel Coordinates Plot</h4><p>此图将超参数值映射到模型指标。它对于磨练导致最佳模型性能的超参数组合很有用。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined5e190366778ad831455f9af2_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695138341_image.png"alt="hyperparameters map to metrics" /><figcaption aria-hidden="true">hyperparameters map tometrics</figcaption></figure><h4 id="hyperparameter-importance-plot">Hyperparameter ImportancePlot</h4><p>超参数重要性图表明哪些超参数是指标的最佳预测因子。我们报告特征重要性（来自随机森林模型）和相关性（隐式线性模型）。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefined5e190367778ad820b35f9af5_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695757573_image.png"alt="parameter importance" /><figcaption aria-hidden="true">parameter importance</figcaption></figure><p>这些可视化可以通过磨练最重要的参数（和值范围）来帮助您节省运行昂贵的超参数优化的时间和资源，因此值得进一步探索。</p><h3 id="get-your-hands-dirty-with-sweeps">Get your hands dirty withsweeps</h3><p>我们创建了一个简单的训练脚本和一些 <ahref="https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion">sweepconfigs</a> 风格供您使用。我们强烈建议您尝试一下。</p><p>该存储库还提供了一些示例，可帮助您尝试更高级的 sweep 功能，例如 <ahref="https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/us0ifmrf?workspace=user-lavanyashukla&amp;_gl=1*1h57q6p*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjYyMjIyOS43LjAuMTY4NjYyMjIyOS42MC4wLjA.">BayesianHyperband</a> 和 <ahref="https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/xbs2wm5e?workspace=user-lavanyashukla&amp;_gl=1*5hk37j*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjYyMjIyOS43LjAuMTY4NjYyMjIyOS42MC4wLjA.">Hyperopt</a>。</p><h2 id="track-models-and-datasets">Track models and datasets</h2><p><ahref="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W&amp;B_Artifacts.ipynb">在此处试用Colab Notebook →</a></p><p>在此笔记本中，我们将向您展示如何使用 W&amp;B Artifacts 跟踪您的 ML实验管道。</p><h4 id="follow-along-with-a-video-tutorial-1">Follow along with a <ahref="http://tiny.cc/wb-artifacts-video">video tutorial</a>!</h4><h5 id="what-are-artifacts-and-why-should-i-care">What are Artifacts andWhy Should I Care?</h5><p>“artifact”，如希腊双耳瓶🏺，是一个生产的对象——一个过程的输出。在 ML中，最重要的工件是 <em>datasets</em> 和 <em>models</em>。</p><p>而且，就像 <ahref="https://indianajones.fandom.com/wiki/Cross_of_Coronado">Cross ofCoronado</a>一样，这些重要的文物属于博物馆！也就是说，应该对它们进行分类和组织，以便您、您的团队和整个ML 社区可以向它们学习。毕竟，那些不跟踪训练的人注定要重蹈覆辙。</p><p>使用我们的 Artifacts API，您可以将 <code>Artifacts</code> 记录为W&amp;B <code>Runs</code> 的输出，或使用 <code>Artifacts</code> 作为<code>Runs</code>的输入，如此图所示，其中训练运行接受数据集并生成模型。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedsimple%20artifact%20diagram%202.png"alt="simple artifact diagram" /><figcaption aria-hidden="true">simple artifact diagram</figcaption></figure><p>由于一次运行可以使用另一次的输出作为输入，因此 Artifacts 和 Runs一起形成了一个有向图——实际上是一个二分 <ahref="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a>！ --带有 <code>Artifact</code>s 和 <code>Run</code>s 的节点，以及将<code>Run</code>s 连接到它们消耗或生产的 <code>Artifact</code>s的箭头。</p><h3 id="install-and-import">0️⃣ Install and Import</h3><p>Artifacts 是我们 Python 库的一部分，从 <code>0.9.2</code>版开始。</p><p>与 ML Python 堆栈的大多数部分一样，它可以通过 <code>pip</code>获得。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Compatible with wandb version 0.9.2+</span><br>!pip install wandb -qqq<br>!apt install tree<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> wandb<br></code></pre></td></tr></table></figure><h3 id="log-a-dataset">1️⃣ Log a Dataset</h3><p>首先，让我们定义一些 Artifacts。</p><p>此示例基于此 PyTorch <ahref="https://github.com/pytorch/examples/tree/master/mnist/">"BasicMNIST Example"</a>，但可以在 <ahref="http://wandb.me/artifacts-colab">TensorFlow</a>、任何其他框架或纯Python 中轻松完成。</p><p>我们从 <code>Dataset</code>s 开始：</p><ul><li>一个 <code>train</code>ing set，用于选择参数，</li><li>一个 <code>validation</code> set，用于选择超参数，</li><li>一个 <code>test</code>ing set，用于评估最终模型</li></ul><p>下面的第一个单元格定义了这三个数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random <br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> TensorDataset<br><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-comment"># Ensure deterministic behavior</span><br>torch.backends.cudnn.deterministic = <span class="hljs-literal">True</span><br>random.seed(<span class="hljs-number">0</span>)<br>torch.manual_seed(<span class="hljs-number">0</span>)<br>torch.cuda.manual_seed_all(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># Device configuration</span><br>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-comment"># Data parameters</span><br>num_classes = <span class="hljs-number">10</span><br>input_shape = (<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br><br><span class="hljs-comment"># drop slow mirror from list of MNIST mirrors</span><br>torchvision.datasets.MNIST.mirrors = [mirror <span class="hljs-keyword">for</span> mirror <span class="hljs-keyword">in</span> torchvision.datasets.MNIST.mirrors<br>                                      <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> mirror.startswith(<span class="hljs-string">&quot;http://yann.lecun.com&quot;</span>)]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load</span>(<span class="hljs-params">train_size=<span class="hljs-number">50_000</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    # Load the data</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># the data, split between train and test sets</span><br>    train = torchvision.datasets.MNIST(<span class="hljs-string">&quot;./&quot;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>)<br>    test = torchvision.datasets.MNIST(<span class="hljs-string">&quot;./&quot;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>)<br>    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)<br><br>    <span class="hljs-comment"># split off a validation set for hyperparameter tuning</span><br>    x_train, x_val = x_train[:train_size], x_train[train_size:]<br>    y_train, y_val = y_train[:train_size], y_train[train_size:]<br><br>    training_set = TensorDataset(x_train, y_train)<br>    validation_set = TensorDataset(x_val, y_val)<br>    test_set = TensorDataset(x_test, y_test)<br><br>    datasets = [training_set, validation_set, test_set]<br><br>    <span class="hljs-keyword">return</span> datasets<br></code></pre></td></tr></table></figure><p>这建立了一个模式，我们将在这个例子中看到重复：将数据记录为工件的代码包裹在生成该数据的代码周围。在这种情况下，用于<code>load</code>ing 数据的代码与用于 <code>load_and_log</code>ging数据的代码分开。</p><p>这是很好的做法！</p><p>为了将这些数据集记录为工件，我们只需要</p><ol type="1"><li>使用 <code>wandb.init</code> 创建 <code>Run</code>，(L4)</li><li>为数据集 (L10) 创建一个 <code>Artifact</code>，以及</li><li>保存并记录相关 <code>file</code>s（L20、L23）。</li></ol><p>查看下面代码单元的示例，然后展开后面的部分以了解更多详细信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_and_log</span>():<br><br>    <span class="hljs-comment"># 🚀 start a run, with a type to label it and a project it can call home</span><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;load-data&quot;</span>) <span class="hljs-keyword">as</span> run:<br>        <br>        datasets = load()  <span class="hljs-comment"># separate code for loading the datasets</span><br>        names = [<span class="hljs-string">&quot;training&quot;</span>, <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>]<br><br>        <span class="hljs-comment"># 🏺 create our Artifact</span><br>        raw_data = wandb.Artifact(<br>            <span class="hljs-string">&quot;mnist-raw&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;dataset&quot;</span>,<br>            description=<span class="hljs-string">&quot;Raw MNIST dataset, split into train/val/test&quot;</span>,<br>            metadata=&#123;<span class="hljs-string">&quot;source&quot;</span>: <span class="hljs-string">&quot;torchvision.datasets.MNIST&quot;</span>,<br>                      <span class="hljs-string">&quot;sizes&quot;</span>: [<span class="hljs-built_in">len</span>(dataset) <span class="hljs-keyword">for</span> dataset <span class="hljs-keyword">in</span> datasets]&#125;)<br><br>        <span class="hljs-keyword">for</span> name, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(names, datasets):<br>            <span class="hljs-comment"># 🐣 Store a new file in the artifact, and write something into its contents.</span><br>            <span class="hljs-keyword">with</span> raw_data.new_file(name + <span class="hljs-string">&quot;.pt&quot;</span>, mode=<span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> file:<br>                x, y = data.tensors<br>                torch.save((x, y), file)<br><br>        <span class="hljs-comment"># ✍️ Save the artifact to W&amp;B.</span><br>        run.log_artifact(raw_data)<br><br>load_and_log()<br></code></pre></td></tr></table></figure><h4 id="wandb.init">🚀 <code>wandb.init</code></h4><p>当我们制作将要产生 <code>Artifact</code>s 的<code>Run</code>时，我们需要说明它属于哪个 <code>project</code>。</p><p>根据您的工作流程，项目可能大到<code>car-that-drives-itself</code>，也可能小到<code>iterative-architecture-experiment-117</code>。</p><p>Depending on your workflow, a project might be as big as<code>car-that-drives-itself</code> or as small as<code>iterative-architecture-experiment-117</code>.</p><blockquote><p>👍规则：如果可以，请将所有共享 <code>Artifact</code>s 的<code>Run</code>s 保留在一个项目中。这使事情变得简单，但不要担心<code>Artifact</code>s 可以跨项目移植！</p></blockquote><p>为了帮助跟踪您可能运行的所有不同类型的作业，在进行 <code>Run</code>s时提供 <code>job_type</code> 很有用。这可以使您的 Artifacts图表保持整洁。</p><blockquote><p>👍规则：<code>job_type</code>应该是描述性的，并且对应于你的管道的单个步骤。在这里，我们将<code>load</code>ing 数据与 <code>preprocess</code>ing 数据分开。</p></blockquote><h4 id="wandb.artifact">🏺 <code>wandb.Artifact</code></h4><p>要将某物记录为 <code>Artifact</code>，我们必须首先创建一个<code>Artifact</code> 对象。</p><p>每个 <code>Artifact</code> 都有一个<code>name</code>——这是第一个参数设置的名称。</p><blockquote><p>👍的规则：<code>name</code>应该是描述性的，但易于记忆和输入——我们喜欢使用连字符分隔的名称，并与代码中的变量名相对应。</p></blockquote><p>它也有一个 <code>type</code>。就像 <code>Run</code>s 的<code>job_types</code> 一样，它用于组织 <code>Run</code>s 和<code>Artifact</code>s 的图表。</p><blockquote><p>👍的规则：<code>type</code> 应该简单：比<code>mnist-data-YYYYMMDD</code> 更像 <code>dataset</code> 或<code>model</code>。</p></blockquote><p>您还可以附加 <code>description</code> 和一些<code>metadata</code>，作为字典。<code>metadata</code> 只需要可序列化为JSON。</p><blockquote><p>👍规则：<code>metadata</code>应尽可能具有描述性。</p></blockquote><h4 id="artifact.new_file-and-run.log_artifact">🐣<code>artifact.new_file</code> and ✍️ <code>run.log_artifact</code></h4><p>一旦我们创建了一个 <code>Artifact</code>对象，我们需要向它添加文件。</p><p>您没看错：带有 s 的 <em>files</em>。<code>Artifact</code>s的结构类似于目录，包含文件和子目录。</p><blockquote><p>👍规则：只要有必要，将 <code>Artifact</code>的内容拆分为多个文件。如果需要扩展，这将有所帮助！</p></blockquote><p>我们使用 <code>new_file</code> 方法同时写入文件并将其附加到<code>Artifact</code>。下面，我们将使用 <code>add_file</code>方法，它将这两个步骤分开</p><p>添加完所有文件后，我们需要将 <code>log_artifact</code> 添加到 <ahref="https://wandb.ai/?_gl=1*r07jdw*_ga*MTUwMzMwNTA4NC4xNjg1MzI0NjI3*_ga_JH1SJHJQXJ*MTY4NjY1MzgzNC45LjEuMTY4NjY1MzgzOS41NS4wLjA.">wandb.ai</a>。</p><p>您会注意到一些 URL 出现在输出中，包括一个用于运行页面的URL。您可以在此处查看 <code>Run</code> 结果，包括已记录的任何<code>Artifact</code>s。</p><p>我们将在下面看到一些示例，这些示例可以更好地利用“运行”页面的其他组件。</p><h3 id="use-a-logged-dataset-artifact">2️⃣ Use a Logged DatasetArtifact</h3><p>与博物馆中的 artifacts 不同，W&amp;B 中的 <code>Artifact</code>s旨在使用，而不仅仅是存储。</p><p>让我们看看它是什么样的。</p><p>下面的单元格定义了一个管道步骤，该步骤接收原始数据集并使用它来生成<code>preprocess</code>ed 数据集：<code>normalize</code>d和正确整形。</p><p>再次注意，我们从与 <code>wandb</code>接口的代码中分离出了代码的主体，即 <code>preprocess</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess</span>(<span class="hljs-params">dataset, normalize=<span class="hljs-literal">True</span>, expand_dims=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    ## Prepare the data</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, y = dataset.tensors<br><br>    <span class="hljs-keyword">if</span> normalize:<br>        <span class="hljs-comment"># Scale images to the [0, 1] range</span><br>        x = x.<span class="hljs-built_in">type</span>(torch.float32) / <span class="hljs-number">255</span><br><br>    <span class="hljs-keyword">if</span> expand_dims:<br>        <span class="hljs-comment"># Make sure images have shape (1, 28, 28)</span><br>        x = torch.unsqueeze(x, <span class="hljs-number">1</span>)<br>    <br>    <span class="hljs-keyword">return</span> TensorDataset(x, y)<br></code></pre></td></tr></table></figure><p>现在是使用 <code>wandb.Artifact</code> 日志记录这个<code>preprocess</code> 步骤的代码。</p><p>请注意，下面的示例都 <code>use</code>s 了一个新的<code>Artifact</code>，并将其 <code>log</code>s下来，这与上一步相同。<code>Artifact</code>s 既是 <code>Run</code>s的输入又是输出！</p><p>我们使用一个新的<code>job_type</code>，<code>preprocess-data</code>，来明确这是一个不同于之前的job。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_and_log</span>(<span class="hljs-params">steps</span>):<br><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;preprocess-data&quot;</span>) <span class="hljs-keyword">as</span> run:<br><br>        processed_data = wandb.Artifact(<br>            <span class="hljs-string">&quot;mnist-preprocess&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;dataset&quot;</span>,<br>            description=<span class="hljs-string">&quot;Preprocessed MNIST dataset&quot;</span>,<br>            metadata=steps)<br>         <br>        <span class="hljs-comment"># ✔️ declare which artifact we&#x27;ll be using</span><br>        raw_data_artifact = run.use_artifact(<span class="hljs-string">&#x27;mnist-raw:latest&#x27;</span>)<br><br>        <span class="hljs-comment"># 📥 if need be, download the artifact</span><br>        raw_dataset = raw_data_artifact.download()<br>        <br>        <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;training&quot;</span>, <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>]:<br>            raw_split = read(raw_dataset, split)<br>            processed_dataset = preprocess(raw_split, **steps)<br><br>            <span class="hljs-keyword">with</span> processed_data.new_file(split + <span class="hljs-string">&quot;.pt&quot;</span>, mode=<span class="hljs-string">&quot;wb&quot;</span>) <span class="hljs-keyword">as</span> file:<br>                x, y = processed_dataset.tensors<br>                torch.save((x, y), file)<br><br>        run.log_artifact(processed_data)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read</span>(<span class="hljs-params">data_dir, split</span>):<br>    filename = split + <span class="hljs-string">&quot;.pt&quot;</span><br>    x, y = torch.load(os.path.join(data_dir, filename))<br><br>    <span class="hljs-keyword">return</span> TensorDataset(x, y)<br></code></pre></td></tr></table></figure><p>这里要注意的一件事是预处理的 <code>steps</code> 作为<code>metadata</code> 与 <code>preprocessed_data</code> 一起保存。</p><p>如果您想让您的实验可重现，捕获大量元数据是个好主意！</p><p>此外，即使我们的数据集是"<code>large artifact</code>"，<code>download</code>步骤也可以在不到一秒的时间内完成。</p><p>展开下面的 markdown 单元格以了解详细信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">steps = &#123;<span class="hljs-string">&quot;normalize&quot;</span>: <span class="hljs-literal">True</span>,<br>         <span class="hljs-string">&quot;expand_dims&quot;</span>: <span class="hljs-literal">True</span>&#125;<br><br>preprocess_and_log(steps)<br></code></pre></td></tr></table></figure><h4 id="run.use_artifact">✔️ <code>run.use_artifact</code></h4><p>这些步骤比较简单。消费者只需要知道 <code>Artifact</code>的<code>name</code>，再加上 bit more。</p><p>“bit more” 是您想要的 <code>Artifact</code> 的特定版本的<code>alias</code>。</p><p>默认情况下，最后上传的版本被标记为<code>latest</code>。否则，您可以选择带有<code>v0</code>/<code>v1</code>等的旧版本，或者您可以提供自己的别名，例如 <code>best</code> 或<code>jit-script</code>。就像 <a href="https://hub.docker.com/">DockerHub</a> 标签一样，别名与名称用 <code>:</code> 分隔，所以我们想要的<code>Artifact</code> 是 <code>mnist-raw:latest</code>。</p><blockquote><p>👍规则：保持别名简短而甜美。当您想要满足某些属性的<code>Artifact</code> 时，请使用自定义 <code>alias</code>es，如<code>latest</code> 或 <code>best</code></p></blockquote><h4 id="artifact.download">📥 <code>artifact.download</code></h4><p>现在，您可能正在担心 <code>download</code>调用。如果我们再下载一份，内存的负担会不会加倍？</p><p>别担心，朋友。在我们实际下载任何东西之前，我们会检查本地是否有正确的版本。使用和版本控制<code>git</code> 和 <ahref="https://en.wikipedia.org/wiki/Torrent_file">torrenting</a>相同的技术：hashing。</p><p>随着 <code>Artifact</code>s 的创建和记录，工作目录中名为<code>artifacts</code> 的文件夹将开始填充子目录，每个<code>Artifact</code>一个。使用 <code>!tree artifacts</code>检查其内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">!tree artifacts<br></code></pre></td></tr></table></figure><h4 id="the-artifacts-page-on-wandb.ai">🌐 The Artifacts page on <ahref="https://wandb.ai/">wandb.ai</a></h4><p>现在我们已经记录并使用了一个 <code>Artifact</code>，让我们检查一下Run 页面上的 Artifacts 选项卡。</p><p>从<code>wandb</code> 输出导航到运行页面URL，然后从左侧边栏中选择“工件”选项卡（它是带有数据库图标的选项卡，看起来像三个冰球叠在一起）。</p><p>单击 "Input Artifacts" 表或 "Output Artifacts"表中的一行，然后查看选项卡（"Overview", "Metadata"）以查看记录的有关<code>Artifact</code> 的所有内容。</p><p>我们特别喜欢 "Graph View"。默认情况下，它显示一个图表，其中<code>Artifact</code>s 的 <code>type</code>s 和 <code>Run</code> 的<code>job_types</code> 是两种类型的节点，箭头代表消费和生产。</p><h3 id="log-a-model">3️⃣ Log a Model</h3><p>这足以了解 <code>Artifact</code>s 的 API如何工作，但让我们按照这个示例一直到管道的末尾，以便我们可以了解<code>Artifact</code>s 如何改进您的 ML 工作流程。</p><p>这里的第一个单元格在 PyTorch 中构建了一个 DNN<code>model</code>——一个非常简单的 ConvNet。</p><p>我们将从初始化 <code>model</code>开始，而不是训练它。这样，我们可以重复训练，同时保持其他一切不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> floor<br><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ConvNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_layer_sizes=[<span class="hljs-number">32</span>, <span class="hljs-number">64</span>],</span><br><span class="hljs-params">                  kernel_sizes=[<span class="hljs-number">3</span>],</span><br><span class="hljs-params">                  activation=<span class="hljs-string">&quot;ReLU&quot;</span>,</span><br><span class="hljs-params">                  pool_sizes=[<span class="hljs-number">2</span>],</span><br><span class="hljs-params">                  dropout=<span class="hljs-number">0.5</span>,</span><br><span class="hljs-params">                  num_classes=num_classes,</span><br><span class="hljs-params">                  input_shape=input_shape</span>):<br>      <br>        <span class="hljs-built_in">super</span>(ConvNet, self).__init__()<br><br>        self.layer1 = nn.Sequential(<br>              nn.Conv2d(in_channels=input_shape[<span class="hljs-number">0</span>], out_channels=hidden_layer_sizes[<span class="hljs-number">0</span>], kernel_size=kernel_sizes[<span class="hljs-number">0</span>]),<br>              <span class="hljs-built_in">getattr</span>(nn, activation)(),<br>              nn.MaxPool2d(kernel_size=pool_sizes[<span class="hljs-number">0</span>])<br>        )<br>        self.layer2 = nn.Sequential(<br>              nn.Conv2d(in_channels=hidden_layer_sizes[<span class="hljs-number">0</span>], out_channels=hidden_layer_sizes[-<span class="hljs-number">1</span>], kernel_size=kernel_sizes[-<span class="hljs-number">1</span>]),<br>              <span class="hljs-built_in">getattr</span>(nn, activation)(),<br>              nn.MaxPool2d(kernel_size=pool_sizes[-<span class="hljs-number">1</span>])<br>        )<br>        self.layer3 = nn.Sequential(<br>              nn.Flatten(),<br>              nn.Dropout(dropout)<br>        )<br><br>        fc_input_dims = floor((input_shape[<span class="hljs-number">1</span>] - kernel_sizes[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>) / pool_sizes[<span class="hljs-number">0</span>]) <span class="hljs-comment"># layer 1 output size</span><br>        fc_input_dims = floor((fc_input_dims - kernel_sizes[-<span class="hljs-number">1</span>] + <span class="hljs-number">1</span>) / pool_sizes[-<span class="hljs-number">1</span>]) <span class="hljs-comment"># layer 2 output size</span><br>        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-<span class="hljs-number">1</span>] <span class="hljs-comment"># layer 3 output size</span><br><br>        self.fc = nn.Linear(fc_input_dims, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.layer1(x)<br>        x = self.layer2(x)<br>        x = self.layer3(x)<br>        x = self.fc(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>在这里，我们使用 W&amp;B 来跟踪运行，因此使用<code>wandb.config</code> 对象来存储所有超参数。</p><p>该 <code>config</code> 对象的 <code>dict</code>ionary版本是一个非常有用的 <code>metadata</code>，所以一定要包含它！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model_and_log</span>(<span class="hljs-params">config</span>):<br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;initialize&quot;</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        config = wandb.config<br>        <br>        model = ConvNet(**config)<br><br>        model_artifact = wandb.Artifact(<br>            <span class="hljs-string">&quot;convnet&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;model&quot;</span>,<br>            description=<span class="hljs-string">&quot;Simple AlexNet style CNN&quot;</span>,<br>            metadata=<span class="hljs-built_in">dict</span>(config))<br><br>        torch.save(model.state_dict(), <span class="hljs-string">&quot;initialized_model.pth&quot;</span>)<br>        <span class="hljs-comment"># ➕ another way to add a file to an Artifact</span><br>        model_artifact.add_file(<span class="hljs-string">&quot;initialized_model.pth&quot;</span>)<br><br>        wandb.save(<span class="hljs-string">&quot;initialized_model.pth&quot;</span>)<br><br>        run.log_artifact(model_artifact)<br><br>model_config = &#123;<span class="hljs-string">&quot;hidden_layer_sizes&quot;</span>: [<span class="hljs-number">32</span>, <span class="hljs-number">64</span>],<br>                <span class="hljs-string">&quot;kernel_sizes&quot;</span>: [<span class="hljs-number">3</span>],<br>                <span class="hljs-string">&quot;activation&quot;</span>: <span class="hljs-string">&quot;ReLU&quot;</span>,<br>                <span class="hljs-string">&quot;pool_sizes&quot;</span>: [<span class="hljs-number">2</span>],<br>                <span class="hljs-string">&quot;dropout&quot;</span>: <span class="hljs-number">0.5</span>,<br>                <span class="hljs-string">&quot;num_classes&quot;</span>: <span class="hljs-number">10</span>&#125;<br><br>build_model_and_log(model_config)<br></code></pre></td></tr></table></figure><h4 id="artifact.add_file">➕ <code>artifact.add_file</code></h4><p>与在数据集日志记录示例中同时编写 <code>new_file</code> 并将其添加到<code>Artifact</code> 不同，我们还可以一步写入文件（此处为<code>torch.save</code>），然后在另一步中将它们 <code>add</code> 到<code>Artifact</code>。</p><blockquote><p>👍规则：尽可能使用 <code>new_file</code>，以防止重复。</p></blockquote><h3 id="use-a-logged-model-artifact">4️⃣ Use a Logged Model Artifact</h3><p>就像我们可以在 <code>dataset</code> 上调用 <code>use_artifact</code>一样，我们可以在我们的 <code>initialized_model</code>上调用它以在另一个运行中使用它。</p><p>这一次，让我们 <code>train</code> <code>model</code>。</p><p>有关更多详细信息，请查看我们关于 <ahref="http://wandb.me/pytorch-colab">instrumenting W&amp;B withPyTorch</a> 的 Colab。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">model, train_loader, valid_loader, config</span>):<br>    optimizer = <span class="hljs-built_in">getattr</span>(torch.optim, config.optimizer)(model.parameters())<br>    model.train()<br>    example_ct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.epochs):<br>        <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>            data, target = data.to(device), target.to(device)<br>            optimizer.zero_grad()<br>            output = model(data)<br>            loss = F.cross_entropy(output, target)<br>            loss.backward()<br>            optimizer.step()<br><br>            example_ct += <span class="hljs-built_in">len</span>(data)<br><br>            <span class="hljs-keyword">if</span> batch_idx % config.batch_log_interval == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0%&#125;)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                    epoch, batch_idx * <span class="hljs-built_in">len</span>(data), <span class="hljs-built_in">len</span>(train_loader.dataset),<br>                    batch_idx / <span class="hljs-built_in">len</span>(train_loader), loss.item()))<br>                <br>                train_log(loss, example_ct, epoch)<br><br>        <span class="hljs-comment"># evaluate the model on the validation set at each epoch</span><br>        loss, accuracy = test(model, valid_loader)  <br>        test_log(loss, accuracy, example_ct, epoch)<br><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">model, test_loader</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    test_loss = <span class="hljs-number">0</span><br>    correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            test_loss += F.cross_entropy(output, target, reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)  <span class="hljs-comment"># sum up batch loss</span><br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># get the index of the max log-probability</span><br>            correct += pred.eq(target.view_as(pred)).<span class="hljs-built_in">sum</span>()<br><br>    test_loss /= <span class="hljs-built_in">len</span>(test_loader.dataset)<br><br>    accuracy = <span class="hljs-number">100.</span> * correct / <span class="hljs-built_in">len</span>(test_loader.dataset)<br>    <br>    <span class="hljs-keyword">return</span> test_loss, accuracy<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_log</span>(<span class="hljs-params">loss, example_ct, epoch</span>):<br>    loss = <span class="hljs-built_in">float</span>(loss)<br><br>    <span class="hljs-comment"># where the magic happens</span><br>    wandb.log(&#123;<span class="hljs-string">&quot;epoch&quot;</span>: epoch, <span class="hljs-string">&quot;train/loss&quot;</span>: loss&#125;, step=example_ct)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Loss after &quot;</span> + <span class="hljs-built_in">str</span>(example_ct).zfill(<span class="hljs-number">5</span>) + <span class="hljs-string">f&quot; examples: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br>    <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_log</span>(<span class="hljs-params">loss, accuracy, example_ct, epoch</span>):<br>    loss = <span class="hljs-built_in">float</span>(loss)<br>    accuracy = <span class="hljs-built_in">float</span>(accuracy)<br><br>    <span class="hljs-comment"># where the magic happens</span><br>    wandb.log(&#123;<span class="hljs-string">&quot;epoch&quot;</span>: epoch, <span class="hljs-string">&quot;validation/loss&quot;</span>: loss, <span class="hljs-string">&quot;validation/accuracy&quot;</span>: accuracy&#125;, step=example_ct)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Loss/accuracy after &quot;</span> + <span class="hljs-built_in">str</span>(example_ct).zfill(<span class="hljs-number">5</span>) + <span class="hljs-string">f&quot; examples: <span class="hljs-subst">&#123;loss:<span class="hljs-number">.3</span>f&#125;</span>/<span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>这次我们将运行两个独立的 <code>Artifact</code> 生产<code>Run</code>s。</p><p>一旦第一个完成 <code>train</code>ing<code>model</code>，第二个将通过评估其在 <code>test_dataset</code>上的性能来使用 <code>trained-model</code> <code>Artifact</code>。</p><p>此外，我们将提取网络最混乱的 32个示例——在这些示例中，<code>categorical_crossentropy</code> 最高。</p><p>这是诊断数据集和模型问题的好方法！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">model, test_loader</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    ## Evaluate the trained model</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    loss, accuracy = test(model, test_loader)<br>    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)<br><br>    <span class="hljs-keyword">return</span> loss, accuracy, highest_losses, hardest_examples, true_labels, predictions<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_hardest_k_examples</span>(<span class="hljs-params">model, testing_set, k=<span class="hljs-number">32</span></span>):<br>    model.<span class="hljs-built_in">eval</span>()<br><br>    loader = DataLoader(testing_set, <span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># get the losses and predictions for each item in the dataset</span><br>    losses = <span class="hljs-literal">None</span><br>    predictions = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> loader:<br>            data, target = data.to(device), target.to(device)<br>            output = model(data)<br>            loss = F.cross_entropy(output, target)<br>            pred = output.argmax(dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            <br>            <span class="hljs-keyword">if</span> losses <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                losses = loss.view((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>                predictions = pred<br>            <span class="hljs-keyword">else</span>:<br>                losses = torch.cat((losses, loss.view((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))), <span class="hljs-number">0</span>)<br>                predictions = torch.cat((predictions, pred), <span class="hljs-number">0</span>)<br><br>    argsort_loss = torch.argsort(losses, dim=<span class="hljs-number">0</span>)<br><br>    highest_k_losses = losses[argsort_loss[-k:]]<br>    hardest_k_examples = testing_set[argsort_loss[-k:]][<span class="hljs-number">0</span>]<br>    true_labels = testing_set[argsort_loss[-k:]][<span class="hljs-number">1</span>]<br>    predicted_labels = predictions[argsort_loss[-k:]]<br><br>    <span class="hljs-keyword">return</span> highest_k_losses, hardest_k_examples, true_labels, predicted_labels<br></code></pre></td></tr></table></figure><p>这些日志记录功能不会添加任何新的 <code>Artifact</code>功能，因此我们不会对其进行评论：我们只是在使用、下载和记录<code>Artifact</code>s。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_and_log</span>(<span class="hljs-params">config</span>):<br><br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;train&quot;</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        config = wandb.config<br><br>        data = run.use_artifact(<span class="hljs-string">&#x27;mnist-preprocess:latest&#x27;</span>)<br>        data_dir = data.download()<br><br>        training_dataset =  read(data_dir, <span class="hljs-string">&quot;training&quot;</span>)<br>        validation_dataset = read(data_dir, <span class="hljs-string">&quot;validation&quot;</span>)<br><br>        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)<br>        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)<br>        <br>        model_artifact = run.use_artifact(<span class="hljs-string">&quot;convnet:latest&quot;</span>)<br>        model_dir = model_artifact.download()<br>        model_path = os.path.join(model_dir, <span class="hljs-string">&quot;initialized_model.pth&quot;</span>)<br>        model_config = model_artifact.metadata<br>        config.update(model_config)<br><br>        model = ConvNet(**model_config)<br>        model.load_state_dict(torch.load(model_path))<br>        model = model.to(device)<br> <br>        train(model, train_loader, validation_loader, config)<br><br>        model_artifact = wandb.Artifact(<br>            <span class="hljs-string">&quot;trained-model&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;model&quot;</span>,<br>            description=<span class="hljs-string">&quot;Trained NN model&quot;</span>,<br>            metadata=<span class="hljs-built_in">dict</span>(model_config))<br><br>        torch.save(model.state_dict(), <span class="hljs-string">&quot;trained_model.pth&quot;</span>)<br>        model_artifact.add_file(<span class="hljs-string">&quot;trained_model.pth&quot;</span>)<br>        wandb.save(<span class="hljs-string">&quot;trained_model.pth&quot;</span>)<br><br>        run.log_artifact(model_artifact)<br><br>    <span class="hljs-keyword">return</span> model<br><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_and_log</span>(<span class="hljs-params">config=<span class="hljs-literal">None</span></span>):<br>    <br>    <span class="hljs-keyword">with</span> wandb.init(project=<span class="hljs-string">&quot;artifacts-example&quot;</span>, job_type=<span class="hljs-string">&quot;report&quot;</span>, config=config) <span class="hljs-keyword">as</span> run:<br>        data = run.use_artifact(<span class="hljs-string">&#x27;mnist-preprocess:latest&#x27;</span>)<br>        data_dir = data.download()<br>        testing_set = read(data_dir, <span class="hljs-string">&quot;test&quot;</span>)<br><br>        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">False</span>)<br><br>        model_artifact = run.use_artifact(<span class="hljs-string">&quot;trained-model:latest&quot;</span>)<br>        model_dir = model_artifact.download()<br>        model_path = os.path.join(model_dir, <span class="hljs-string">&quot;trained_model.pth&quot;</span>)<br>        model_config = model_artifact.metadata<br><br>        model = ConvNet(**model_config)<br>        model.load_state_dict(torch.load(model_path))<br>        model.to(device)<br><br>        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)<br><br>        run.summary.update(&#123;<span class="hljs-string">&quot;loss&quot;</span>: loss, <span class="hljs-string">&quot;accuracy&quot;</span>: accuracy&#125;)<br><br>        wandb.log(&#123;<span class="hljs-string">&quot;high-loss-examples&quot;</span>:<br>            [wandb.Image(hard_example, caption=<span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(pred)) + <span class="hljs-string">&quot;,&quot;</span> +  <span class="hljs-built_in">str</span>(<span class="hljs-built_in">int</span>(label)))<br>             <span class="hljs-keyword">for</span> hard_example, pred, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(hardest_examples, preds, true_labels)]&#125;)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">train_config = &#123;<span class="hljs-string">&quot;batch_size&quot;</span>: <span class="hljs-number">128</span>,<br>                <span class="hljs-string">&quot;epochs&quot;</span>: <span class="hljs-number">5</span>,<br>                <span class="hljs-string">&quot;batch_log_interval&quot;</span>: <span class="hljs-number">25</span>,<br>                <span class="hljs-string">&quot;optimizer&quot;</span>: <span class="hljs-string">&quot;Adam&quot;</span>&#125;<br><br>model = train_and_log(train_config)<br>evaluate_and_log()<br></code></pre></td></tr></table></figure><h4 id="the-graph-view">🔁 The Graph View</h4><p>请注意，我们更改了 <code>Artifact</code> 的 <code>type</code>：这些<code>Run</code>s 使用的是模型，而不是数据集。在 Artifacts页面的图形视图中，生产模型的 <code>Run</code>s 将与生成<code>dataset</code>s 的运行分开。</p><p>去看看吧！和以前一样，您需要前往 Run 页面，从左侧栏中选择 "Artifacts"选项卡，选择一个 <code>Artifact</code>，然后单击 "Graph View"选项卡。</p><h4 id="exploded-graphs">💣 Exploded Graphs</h4><p>您可能已经注意到标有“爆炸”的按钮。不要点击它，因为它会在 W&amp;B总部您不起眼的作者办公桌下引爆一枚小炸弹！</p><p>只是在开玩笑。它以更温和的方式“分解”图表：<code>Artifact</code>s 和<code>Run</code>s 在单个实例级别而不是类型级别分离：节点不是<code>dataset</code> 和 <code>load-data</code>，而是<code>dataset:mnist-raw:v1</code> 和<code>load-data:sunny-smoke-1</code>，等等。</p><p>这提供了对您的管道的全面洞察，记录的指标、元数据等都触手可及——您仅受限于您选择与我们一起记录的内容。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（四）Typora + PicGo+ 腾讯云图床</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-san-typora-teng-xun-yun-tu-chuang/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-san-typora-teng-xun-yun-tu-chuang/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（五）自定义域名及百度谷歌收录</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-wu-zi-ding-yi-yu-ming-ji-bai-du-gu-ge-shou-lu/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-wu-zi-ding-yi-yu-ming-ji-bai-du-gu-ge-shou-lu/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（二）Matery 主题</title>
    <link href="/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-er-matery-zhu-ti/"/>
    <url>/2023/06/04/github-pages-ge-ren-bo-ke-da-jian-er-matery-zhu-ti/</url>
    
    <content type="html"><![CDATA[<h2 id="下载">下载</h2><p>本主题<strong>推荐你使用 Hexo 5.0.0及以上的版本</strong>。如果，你已经有一个自己的 <ahref="https://hexo.io/zh-cn/">Hexo</a> 博客了，建议你将 Hexo升级到最新稳定的版本。</p><p>点击 <ahref="https://codeload.github.com/blinkfox/hexo-theme-matery/zip/master">这里</a>下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将<code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的<code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用<code>git clone</code> 命令来下载:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/blinkfox/hexo-theme-matery.git<br></code></pre></td></tr></table></figure><h2 id="配置">配置</h2><h3 id="切换主题">切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的 <code>theme</code>的值：<code>theme: hexo-theme-matery</code></p><h4 id="config.yml-文件的其它修改建议"><code>_config.yml</code>文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主<code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code>的倍数，如：<code>12</code>、<code>18</code>等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为<code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页">新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客<code>source</code> 目录下还没有 <code>categories/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;categories&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/categories/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">categories</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;categories&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;categories&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="新建标签-tags-页">新建标签 tags 页</h3><p><code>tags</code> 页是用来展示所有标签的页面，如果在你的博客<code>source</code> 目录下还没有 <code>tags/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;tags&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/tags/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">tags</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 18:23:38</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;tags&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;tags&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="新建关于我-about-页">新建关于我 about 页</h3><p><code>about</code>页是用来展示<strong>关于我和我的博客</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>about/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;about&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/about/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">about</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;about&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;about&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="新建留言板-contact-页可选的">新建留言板 contact页（可选的）</h3><p><code>contact</code>页是用来展示<strong>留言板</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>contact/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;contact&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/contact/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">contact</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;contact&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;contact&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><blockquote><p><strong>注</strong>：本留言板功能依赖于第三方评论系统，请<strong>激活</strong>你的评论系统才有效果。并且在主题的<code>_config.yml</code> 文件中，第 <code>19</code> 至 <code>21</code>行的“<strong>菜单</strong>”配置，取消关于留言板的注释即可。</p></blockquote><h3 id="新建友情链接-friends-页可选的">新建友情链接 friends页（可选的）</h3><p><code>friends</code>页是用来展示<strong>友情链接</strong>信息的页面，如果在你的博客<code>source</code> 目录下还没有 <code>friends/index.md</code>文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new page <span class="hljs-string">&quot;friends&quot;</span><br></code></pre></td></tr></table></figure><p>编辑你刚刚新建的页面文件<code>/source/friends/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">friends</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-12-12 21:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;friends&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;friends&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><p>同时，在你的博客 <code>source</code> 目录下新建 <code>_data</code>目录，在 <code>_data</code> 目录中新建 <code>friends.json</code>文件，文件内容如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;avatar&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;http://image.luokangyuan.com/1_qq_27922023.jpg&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;码酱&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;introduction&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我不是大佬，只是在追寻大佬的脚步&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;http://luokangyuan.com/&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;前去学习&quot;</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;avatar&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;http://image.luokangyuan.com/4027734.jpeg&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;闪烁之狐&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;introduction&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;https://blinkfox.github.io/&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;前去学习&quot;</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;avatar&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;http://image.luokangyuan.com/avatar.jpg&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;ja_rome&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;introduction&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;平凡的脚步也可以走出伟大的行程&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;url&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;https://me.csdn.net/jlh912008548&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;title&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;前去学习&quot;</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><h3 id="新建-404-页">新建 404 页</h3><p>如果在你的博客 <code>source</code> 目录下还没有 <code>404.md</code>文件，那么你就需要新建一个。编辑你刚刚新建的页面文件<code>/source/404.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-number">404</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-30 17:25:30</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">&quot;404&quot;</span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">&quot;404&quot;</span><br><span class="hljs-attr">description:</span> <span class="hljs-string">&quot;Oops～，我崩溃了！找不到你想要的页面 :(&quot;</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="菜单导航配置">菜单导航配置</h3><h4id="配置基本菜单导航的名称路径url和图标icon.">配置基本菜单导航的名称、路径url和图标icon.</h4><p>1.菜单导航名称可以是中文也可以是英文(如：<code>Index</code>或<code>主页</code>)2.图标icon 可以在<a href="https://fontawesome.com/icons">FontAwesome</a> 中查找</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-attr">Index:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-home</span><br>  <span class="hljs-attr">Tags:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/tags</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-tags</span><br>  <span class="hljs-attr">Categories:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/categories</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-bookmark</span><br>  <span class="hljs-attr">Archives:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/archives</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-archive</span><br>  <span class="hljs-attr">About:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/about</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-user-circle</span><br>  <span class="hljs-attr">Friends:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/friends</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-address-book</span><br></code></pre></td></tr></table></figure><h4 id="二级菜单配置方法">二级菜单配置方法</h4><p>如果你需要二级菜单则可以在原基本菜单导航的基础上如下操作</p><ol type="1"><li>在需要添加二级菜单的一级菜单下添加<code>children</code>关键字(如:<code>About</code>菜单下添加<code>children</code>)<br /></li><li>在<code>children</code>下创建二级菜单的名称name,路径url和图标icon.<br /></li><li>注意每个二级菜单模块前要加 <code>-</code>.<br /></li><li>注意缩进格式</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-attr">Index:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-home</span><br>  <span class="hljs-attr">Tags:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/tags</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-tags</span><br>  <span class="hljs-attr">Categories:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/categories</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-bookmark</span><br>  <span class="hljs-attr">Archives:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/archives</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-archive</span><br>  <span class="hljs-attr">About:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/about</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-user-circle-o</span><br>  <span class="hljs-attr">Friends:</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">/friends</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-address-book</span><br>  <span class="hljs-attr">Medias:</span><br>    <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-list</span><br>    <span class="hljs-attr">children:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Music</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/music</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-music</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Movies</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/movies</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-film</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Books</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/books</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-book</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Galleries</span><br>        <span class="hljs-attr">url:</span> <span class="hljs-string">/galleries</span><br>        <span class="hljs-attr">icon:</span> <span class="hljs-string">fas</span> <span class="hljs-string">fa-image</span><br></code></pre></td></tr></table></figure><h3 id="代码高亮">代码高亮</h3><p>从 Hexo5.0 版本开始自带了 <code>prismjs</code>代码语法高亮的支持，本主题对此进行了改造支持。</p><p>如果你的博客中曾经安装过 <code>hexo-prism-plugin</code>的插件，那么你须要执行 <code>npm uninstall hexo-prism-plugin</code>来卸载掉它，否则生成的代码中会有 <code>&amp;#123;</code> 和<code>&amp;#125;</code> 的转义字符。</p><p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中<code>highlight.enable</code> 的值为 <code>false</code>，并将<code>prismjs.enable</code> 的值设置为<code>true</code>，主要配置如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">highlight:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">line_number:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">auto_detect:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">tab_replace:</span> <span class="hljs-string">&#x27;&#x27;</span><br>  <span class="hljs-attr">wrap:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">hljs:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">prismjs:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">preprocess:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">line_number:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">tab_replace:</span> <span class="hljs-string">&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>主题中默认的 <code>prismjs</code> 主题是<code>Tomorrow Night</code>，如果你想定制自己的主题，可以前往 <ahref="https://prismjs.com/download.html">prismjs 下载页面</a>定制下载自己喜欢的主题 <code>css</code> 文件，然后将此 css主题文件取名为 <code>prism.css</code>，替换掉<code>hexo-theme-matery</code> 主题文件夹中的<code>source/libs/prism/prism.css</code> 文件即可。</p><h3 id="搜索">搜索</h3><p>本主题中还使用到了 <ahref="https://github.com/wzpan/hexo-generator-search">hexo-generator-search</a>的 Hexo 插件来做内容搜索，安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-generator-search --save<br></code></pre></td></tr></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">search:</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">search.xml</span><br>  <span class="hljs-attr">field:</span> <span class="hljs-string">post</span><br></code></pre></td></tr></table></figure><h3 id="中文链接转拼音建议安装">中文链接转拼音（建议安装）</h3><p>如果你的文章名称是中文的，那么 Hexo默认生成的永久链接也会有中文，这样不利于 <code>SEO</code>，且<code>gitment</code> 评论对中文链接也不支持。我们可以用 <ahref="https://github.com/viko16/hexo-permalink-pinyin">hexo-permalink-pinyin</a>Hexo 插件使在生成文章时生成中文拼音的永久链接。</p><p>安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm i hexo-permalink-pinyin --save<br></code></pre></td></tr></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">permalink_pinyin:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">separator:</span> <span class="hljs-string">&#x27;-&#x27;</span> <span class="hljs-comment"># default: &#x27;-&#x27;</span><br></code></pre></td></tr></table></figure><blockquote><p><strong>注</strong>：除了此插件外，<ahref="https://github.com/rozbo/hexo-abbrlink">hexo-abbrlink</a>插件也可以生成非中文的链接。</p></blockquote><h3 id="文章字数统计插件建议安装">文章字数统计插件（建议安装）</h3><p>如果你想要在文章中显示文章字数、阅读时长信息，可以安装 <ahref="https://github.com/willin/hexo-wordcount">hexo-wordcount</a>插件。</p><p>安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm i --save hexo-wordcount<br></code></pre></td></tr></table></figure><p>然后只需在本主题下的 <code>_config.yml</code>文件中，将各个文章字数相关的配置激活即可：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">postInfo:</span><br>  <span class="hljs-attr">date:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">update:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">wordCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 设置文章字数统计为 true.</span><br>  <span class="hljs-attr">totalCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 设置站点文章总字数统计为 true.</span><br>  <span class="hljs-attr">min2read:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 阅读时长.</span><br>  <span class="hljs-attr">readCount:</span> <span class="hljs-literal">false</span> <span class="hljs-comment"># 阅读次数.</span><br></code></pre></td></tr></table></figure><h3 id="添加emoji表情支持可选的">添加emoji表情支持（可选的）</h3><p>本主题新增了对<code>emoji</code>表情的支持，使用到了 <ahref="https://npm.taobao.org/package/hexo-filter-github-emojis">hexo-filter-github-emojis</a>的 Hexo 插件来支持<code>emoji</code>表情的生成，把对应的<code>markdown emoji</code>语法（<code>::</code>,例如：<code>:smile:</code>）转变成会跳跃的<code>emoji</code>表情，安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-filter-github-emojis --save<br></code></pre></td></tr></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">githubEmojis:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">className:</span> <span class="hljs-string">github-emoji</span><br>  <span class="hljs-attr">inject:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">styles:</span><br>  <span class="hljs-attr">customEmojis:</span><br></code></pre></td></tr></table></figure><p>执行 <code>hexo clean &amp;&amp; hexo g</code>重新生成博客文件，然后就可以在文章中对应位置看到你用<code>emoji</code>语法写的表情了。</p><h3 id="添加-rss-订阅支持可选的">添加 RSS 订阅支持（可选的）</h3><p>本主题中还使用到了 <ahref="https://github.com/hexojs/hexo-generator-feed">hexo-generator-feed</a>的 Hexo 插件来做 <code>RSS</code>，安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-generator-feed --save<br></code></pre></td></tr></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code>文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">feed:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">atom</span><br>  <span class="hljs-attr">path:</span> <span class="hljs-string">atom.xml</span><br>  <span class="hljs-attr">limit:</span> <span class="hljs-number">20</span><br>  <span class="hljs-attr">hub:</span><br>  <span class="hljs-attr">content:</span><br>  <span class="hljs-attr">content_limit:</span> <span class="hljs-number">140</span><br>  <span class="hljs-attr">content_limit_delim:</span> <span class="hljs-string">&#x27; &#x27;</span><br>  <span class="hljs-attr">order_by:</span> <span class="hljs-string">-date</span><br></code></pre></td></tr></table></figure><p>执行 <code>hexo clean &amp;&amp; hexo g</code>重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到<code>atom.xml</code> 文件，说明你已经安装成功了。</p><h3 id="添加-daovoice-在线聊天功能可选的">添加 <ahref="http://www.daovoice.io/">DaoVoice</a> 在线聊天功能（可选的）</h3><p>前往 <a href="http://www.daovoice.io/">DaoVoice</a> 官网注册并且获取<code>app_id</code>，并将 <code>app_id</code> 填入主题的<code>_config.yml</code> 文件中。</p><h3 id="添加-tidio-在线聊天功能可选的">添加 <ahref="https://www.tidio.com/">Tidio</a> 在线聊天功能（可选的）</h3><p>前往 <a href="https://www.tidio.com/">Tidio</a> 官网注册并且获取<code>Public Key</code>，并将 <code>Public Key</code> 填入主题的<code>_config.yml</code> 文件中。</p><h3 id="修改页脚">修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的<code>/layout/_partial/footer.ejs</code>文件中，包括站点、使用的主题、访问量等。</p><h3 id="添加中文繁简转换">添加中文繁简转换</h3><p>在主题的 <code>_config.yml</code> 文件中，开启 translate 为enable。</p><blockquote><p>开启中文繁简转换如下修改。默认不开启。 实例演示： <ahref="https://blog.17lai.site">繁简转换</a> 底下 footer 栏</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">translate:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><h3 id="修改社交链接">修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持<code>QQ</code>、<code>GitHub</code> 和邮箱等的配置，你可以在主题文件的<code>/layout/_partial/social-link.ejs</code>文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs html">&lt;% if (theme.socialLink.github) &#123; %&gt;<br>    <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;&lt;%= theme.socialLink.github %&gt;&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;tooltipped&quot;</span> <span class="hljs-attr">target</span>=<span class="hljs-string">&quot;_blank&quot;</span> <span class="hljs-attr">data-tooltip</span>=<span class="hljs-string">&quot;访问我的GitHub&quot;</span> <span class="hljs-attr">data-position</span>=<span class="hljs-string">&quot;top&quot;</span> <span class="hljs-attr">data-delay</span>=<span class="hljs-string">&quot;50&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">i</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;fab fa-github&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">i</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>&lt;% &#125; %&gt;<br></code></pre></td></tr></table></figure><p>其中，社交图标（如：<code>fa-github</code>）你可以在 <ahref="https://fontawesome.com/icons">Font Awesome</a>中搜索找到。以下是常用社交图标的标识，供你参考：</p><ul><li>Facebook: <code>fab fa-facebook</code></li><li>Twitter: <code>fab fa-twitter</code></li><li>Google-plus: <code>fab fa-google-plus</code></li><li>Linkedin: <code>fab fa-linkedin</code></li><li>Tumblr: <code>fab fa-tumblr</code></li><li>Medium: <code>fab fa-medium</code></li><li>Slack: <code>fab fa-slack</code></li><li>Sina Weibo: <code>fab fa-weibo</code></li><li>Wechat: <code>fab fa-weixin</code></li><li>QQ: <code>fab fa-qq</code></li><li>Zhihu: <code>fab fa-zhihu</code></li></ul><blockquote><p><strong>注意</strong>: 本主题中使用的 <code>Font Awesome</code>版本为 <code>5.11.0</code>。</p></blockquote><h3 id="修改打赏的二维码图片">修改打赏的二维码图片</h3><p>在主题文件的 <code>source/medias/reward</code>文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p><h3 id="配置音乐播放器可选的">配置音乐播放器（可选的）</h3><p>要支持音乐播放，在主题的 <code>_config.yml</code>配置文件中激活music配置即可：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 是否在首页显示音乐</span><br><span class="hljs-attr">music:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">title:</span>         <span class="hljs-comment"># 非吸底模式有效</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">show:</span> <span class="hljs-string">听听音乐</span><br>  <span class="hljs-attr">server:</span> <span class="hljs-string">netease</span>   <span class="hljs-comment"># require music platform: netease, tencent, kugou, xiami, baidu</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">playlist</span>    <span class="hljs-comment"># require song, playlist, album, search, artist</span><br>  <span class="hljs-attr">id:</span> <span class="hljs-number">503838841</span>     <span class="hljs-comment"># require song id / playlist id / album id / search keyword</span><br>  <span class="hljs-attr">fixed:</span> <span class="hljs-literal">false</span>      <span class="hljs-comment"># 开启吸底模式</span><br>  <span class="hljs-attr">autoplay:</span> <span class="hljs-literal">false</span>   <span class="hljs-comment"># 是否自动播放</span><br>  <span class="hljs-attr">theme:</span> <span class="hljs-string">&#x27;#42b983&#x27;</span><br>  <span class="hljs-attr">loop:</span> <span class="hljs-string">&#x27;all&#x27;</span>       <span class="hljs-comment"># 音频循环播放, 可选值: &#x27;all&#x27;, &#x27;one&#x27;, &#x27;none&#x27;</span><br>  <span class="hljs-attr">order:</span> <span class="hljs-string">&#x27;random&#x27;</span>   <span class="hljs-comment"># 音频循环顺序, 可选值: &#x27;list&#x27;, &#x27;random&#x27;</span><br>  <span class="hljs-attr">preload:</span> <span class="hljs-string">&#x27;auto&#x27;</span>   <span class="hljs-comment"># 预加载，可选值: &#x27;none&#x27;, &#x27;metadata&#x27;, &#x27;auto&#x27;</span><br>  <span class="hljs-attr">volume:</span> <span class="hljs-number">0.7</span>       <span class="hljs-comment"># 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效</span><br>  <span class="hljs-attr">listFolded:</span> <span class="hljs-literal">true</span>  <span class="hljs-comment"># 列表默认折叠</span><br></code></pre></td></tr></table></figure><blockquote><p><code>server</code>可选<code>netease</code>（网易云音乐），<code>tencent</code>（QQ音乐），<code>kugou</code>（酷狗音乐），<code>xiami</code>（虾米音乐），</p><p><code>baidu</code>（百度音乐）。</p><p><code>type</code>可选<code>song</code>（歌曲），<code>playlist</code>（歌单），<code>album</code>（专辑），<code>search</code>（搜索关键字），<code>artist</code>（歌手）</p><p><code>id</code>获取方法示例:浏览器打开网易云音乐，点击我喜欢的音乐歌单，浏览器地址栏后面会有一串数字，<code>playlist</code>的<code>id</code></p><p>即为这串数字。</p></blockquote><h3 id="添加note">添加note</h3><blockquote><p><ahref="https://blog.17lai.site/posts/cf0f47fd/#tag-note">演示</a></p></blockquote><h4 id="usage">Usage</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% note [class] [no-icon] [summary] %&#125;<br>Any content (support inline tags too).<br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><ul><li><code>[class]</code> : <em>Optional parameter.</em> Supportedvalues: default | primary | success | info | warning | danger.</li><li><code>[no-icon]</code> : <em>Optional parameter.</em> Disable iconin note.</li><li><code>[summary]</code> : <em>Optional parameter.</em> Optionalsummary of the note.</li></ul><p>All parameters are optional.</p><h4 id="example">example</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% note %&#125;<br>#### Header<br>(without define class style)<br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><h3 id="添加button">添加button</h3><blockquote><p><ahref="https://blog.17lai.site/posts/cf0f47fd/#tag-button">演示</a></p></blockquote><h4 id="usage-1">Usage</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% button url, text, icon [class], [title] %&#125;<br></code></pre></td></tr></table></figure><p>or</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% btn url, text, icon [class], [title] %&#125;<br></code></pre></td></tr></table></figure><ul><li><code>url</code> : Absolute or relative path to URL.</li><li><code>text</code> : Button text. Required if no icon specified.</li><li><code>icon</code> : Font Awesome icon name. Required if no textspecified.</li><li><code>[class]</code> : <em>Optional parameter.</em> Font Awesomeclass(es): <code>fa-fw</code> | <code>fa-lg</code> | <code>fa-2x</code>| <code>fa-3x</code> | <code>fa-4x</code> | <code>fa-5x</code></li><li><code>[title]</code> : <em>Optional parameter.</em> Tooltip atmouseover.</li></ul><h4 id="examples">Examples</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html">&#123;% button #, Text %&#125;<br></code></pre></td></tr></table></figure><h2 id="文章-front-matter-介绍">文章 Front-matter 介绍</h2><h3 id="front-matter-选项详解">Front-matter 选项详解</h3><p><code>Front-matter</code>选项中的所有内容均为<strong>非必填</strong>的。但我仍然建议至少填写<code>title</code> 和 <code>date</code> 的值。</p><table><thead><tr class="header"><th>配置选项</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>title</td><td><code>Markdown</code> 的文件标题</td><td>文章标题，强烈建议填写此选项</td></tr><tr class="even"><td>date</td><td>文件创建时的日期时间</td><td>发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr class="odd"><td>author</td><td>根 <code>_config.yml</code> 中的 <code>author</code></td><td>文章作者</td></tr><tr class="even"><td>img</td><td><code>featureImages</code> 中的某个值</td><td>文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如:<code>http://xxx.com/xxx.jpg</code></td></tr><tr class="odd"><td>top</td><td><code>true</code></td><td>推荐文章（文章是否置顶），如果 <code>top</code> 值为<code>true</code>，则会作为首页推荐文章</td></tr><tr class="even"><td>hide</td><td><code>false</code></td><td>隐藏文章，如果<code>hide</code>值为<code>true</code>，则文章不会在首页显示</td></tr><tr class="odd"><td>cover</td><td><code>false</code></td><td><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr class="even"><td>coverImg</td><td>无</td><td><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr class="odd"><td>password</td><td>无</td><td>文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置<code>password</code> 的值，该值必须是用 <code>SHA256</code>加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code>中激活了 <code>verifyPassword</code> 选项</td></tr><tr class="even"><td>toc</td><td><code>true</code></td><td>是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的<code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr class="odd"><td>mathjax</td><td><code>false</code></td><td>是否开启数学公式支持 ，本文章是否开启<code>mathjax</code>，且需要在主题的 <code>_config.yml</code>文件中也需要开启才行</td></tr><tr class="even"><td>summary</td><td>无</td><td>文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr class="odd"><td>categories</td><td>无</td><td>文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr class="even"><td>tags</td><td>无</td><td>文章标签，一篇文章可以多个标签</td></tr><tr class="odd"><td>keywords</td><td>文章标题</td><td>文章关键字，SEO 时需要</td></tr><tr class="even"><td>reprintPolicy</td><td>cc_by</td><td>文章转载规则， 可以是 cc_by, cc_by_nd, cc_by_sa, cc_by_nc,cc_by_nc_nd, cc_by_nc_sa, cc0, noreprint 或 pay 中的一个</td></tr></tbody></table><blockquote><p><strong>注意</strong>: 1. 如果 <code>img</code>属性不填写的话，文章特色图会根据文章标题的 <code>hashcode</code>的值取余，然后选取主题中对应的特色图片，从而达到让所有文章的特色图<strong>各有特色</strong>。2. <code>date</code> 的值尽量保证每篇文章是唯一的，因为本主题中<code>Gitalk</code> 和 <code>Gitment</code> 识别 <code>id</code> 是通过<code>date</code> 的值来作为唯一标识的。 3.如果要对文章设置阅读验证密码的功能，不仅要在 Front-matter 中设置采用了SHA256 加密的 password 的值，还需要在主题的 <code>_config.yml</code>中激活了配置。有些在线的 SHA256 加密的地址，可供你使用：<ahref="http://tool.oschina.net/encrypt?type=2">开源中国在线工具</a>、<ahref="http://encode.chahuo.com/">chahuo</a>、<ahref="http://tool.chinaz.com/tools/hash.aspx">站长工具</a>。 4.您可以在文章md文件的 front-matter 中指定 reprintPolicy来给单个文章配置转载规则</p></blockquote><p>以下为文章的 <code>Front-matter</code> 示例。</p><h3 id="最简示例">最简示例</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">typora-vue-theme主题介绍</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-07 09:25:00</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h3 id="最全示例">最全示例</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">typora-vue-theme主题介绍</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2018-09-07 09:25:00</span><br><span class="hljs-attr">author:</span> <span class="hljs-string">赵奇</span><br><span class="hljs-attr">img:</span> <span class="hljs-string">/source/images/xxx.jpg</span><br><span class="hljs-attr">top:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">hide:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">cover:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">coverImg:</span> <span class="hljs-string">/images/1.jpg</span><br><span class="hljs-attr">password:</span> <span class="hljs-string">8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92</span><br><span class="hljs-attr">toc:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">mathjax:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">summary:</span> <span class="hljs-string">这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</span><br><span class="hljs-attr">categories:</span> <span class="hljs-string">Markdown</span><br><span class="hljs-attr">tags:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Typora</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">Markdown</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure><h2 id="自定制修改">自定制修改</h2><p>在本主题的 <code>_config.yml</code>中可以修改部分自定义信息，有以下几个部分：</p><ul><li>菜单</li><li>我的梦想</li><li>首页的音乐播放器和视频播放器配置</li><li>是否显示推荐文章名称和按钮配置</li><li><code>favicon</code> 和 <code>Logo</code></li><li>个人信息</li><li>TOC 目录</li><li>文章打赏信息</li><li>复制文章内容时追加版权信息</li><li>MathJax</li><li>文章字数统计、阅读时长</li><li>点击页面的'爱心'效果</li><li>我的项目</li><li>我的技能</li><li>我的相册</li><li><code>Gitalk</code>、<code>Gitment</code>、<code>Valine</code> 和<code>disqus</code> 评论配置</li><li><ahref="http://busuanzi.ibruce.info/">不蒜子统计</a>和谷歌分析（<code>Google Analytics</code>）</li><li>默认特色图的集合。当文章没有设置特色图时，本主题会根据文章标题的<code>hashcode</code> 值取余，来选择展示对应的特色图</li></ul><p><strong>我认为个人博客应该都有自己的风格和特色</strong>。如果本主题中的诸多功能和主题色彩你不满意，可以在主题中自定义修改，很多更自由的功能和细节点的修改难以在主题的<code>_config.yml</code>中完成，需要修改源代码才来完成。以下列出了可能对你有用的地方：</p><h3 id="修改主题颜色">修改主题颜色</h3><p>在主题文件的 <code>/source/css/matery.css</code> 文件中，搜索<code>.bg-color</code> 来修改背景颜色：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-comment">/* 整体背景颜色，包括导航、移动端的导航、页尾、标签页等的背景颜色. */</span><br><span class="hljs-selector-class">.bg-color</span> &#123;<br>    <span class="hljs-attribute">background-image</span>: <span class="hljs-built_in">linear-gradient</span>(to right, <span class="hljs-number">#4cbf30</span> <span class="hljs-number">0%</span>, <span class="hljs-number">#0f9d58</span> <span class="hljs-number">100%</span>);<br>&#125;<br><br><span class="hljs-keyword">@-webkit-keyframes</span> rainbow &#123;<br>   <span class="hljs-comment">/* 动态切换背景颜色. */</span><br>&#125;<br><br><span class="hljs-keyword">@keyframes</span> rainbow &#123;<br>    <span class="hljs-comment">/* 动态切换背景颜色. */</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="修改-banner-图和文章特色图">修改 banner 图和文章特色图</h3><p>你可以直接在 <code>/source/medias/banner</code> 文件夹中更换你喜欢的<code>banner</code> 图片，主题代码中是每天动态切换一张，只需<code>7</code> 张即可。如果你会 <code>JavaScript</code>代码，可以修改成你自己喜欢切换逻辑，如：随机切换等，<code>banner</code>切换的代码位置在 <code>/layout/_partial/bg-cover-content.ejs</code>文件的 <code>&lt;script&gt;&lt;/script&gt;</code> 代码中：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs javascript">$(<span class="hljs-string">&#x27;.bg-cover&#x27;</span>).<span class="hljs-title function_">css</span>(<span class="hljs-string">&#x27;background-image&#x27;</span>, <span class="hljs-string">&#x27;url(/medias/banner/&#x27;</span> + <span class="hljs-keyword">new</span> <span class="hljs-title class_">Date</span>().<span class="hljs-title function_">getDay</span>() + <span class="hljs-string">&#x27;.jpg)&#x27;</span>);<br></code></pre></td></tr></table></figure><p>在 <code>/source/medias/featureimages</code> 文件夹中默认有 24张特色图片，你可以再增加或者减少，并需要在 <code>_config.yml</code>做同步修改。</p><p>参考</p><p>[1] <ahref="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">hexo-theme-matery中文文档</a></p>]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Latex &amp; Markdown 公式篇</title>
    <link href="/2023/06/01/latex-markdown-gong-shi-pian/"/>
    <url>/2023/06/01/latex-markdown-gong-shi-pian/</url>
    
    <content type="html"><![CDATA[<p>在 LaTeX中，使用“align”环境可以方便地实现等号对齐。使用“&amp;”符号可以在每行中分隔等号两边的内容，表示对齐位置，‘’\\‘’表示换行<spanclass="math inline">\(\lambda\)</span>：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;aligned&#125;<br>  2x + 3y <span class="hljs-built_in">&amp;</span>= 7 <span class="hljs-keyword">\\</span><br>  5x - 2y <span class="hljs-built_in">&amp;</span>= 1<br><span class="hljs-keyword">\end</span>&#123;aligned&#125;<br></code></pre></td></tr></table></figure><p>该代码将产生以下等式： <span class="math display">\[\begin{align}  2x + 3y &amp;= 7 \\\\  5x - 2y &amp;= 1\end{align}\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Latex</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Latex</tag>
      
      <tag>Markdown</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github kex_exchange_identification Connection closed by remote host</title>
    <link href="/2023/05/31/github-kex-exchange/"/>
    <url>/2023/05/31/github-kex-exchange/</url>
    
    <content type="html"><![CDATA[<h2 id="问题描述">问题描述</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">kex_exchange_identification: Connection closed by remote host<br>Connection closed by 20.205.243.166 port 22<br>fatal: Could not <span class="hljs-built_in">read</span> from remote repository.<br><br>Please make sure you have the correct access rights<br>and the repository exists.<br></code></pre></td></tr></table></figure><h2 id="解决方法">解决方法</h2><p><strong>本地生成.pub文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-keygen -t ed25519 -C <span class="hljs-string">&quot;your_mail@xxx.com&quot;</span><br>ssh-agent bash<br></code></pre></td></tr></table></figure><p>会在<code>C:\Users\admin\.ssh</code>目录生成一个<code>id_ed25519.pub</code>文件，复制里面的内容。</p><p><strong>Github 新建 SSH key</strong></p><p>打开 Github，点击 Setting 如下图所示：</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531145549324.png"alt="Setting" /><figcaption aria-hidden="true">Setting</figcaption></figure><p>在 Setting 页面中找到 SSH and GPG keys 选项，新建 SSH key。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531145936726.png"alt="SSH and GPG keys" /><figcaption aria-hidden="true">SSH and GPG keys</figcaption></figure><p>Title 内容任意，Key 文本框内填入 <code>id_ed25519.pub</code>内的内容。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230531150100824.png"alt="SSH keys" /><figcaption aria-hidden="true">SSH keys</figcaption></figure><p>重新尝试上传。</p><p>如果仍然有上述错误提示可进行以下操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">git init<br>git add .<br>git commit -m &quot;init&quot;<br>git branch -M main<br>git remote add origin git@github.com:xxx/xxx.git<br>git push -f -u origin main<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Github</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Github</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Github Pages + Hexo + Vercel 个人博客搭建（一）基础部署</title>
    <link href="/2023/05/31/github-pages-ge-ren-bo-ke-da-jian-yi-ji-chu-bu-shu/"/>
    <url>/2023/05/31/github-pages-ge-ren-bo-ke-da-jian-yi-ji-chu-bu-shu/</url>
    
    <content type="html"><![CDATA[<h2 id="本地创建环境">本地创建环境</h2><p>本人环境：</p><p><code>windows 10</code></p><h3 id="安装-node.js">安装 node.js</h3><p>建议使用 nvm （node version manager（node版本管理工具））安装node.js，</p><p><strong>下载地址</strong>：https://github.com/coreybutler/nvm-windows/releases</p><p><strong>安装</strong></p><p>（1）双击解压后的文件<code>nvm-setup.exe</code>； （2）选择 nvm安装路径（填坑警告：路径不能有空格！！！） （3）选择 node.js 路径；（4）确认安装； （5）检测：打开 cmd，输入<code>nvm</code>，显示当前 nvm版本以及 nvm 命令，成功！</p><p><strong>使用 nvm</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 1.nvm list 命令 - 显示版本列表</span><br>nvm list // 显示已安装的版本（同 nvm list installed）<br>nvm list installed // 显示已安装的版本<br>nvm list available // 显示所有可以下载的版本<br><br><span class="hljs-comment"># 2.nvm install 命令 - 安装指定版本nodejs</span><br>nvm install 16.15.1// 安装16.15.1版本node<br>nvm install latest // 安装最新版本node<br><br><span class="hljs-comment"># 3.nvm use 命令 - 使用指定版本node</span><br>nvm use 16.15.1 // 使用16.15.1版本node<br><br><span class="hljs-comment"># 4.nvm uninstall 命令 - 卸载指定版本 node</span><br>nvm uninstall 16.15.1 // 卸载16.15.1版本node<br></code></pre></td></tr></table></figure><p>填坑警告：nvm install 的时候，出现无权安装，需<code>以管理员身份</code>运行 cmd。！！！</p><p>本人目前安装 node.js 版本为16.15.1.</p><p><strong>设置nodejsprefix（全局）和cache（缓存）路径（非必须操作）</strong></p><p>在<code>nodejs</code>安装目录下新建两个文件夹，用于存放全局包和缓存，如下： 我的 node.js安装目录：<code>E:\Program\nvm</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm config <span class="hljs-built_in">set</span> prefix <span class="hljs-string">&quot;E:\Program\nvm\node_gobal&quot;</span> <br>npm config <span class="hljs-built_in">set</span> cache <span class="hljs-string">&quot;E:\Program\nvm\node_cache&quot;</span><br></code></pre></td></tr></table></figure><h3 id="安装-hexo">安装 Hexo</h3><p>命令行输入以下命令安装 Hexo：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install -g hexo-cli<br></code></pre></td></tr></table></figure><p>输入以下命令验证是否安装成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo -v<br></code></pre></td></tr></table></figure><h2 id="本地部署">本地部署</h2><p>选择一个准备放置博客网站的目录，然后使用以下命令来初始化一个项目：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init LFD-byte.github.io<br><span class="hljs-built_in">cd</span> LFD-byte.github.io<br>npm install<br></code></pre></td></tr></table></figure><p>该命令将会在当前目录下，生成一个名为 <code>LFD-byte.github.io</code>的新目录，当然，你可以把这个名字换成任何你想要的名字，并将<code>hexo</code> 的初始化文件写入其中。</p><p>新建完成后，<code>LFD-byte.github.io</code> 文件夹的目录如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">.<br>├── _config.yml <br>├── package.json <br>├── node_modules <br>├── scaffolds <br>├── <span class="hljs-built_in">source</span> <br>| ├── _drafts <br>| └── _posts <br>└── themes<br></code></pre></td></tr></table></figure><p><code>_config.yml</code>是配置文件，里面有很多可以配置的数据，这里暂时不多介绍，后面的文章里会进行详细说明。</p><p><code>package.json</code> 是应用程序信息，通常不需要关心。</p><p><code>node_modules</code> 用来存放 <code>node</code>相关的模块，通常不需要关心。</p><p><code>scaffolds</code>里面是模版文件，也就是每次新建文章时，都会根据模版文件来创建对应的<code>md</code> 文件，这一点也会在后续的文章里进行详细介绍。</p><p><code>source</code> 是资源文件夹，用来存放用户资源的地方。除<code>_posts</code> 文件夹之外，开头命名为 _ (下划线)的文件 /文件夹和隐藏的文件将会被忽略。</p><p><code>theme</code>是主题文件夹，每个主题的配置都会有些不一样，需要根据具体主题情况来定，后续介绍主题的文章里会有说明。</p><p>在 <code>breeze-blog</code> 目录下使用以下命令来运行我们的博客：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo server<br></code></pre></td></tr></table></figure><p>在默认情况下，服务会使用 <code>4000</code>端口，如果已经被占用，也可以添加 <code>-p</code>参数来换用其它端口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo server -p 8080<br></code></pre></td></tr></table></figure><p>打开 <code>http://localhost:4000</code>即可访问我们生成的网站了。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedntX31VrhMTjFozQ.jpg"alt="Hexo" /><figcaption aria-hidden="true">Hexo</figcaption></figure><p>这样，我们的博客就搭建起来了。</p><h2 id="部署到-github-pages">部署到 Github Pages</h2><p>在你的 Github 账号创建一个新的仓库，仓库命名规范为<code>账号名.github.io</code>，公开仓库。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230604151148301.png"alt="create repository" /><figcaption aria-hidden="true">create repository</figcaption></figure><p>在 Git bash 或 CMD 中<code>LFD-byte.github.io</code>博客目录下执行以下命令连接到 Github：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">git init<br>git add .<br>git commit -m <span class="hljs-string">&quot;first commit&quot;</span><br>git branch -M main<br>git remote add origin git@github.com:xxx/xxx.git<br>git push -u origin main<br></code></pre></td></tr></table></figure><p>在 <code>LFD-byte.github.io</code> 仓库下 Settings 中开启 GithubPages：</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230604152021402.png"alt="open github pages" /><figcaption aria-hidden="true">open github pages</figcaption></figure><p>然后我们修改一下本地的 <code>hexo</code>的配置文件(<code>_config.yml</code>)，我的在<code>LFD-byte.github.io</code>根目录下，找到对应的地方进行修改，指定我们的仓库信息，并修改<code>deploy</code> 信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">deploy:<br><span class="hljs-built_in">type</span>: git<br>repo: git@github.com:xxx/blogxxx<br>branch: main<br></code></pre></td></tr></table></figure><p>把这里的 <code>repo</code> 地址修改为你的仓库地址即可。</p><p>安装 <code>hexo-deployer-git</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> LFD-byte.github.io<br>npm install hexo-deployer-git --save<br></code></pre></td></tr></table></figure><p>进行部署</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean &amp;&amp; hexo generate<br>hexo deploy<br></code></pre></td></tr></table></figure><p>运行完成后，我们的博客文件就顺利部署到 <code>github pages</code>上了，现在我们打开下面网址来查看我们的博客效果： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">https://用户名.github.io<br></code></pre></td></tr></table></figure></p><p>之后每次我们添加或修改完本地文件后，使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean &amp;&amp; hexo g -d<br></code></pre></td></tr></table></figure><p>即可重新生成项目文件。</p><h2 id="部署到-vercel">部署到 Vercel</h2><p>注册 <a href="https://vercel.com/">Vercel</a>账号，建议谷歌邮箱注册。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedvercel-logo-freelogovectors.net.jpg"alt="vercel" /><figcaption aria-hidden="true">vercel</figcaption></figure><p>使用 GitHub 账户登录 <a href="https://vercel.com/">Vercel</a>，授予Vercel repo 的 read 权限。</p><p>导入 GitHub 账户中的网站 repo，比如此处的 LFD-byte.github.io。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205007431.png"alt="Project import" /><figcaption aria-hidden="true">Project import</figcaption></figure><p>在项目构建中，Framework Preset 选择 Other，其余不做改动。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205116417.png"alt="Build" /><figcaption aria-hidden="true">Build</figcaption></figure><p>稍等片刻，部署成功，此时我们就可以直接通过部署完成后 Vercel提供的域名访问个人网站了，点击 Visit 进行访问。</p><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205341204.png"alt="Production Deployment" /><figcaption aria-hidden="true">Production Deployment</figcaption></figure><figure><imgsrc="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20230606205507057.png"alt="Visit Blog" /><figcaption aria-hidden="true">Visit Blog</figcaption></figure><p>之后每次我们添加或修改完本地文件后，使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo clean &amp;&amp; hexo g -d<br></code></pre></td></tr></table></figure><p>就可以生成静态文件同步部署到 Vercel 上了。</p><h2 id="注意">注意</h2><p>若 npm 不能下载包，可通过 nvm 更换 node.js 版本重新尝试下载。</p><h2 id="参考">参考</h2><p>[1] <ahref="https://mfrank2016.github.io/breeze-blog/2020/05/02/hexo/hexo-start/">【Hexo】使用Hexo+githubpages+travis ci搭建好看的个人博客（一）</a></p><p>[2] <ahref="https://blog.csdn.net/liangpingguo/article/details/125324362">Windows下使用nvm安装nodejs</a></p><p>[3] <ahref="https://blog.csdn.net/weixin_40026797/article/details/126919662">建站过程中的踩坑记录：自定义域名、百度收录与备案</a></p>]]></content>
    
    
    <categories>
      
      <category>Blog</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Blog</tag>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（四）图神经网络（GNN）</title>
    <link href="/2023/05/31/tu-shen-jing-wang-luo-xue-xi-ri-ji-si-tu-shen-jing-wang-luo-gnn/"/>
    <url>/2023/05/31/tu-shen-jing-wang-luo-xue-xi-ri-ji-si-tu-shen-jing-wang-luo-gnn/</url>
    
    <content type="html"><![CDATA[<p><strong>置换不变性和置换同变性</strong></p><p>任何将邻接矩阵<spanclass="math inline">\(A\)</span>作为输入的函数<spanclass="math inline">\(f\)</span>在理想状态下，都应满足下面两个条件之一：<span class="math display">\[\begin{align}f(PAP^T)&amp;=f(A)\text{(置换不变)} \\f(PAP^T)&amp;=Pf(A)\text{(置换同变)}\end{align}\]</span></p><p>其中<spanclass="math inline">\(P\)</span>是置换矩阵。置换不变是指函数不依赖邻接矩阵中行/列的任意顺序，置换同变表示当置换邻接矩阵时<spanclass="math inline">\(f\)</span>的输出以一致的方式置换。</p><h2 id="神经消息传递">神经消息传递</h2><h3 id="gnn-框架">GNN 框架</h3><p>在 GNN 的每个消息传递迭代期，通过聚合每个节点<spanclass="math inline">\(u \in \mathcal{V}\)</span>的邻域<spanclass="math inline">\(\mathcal{N}(u)\)</span>的信息来更新其隐藏嵌入<spanclass="math inline">\(h_u^{(k)}\)</span>，过程如下式所示： <spanclass="math display">\[\begin{align}  h_u^{(k+1)} &amp; = UPDATE^{(k)}(h_u^{(k)},AGGREGATE^{(k)}(\{h_v^{(k)}, \forall v \in \mathcal{V}(u)\})) \\\\  &amp; = UPDATE^{(k)}(h_u^{(k)}, m_{\mathcal{N}(u)}^{(k)})\end{align}\]</span></p><p>其中 <span class="math inline">\(UPDATA\)</span> 和 <spanclass="math inline">\(AGGREGATE\)</span> 是任意可微函数，<spanclass="math inline">\(m_{\mathcal{N}(u)}\)</span> 是聚合节点 <spanclass="math inline">\(u\)</span>邻域消息的结果，上标表示消息迭代期的索引。</p><p>迭代最后一层的输出定义为每个节点的嵌入： <spanclass="math display">\[z_u = h_u^{(K)}, \forall u \in \mathcal{V}\]</span> 由于 <span class="math inline">\(AGGREGATE\)</span>函数将整个集合作为输入，这种方式定义的 GNN 是置换同变的。</p><p>节点嵌入编码了两种形式的信息。</p><p><strong>图的结构信息</strong></p><p><strong>基于节点特征的信息</strong></p><h3 id="gnn-实例">GNN 实例</h3><p>基本 GNN 的消息传递定义如下式： <span class="math display">\[h_u^{(k)} = \sigma(W_{self}^{(k)}h_u^{(k-1)} + W_{neigh}^{(k)}\sum_{v\in \mathcal{N}(u)} h_v^{(k-1)} + b^{(k)})\]</span> 其中，<span class="math inline">\(W_{self}^{(k)}\)</span> 和<span class="math inline">\(W_{neigh}^{(k)} \in \mathbb{R}^{d^{(k)}\times d^{(k-1)}}\)</span> 是可训练参数矩阵，<spanclass="math inline">\(\sigma\)</span> 表示逐元素的非线性函数。</p><p>通过定义更新和聚合函数等效地定义基本的 GNN： <spanclass="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)} h_v \\\\UPDATE(h_u, m_{\mathcal{N}(u)}) = \sigma (W_{self} h_u +W_{neigh}m_{\mathcal{N}(u)})\]</span> 下式可作为从节点 <span class="math inline">\(u\)</span>的图上邻域聚合消息的简写： <span class="math display">\[m_{\mathcal{N}(u)} = AGGREGATE^{(k)}(\{h_v^{(k)}, \forall v \in\mathcal{N}(u)\})\]</span> <strong>图级别 GNN 定义</strong> <span class="math display">\[H^{(k)} = \sigma(AH^{(k-1)}W_{neigh}^{(k)} + H^{(k-1)}W_{self}^{(k)})\]</span> 其中，<span class="math inline">\(H^{(k)} \in\mathbb{R}^{|\mathcal{V}| \times d}\)</span> 表示 GNN 中第 <spanclass="math inline">\(k\)</span>层的节点表示矩阵（每个节点对应矩阵的一行），<spanclass="math inline">\(A\)</span> 是邻接矩阵。</p><p><strong>自环消息传递</strong></p><p>添加自环并省略显示的更新步骤消息传递可定义如下： <spanclass="math display">\[h_u^{(k)} = AGGREGATE(\{ h_v^{(k-1)}, \forall v \in \mathcal{N}(n)\bigcup \{u\} \})\]</span> 其中，聚合在集合 <span class="math inline">\(\mathcal{N}(u)\bigcup \{u\}\)</span>上进行。这种消息传递方式可以缓解拟合问题，也因为无法区分节点和邻域的信息严重限制了GNN 的表达能力。</p><p>在基本 GNN 模型中，添加自环等效于在 <spanclass="math inline">\(W_{self}\)</span> 和 <spanclass="math inline">\(W_{neigh}\)</span>矩阵之间共享参数，<strong>图级别更新方式</strong>如下所示： <spanclass="math display">\[H^{(t)} = \sigma ((A+I)H^{(t-1)}W^{(t)})\]</span></p><h2 id="广义邻域聚合">广义邻域聚合</h2><h3 id="邻域归一化">邻域归一化</h3><p>最基本的邻域聚合函数仅取邻居嵌入的总和，这种方法有可能不稳定并且对节点度高度敏感。解决该问题方案之一是基于所涉及节点的度来归一化聚合操作。</p><p>均值代替求和，如下式所示： <span class="math display">\[m_{\mathcal{N}(u)} = \frac{\sum_{v \in\mathcal{N}(n)}h_v}{|\mathcal{N}(u)|}\]</span> 对称归一化，如下式所示： <span class="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)}\frac{h_v}{\sqrt{|\mathcal{N}(u)||\mathcal{N}(v)|}}\]</span> 图卷积神经网络（GCN）</p><p>采用对称归一化聚合及自环更新方法，GCN 消息传递函数如下式定义： <spanclass="math display">\[h_u^{(k)} = \sigma (W^{(k)} \sum_{v \in \mathcal{N}(u)} \bigcup \{u\}\frac{h_v}{|\mathcal{N}(u)||\mathcal{N}(v)|})\]</span> 是否归一化？</p><p>归一化可能导致信息丢失，在归一化后可能很难使用学习到的嵌入来区分不同度的节点，并且归一化会掩盖各种其他的图结构特征。</p><p>在通常情况下，在节点特征信息远比结构信息有用或由于节点度范围过于广泛导致优化过程可能不稳定的任务中，归一化最有用。</p><h3 id="集合聚合操作">集合聚合操作</h3><p><strong>集合池化</strong></p><p>一种定义聚合函数的原则是基于置换不变神经网络的理论，具有下式的聚合函数是通用的集合函数逼近器：<span class="math display">\[m_{\mathcal{N}(u)} = MLP_{\theta} (\sum_{v \in \mathcal{N}(u)}MLP_{\phi}(h_v))\]</span> 依照惯例用 <span class="math inline">\(MLP_{\theta}\)</span>表示可训练参数为 <span class="math inline">\(\theta\)</span>的任一深度多层感知器。将一组嵌入映射到一个嵌入的任何置换不变函数都可以基于上式的模型逼近到任意精度。</p><p><strong>Janossy 池化</strong></p><p>不使用置换不变的压缩方法（如求和或取均值），而是采用置换敏感的函数并对多种可能的置换取均值。具体操作为：令<span class="math inline">\(\pi_i \in \Pi\)</span> 表示将集合 <spanclass="math inline">\(\{h_v, \forall v \in \mathcal{N}(u)\}\)</span>映射到特定序列 <span class="math inline">\(((h_{v_1}, h_{v_2}, \cdots,h_{v_{| \mathcal{N}(u) |}})_{\pi_i})\)</span> 的置换函数。即 <spanclass="math inline">\(\pi_i\)</span>将无序的邻居嵌入集置于任意排列的序列中。然后通过 Janossy池化实现邻域聚合，如下式所示：</p><p><span class="math display">\[m_{\mathcal{N}(u)} = MLP_{\theta} (\frac{1}{|\Pi|} \sum_{\pi \in \Pi}\rho_{\phi} (h_{v_1}, h_{v_2}, \cdots,h_{v_{|\mathcal{N}(u)|}})_{\pi_i})\]</span></p><p>其中，<span class="math inline">\(\Pi\)</span> 表示一组置换函数，<span class="math inline">\(\rho_{\phi}\)</span>是置换敏感的函数（如应用于序列数据集的神经网络）。在实践中，通常将 <spanclass="math inline">\(\rho_{\phi}\)</span> 定义为 LSTM。</p><p>如果上式中的置换函数集合 <span class="math inline">\(\Pi\)</span>包含所有可能的置换函数，则上式中的聚合函数也是通用集合函数逼近器。但是对所有可能的置换求和很困难，在实践中通常采用如下两种方法进行Janossy 池化：</p><p>1在每次应用聚合函数时，对所有可能的置换采样出一个随机子集，并且对该随机自己进行求和。</p><p>2对邻域中的节点进行规范化排序，例如，根据节点度对节点进行降序排序，并随机断开一些关联关系。</p><h3 id="邻域注意力模型">邻域注意力模型</h3><p>基本思想是为每个邻域中的节点分配注意力权重，该权重用于在聚合步骤中权衡该节点的影响力。第一个引入注意力机制的GNN模型是图注意力网络GAT，该网络使用注意力权重来定义邻域的加权和，如下式所示： <spanclass="math display">\[m_{\mathcal{N}(u)} = \sum_{v \in \mathcal{N}(u)} \alpha_{u,v}h_v\]</span> 其中，<span class="math inline">\(\alpha_{u,v}\)</span>表示在节点 <span class="math inline">\(u\)</span>处聚合信息时，其邻域中的节点 <span class="math inline">\(v \in\mathcal{N}(u)\)</span> 的注意力权重。GAT 中注意力权重的定义如下式所示：<span class="math display">\[\alpha_{u,v} = \frac{exp([Wh_v \bigoplus Wh_v])}{\sum_{v&#39; \in\mathcal{N}(u)} exp(a^T[Wh_v \bigoplus Wh_{v&#39;}])}\]</span> 其中，<span class="math inline">\(a\)</span>是可训练的注意力向量，<span class="math inline">\(W\)</span>是可训练的矩阵，<span class="math inline">\(\bigoplus\)</span>表示拼接操作。</p><p>注意力机制变体： <span class="math display">\[\alpha_{u,v} = \frac{exp(h_u^TWh_v)}{\sum_{v&#39; \in \mathcal{N}(u)}exp(h_u^TWh_{v&#39;})}\]</span> MLP 注意力层的变体： <span class="math display">\[\alpha_{u,v} = \frac{exp(MLP(h_u,h_v))}{\sum_{v&#39; \in \mathcal{N}(u)}exp(MLP(h_u,h_{v&#39;}))}\]</span> 上式限定 MLP 输出为标量。</p><p>添加多注意力头，使用彼此独立的 <span class="math inline">\(K\)</span>个注意力层计算 <span class="math inline">\(K\)</span> 个不同的注意力权重<span class="math inline">\(\alpha_{u,v,k}\)</span>，然后使用不同的注意力权重聚合的消息会在聚合步骤中进行转换和合并，通常是先进性线性映射，再进行拼接操作，如下式所示：<span class="math display">\[\begin{aligned}m_{\mathcal{N}(u)} &amp;= [a_1 \bigoplus a_2 \bigoplus \cdots \bigoplusa_K] \\\\a_k &amp;= W_i \sum_{v \in \mathcal{N}(u)} \alpha_{u,v,k}h_v\end{aligned}\]</span> 其中，<span class="math inline">\(K\)</span>个注意力头中的每一个注意力权重 <spanclass="math inline">\(\alpha_{u,v,k}\)</span>可以使用上述任何一种注意力机制进行计算。</p><h2 id="广义更新方法">广义更新方法</h2><p><strong>过度平滑和邻域影响</strong></p><p>GNN 的一个常见问题是过度平滑。过度平滑的基本原理是：经过多次 GNN消息传递后，图中所有节点的表示可能变得非常相似。过度平滑导致无法建立更深的GNN 模型以利用图上的长期依赖关系，因为这些深层的 GNN模型往往会生成过度平滑的嵌入。</p><p>可以通过定义每个节点的输入特征 <spanclass="math inline">\(h_u^{(0)}=x_u\)</span>对图上其它节点的最终层输出的嵌入（<spanclass="math inline">\(h_v^{(K)},\forall v \in\mathcal{V}\)</span>）的影响来形式化定义 GNN中的过度平滑问题。对于任意一对节点 <spanclass="math inline">\(u\)</span> 和 <spanclass="math inline">\(v\)</span>，可以通过检查相应的雅可比矩阵的大小来量化 GNN 中节点 <spanclass="math inline">\(u\)</span> 对节点 <spanclass="math inline">\(v\)</span> 的影响，如下式所示： <spanclass="math display">\[I_{K(u,v)} = 1^T(\frac{\partial h_v^{(K)}}{\partial h_u^{(0)}})1\]</span> 其中，<span class="math inline">\(1\)</span> 是元素全为 1的向量。<span class="math inline">\(I_{K(u,v)}\)</span> 是雅可比矩阵<span class="math inline">\(\frac{\partial h_v^{(K)}}{\partialh_u^{(0)}}\)</span> 中的元素之和。用来衡量 GNN 中节点 <spanclass="math inline">\(u\)</span> 的初始嵌入对节点 <spanclass="math inline">\(v\)</span> 的最终嵌入的影响程度。</p><p>定理：对于任何使用自环更新方法和用下式表示聚合函数的 GNN 模型 <spanclass="math display">\[AGGRGATE(\{h_v, \forall v \in \mathcal{N}(u) \bigcup \{u\}\}) =\frac{1}{f_n(|\mathcal{N}(u) \bigcup \{u\}|)} \sum_{v \in \mathcal{N}(u)\bigcup \{u\}} h_v\]</span> 其中，<span class="math inline">\(f:\mathbb{R}^+ \rightarrow\mathbb{R}^+\)</span> 是任意可微的归一化函数。</p><p>可得出下式结论： <span class="math display">\[I_K (u,v) \propto p_{\mathcal{G},K}(u|v)\]</span> 其中，<spanclass="math inline">\(p_{\mathcal{G},K}(u|v)\)</span> 表示从节点 <spanclass="math inline">\(u\)</span> 开始的 <spanclass="math inline">\(K\)</span> 步随机游走过程中访问节点 <spanclass="math inline">\(v\)</span> 的概率。</p><p>当使用 <span class="math inline">\(K\)</span> 层 GCN 型魔性时，节点<span class="math inline">\(u\)</span> 对节点 <spanclass="math inline">\(v\)</span> 的影响从节点 <spanclass="math inline">\(u\)</span> 开始经过 <spanclass="math inline">\(K\)</span> 步随机游走到达节点 <spanclass="math inline">\(v\)</span> 的概率成正比。但是，随着 <spanclass="math inline">\(K \rightarrow\infty\)</span>，每个节点的影响都接近图上随机游走的平稳分布，这意味着本地邻域信息会丢失。</p><p>上述定理直接适用于使用自环更新方法的模型，但是只要任意层 <spanclass="math inline">\(k\)</span> 满足 <span class="math inline">\(\|W_{self}^{(k)} \| &lt; \| W_{neigh}^{(k)}\|\)</span>，其结果也可以渐近地扩展基本 GNN 的更新。因此当使用简单 GNN模型时，构建更深的模型实际上会损害模型性能。随着更多层的加入，模型将丢失更多关于本地邻域结构的信息，并且学习的嵌入会变得过于平滑，接近几乎均匀的分布。</p><h3 id="拼接和跳跃连接">拼接和跳跃连接</h3><p>最简单更新跳跃连接的方式之一是使用拼接操作再消息传递期间保留更多节点级别信息，如下式所示：<span class="math display">\[UPDATE_{concat}(h_u, m_{\mathcal{N}(u)}) =[UPDATE_{base}(h_u,m_{\mathcal{N}(u)}) \bigoplus h_u]\]</span>其中，直接将基本更新函数的输出与节点的上一层表示拼接，鼓励模型再消息传递过程中解耦信息，将来自邻域的信息（<spanclass="math inline">\(m_{\mathcal{N}(u)}\)</span>）与每个节点当前的表示（<spanclass="math inline">\(h_u\)</span>）分开。</p><p>线性插值法跳跃连接，如下式所示： <span class="math display">\[UPDATE_{interpolate}(h_u,m_{\mathcal{N}(u)}) = \alpha_1 \circUPDATE_{base}(h_u,m_{\mathcal{N}(u)}) + \alpha_2 \bigodot h_u\]</span> 其中，<span class="math inline">\(\alpha_1,\alpha_2 \in[0,1]^d\)</span> 是满足 <span class="math inline">\(\alpha_2 = 1 -\alpha_1\)</span> 的门控向量，<span class="math inline">\(\circ\)</span>表示逐元素相乘。</p><p>最终更新的表示是先前表示与基于邻域信息进行更新的表示之间的线性插值。</p><p>在通常情况下，拼接和跳跃连接有助于缓解 GNN中过渡平滑问题，同时可以提高优化数值的稳定性。</p><h3 id="门控更新函数">门控更新函数</h3><p>一种解读 GNN消息传递算法的观点是：聚合函数从邻域接收观察结果，然后将其用于更新每个节点的隐状态。基于这一观点可以根据观察结果直接使用更新RNN 框架的隐状态的方法，最早的 GNN 架构之一定义更新函数如下式所示：<span class="math display">\[h_u^{(k)} = GRU(h_u^{(k-1)},m_{\mathcal{N}(u)}^{(k)})\]</span> 其中，GRU 表示 GRU 单元的更新函数。</p><p>门控更新方法在提高 GNN框架的模型深度（超过10层）和防止过度平滑问题方面非常有效。</p><h3 id="跳跃知识连接">跳跃知识连接</h3><p>提高最终的节点表示质量的一种补充策略是利用消息传递的每一层输出的表示，叫做加入跳跃知识，如下式所示：<span class="math display">\[z_u = f_{JK}(h_u^{(0)} \bigoplus h_u^{(1)} \bigoplus \cdots \bigoplush_u^{(K)})\]</span> 其中，<span class="math inline">\(f_{JK}\)</span>是任意微分函数。</p><h2 id="边特征和多元关系-gnn">边特征和多元关系 GNN</h2><p>下面介绍 GNN 在多元关系图或其它异构图中的应用。</p><h3 id="关系-gnn">关系 GNN</h3><p>关系图卷积网络（RGCN），通过为每种关系类型指定一个单独的变化矩阵来增强聚合函数处理多种关系的能力，如下式所示：<span class="math display">\[m_{\mathcal{N}(u)} = \sum_{\tau \in \mathcal{R}} \sum_{v \in\mathcal{N}_{\tau}(u)} \frac{W_{\tau}h_v}{f_n(\mathcal{N}(u),\mathcal{N}(v))}\]</span> 其中， <span class="math inline">\(f_n\)</span>是一个归一化函数，它的值取决于节点 <spanclass="math inline">\(u\)</span> 的邻域以及被聚合的节点 <spanclass="math inline">\(v\)</span> 的邻域。RGCN中的多元关系聚合类似具有归一化函数的基本GNN，但根据边的类型不同分别聚合信息。</p><p><strong>参数共享</strong></p><p>朴素 RGCN方法的一个缺点是由于每一种关系类型都需要一个可训练的矩阵导致参数量急剧增加，这种参数量的激增可能导致过拟合和训练缓慢的问题。</p><p>通过与基矩阵共享参数的方法来解决此问题，如下式： <spanclass="math display">\[W_{\tau} = \sum_{i=1}^b \alpha_{i,\tau}B_i\]</span> 该方法中，所有关系矩阵都定义为 <spanclass="math inline">\(b\)</span> 个基矩阵（<spanclass="math inline">\(B1,\cdots,B_b\)</span>）的线性组合；唯一的关于关系的参数是每种关系<span class="math inline">\(\tau\)</span> 的 <spanclass="math inline">\(b\)</span> 个组合权重 <spanclass="math inline">\(\alpha_{1,\tau}, \cdots,\alpha_{b,\tau}\)</span>。在这种基本共享方法中，看可以将完整聚合函数表示为下式：<span class="math display">\[m_{\mathcal{N}(u)} = \sum_{\tau \in \mathcal{R}} \sum_{v \in\mathcal{N}_{\tau}(u)} \frac{\alpha_{\tau} \times_{1} B \times_{2}h_v}{f_n(\mathcal{N}(u), \mathcal{N}(v))}\]</span> 其中，<span class="math inline">\(B = (B_1, \cdots,B_b)\)</span> 是一个由基矩阵堆叠构成的张量， <spanclass="math inline">\(\alpha_{\tau} = \alpha_{1,\tau}, \cdots,\alpha_{b,\tau}\)</span> 是一个关于关系 <spanclass="math inline">\(\tau\)</span> 的包含基矩阵组合权重的向量，<spanclass="math inline">\(\times_{i}\)</span> 表示沿着模 <spanclass="math inline">\(i\)</span> 的张量积。另一种理解参数共享 RGCN方法的过程是：学习每个关系的嵌入及所有关系之间共享的张量。</p><h3 id="注意力机制和特征拼接">注意力机制和特征拼接</h3><p>为适应更一般形式的边特征的情况，可以在消息传递过程中基于注意力机制或将这些信息与邻域嵌入拼接来充分利用这些特征。在给定任意基本聚合方法<span class="math inline">\(AGGREGATE_{base}\)</span>的情况下，利用边特征的一种简单策略是如下式定义新的聚合函数： <spanclass="math display">\[m_{\mathcal{N}(u)} = AGGREGATE_{base}(\{h_v \bigoplus e_{(u,\tau,v)},\forall v \in \mathcal{N}(u) \})\]</span> 其中，<span class="math inline">\(e_{(u,\tau,v)}\)</span>表示边 <span class="math inline">\((u,\tau,v)\)</span> 的特征。</p><h2 id="图池化">图池化</h2><p><strong>集合池化方法</strong></p><p>与 AGGREGATE操作类似，图池化任务可以看作是解决集合上的问题。要设计一个池化函数 <spanclass="math inline">\(f_p\)</span> 将一组节点嵌入 <spanclass="math inline">\(\{ z_1, \cdots, z_{|V|} \}\)</span>映射为表示整张图的嵌入 <spanclass="math inline">\(z_{\mathcal{G}}\)</span>。</p><p>第一种常用方法是对节点嵌入求和（或取均值），如下式所示： <spanclass="math display">\[z_{\mathcal{G}} = \frac{\sum_{v \in \mathcal{V}}z_c}{f_n(|\mathcal{V}|)}\]</span> 其中，<span class="math inline">\(f_n\)</span>是归一化函数（如恒等函数）。适用于小规模图。</p><p>第二种常用方法基于集合的方法，结合了 LSTM和注意力机制来池化节点嵌入。这种池化方法迭代 <spanclass="math inline">\(t=1,\cdots,T\)</span>步基于注意力机制的聚合操作，如下式所示： <span class="math display">\[\begin{aligned}q_i &amp;= LSTM(o_{t-1}, q_{t-1}) \\\\e_{v,t} &amp;= f_a(z_v,q_t),\forall v \in \mathcal{V} \\\\a_{v,t} &amp;= \frac{exp(e_{v,i})}{\sum_{u \in \mathcal{V}} e_{u,t}},\forall v \in \mathcal{V} \\\\o_t &amp;= \sum_{v \in \mathcal{V}} a_{v,t} z_v\end{aligned}\]</span></p><p>其中，<span class="math inline">\(q_t\)</span> 表示每次迭代 <spanclass="math inline">\(t\)</span>次注意力机制中的查询向量。查询向量用于使用注意力函数 <spanclass="math inline">\(f_a: \mathbb{R}^d \times \mathbb{R} \rightarrow\mathbb{R}\)</span>（如点积）为每个节点计算注意力分数，然后将该注意力分数进行归一化，最后根据注意力权重计算节点嵌入的加权和，并基于该加权和采用LSTM 更新来更新查询向量。通常情况下，用全零向量初始化 <spanclass="math inline">\(q_0\)</span> 和 <spanclass="math inline">\(o_0\)</span> ，进行了 <spanclass="math inline">\(T\)</span> 次迭代后计算整张图的嵌入如下式所示：<span class="math display">\[z_{\mathcal{G}} = o_1 \bigoplus o_2 \bigoplus \cdots \bigoplus o_T\]</span> <strong>图粗糙化方法</strong></p><p>集合池化方法的局限性在于不能利用图的结构信息。在池化阶段利用图的拓扑信息可以进一步提供增益，实现次目的的一种流形策略是用图聚类或粗糙化作为池化节点表示的一种方法。</p><p>假设有聚类函数如下式所示： <span class="math display">\[f_c \rightarrow \mathcal{G} \times \mathbb{R}^{|V| \times d} \rightarrow\mathbb{R}^{+|V| \times c}\]</span> 聚类函数将图上的所有节点分为 <spanclass="math inline">\(c\)</span> 个簇。假定该函数输出一个分配矩阵 <spanclass="math inline">\(S = f_c(\mathcal{G}, Z)\)</span>，其中，<spanclass="math inline">\(S[u,i] \in \mathbb{R}\)</span> 表示节点 <spanclass="math inline">\(u\)</span> 和簇 <spanclass="math inline">\(i\)</span> 之间的关联强度。</p><p>图粗糙化方法的关键思想是使用聚类分配矩阵来粗糙化图。这里使用分配矩阵<span class="math inline">\(S\)</span>来计算新的粗糙化邻接矩阵和一个新的节点特征集合，如下式所示： <spanclass="math display">\[\begin{aligned}A^{new} &amp;= S^TAS \in \mathbb{R}^{+c \times c} \\\\X^{new} &amp;= S^TX \in \mathbb{R}^{c \times d}\end{aligned}\]</span>这一新的邻接矩阵表示图中的簇之间的关联强度（边），而新的特征矩阵表示聚合分配给每个簇的所有节点嵌入的结果。在该粗糙化的图上运行GNN，并在每次迭代的过程中重复粗糙化过程，图在每一步都会减小，最后在足够粗糙化的图上对节点嵌入执行集合池化可以获得图的最终表示。</p><h2 id="通用消息传递方法">通用消息传递方法</h2><p>GNN消息传递方法可以泛化为在消息传递的每个阶段利用边和图级别的信息。</p><p>更为通用的消息传递方法如下式： <span class="math display">\[\begin{aligned}h_{(u,v)}^{(k)} &amp;= UPDATE_{edge}(h_{(u,v)}^{(k-1)}, h_u^{(k-1)},h_v^{(k-1)}, h_{\mathcal{G}}^{(k-1)}) \\\\m_{\mathcal{N}(u)} &amp;= AGGREGATE_{node}(\{ h_{(u,v)}^{(k-1)}, \forallv \in \mathcal{N}(u) \}) \\\\h_u^{(k)} &amp;= UPDATE_{node}(h_u^{(k-1)}, m_{\mathcal{N}(u)},h_{\mathcal{G}}^{(k-1)}) \\\\h_{\mathcal{G}}^{(k)} &amp;= UPDATE_{graph}(h_{\mathcal{G}}^{(k-1)}, \{h_u^{(k-1)}, \forall u \in \mathcal{V}, \{ h_{(u,v)}^{(k)}, \forall(u,v) \in \varepsilon \} \})\end{aligned}\]</span> 通用消息传递框架中，在消息传递过程中为图上的每条边生成嵌入<spanclass="math inline">\(h_(u,v)^{(k)}\)</span>，并为整张图生成相应的嵌入<spanclass="math inline">\(h_{\mathcal{G}}^{(k)}\)</span>，这使得消息传递模型可以聚合边和图级别的特征。</p><p>在通用消息传递框架中进行消息传递时，首先根据便关联的节点的嵌入来更新边的嵌入。接下来，通过聚合节点关联所有边的嵌入来更新节点嵌入。图嵌入被用于节点和边表示的更新函数中，并且图级别的嵌入本身通过在每次迭代结束时对所有节点和边的嵌入进行聚合来更新。</p><h2 id="损失函数">损失函数</h2><p><strong>用于节点分类的 GNN</strong></p><p>以完全监督方式训练 GNN，使用 softmax分类函数和负对数似然损失来定义损失函数，如下式：</p><p><span class="math display">\[\mathcal{L} = \sum_{u \in \mathcal{V}_{train}} = -log(softmax(z_u,y_u))\]</span></p><p>其中，假设 <span class="math inline">\(y_u \in \mathbb{Z}^c\)</span>是一个独热向量，表示用于训练的节点 <span class="math inline">\(u \in\mathcal{V}_{train}\)</span> 的类。</p><p>在引用网络中，<span class="math inline">\(y_u\)</span>表示论文的主题，<span class="math inline">\(Softmax(z_u,y_u)\)</span>表示通过 softmax 函数计算节点属于类 <spanclass="math inline">\(y_u\)</span> 的概率，如下式：</p><p><span class="math display">\[softmax(z_u,y_u) = \sum_{i=1}^c y_u[i]\frac{e^{z_u^Tw_i}}{\sum_{j=1}^ce^{z_u^Tw_j}}\]</span></p><p>其中，<span class="math inline">\(w_i \in\mathbb{R}^d,i=1,\cdots,c\)</span> 是可训练的参数。</p><p><strong>用于图分类的 GNN</strong></p><p>图分类的损失函数值是通过一组有标记的训练图 <spanclass="math inline">\(\mathcal{T} ={\mathcal{G}_1,\cdots,\mathcal{G}_n}\)</span> 上学习的图嵌入 <spanclass="math inline">\(z_{\mathcal{G}_i}\)</span>计算，通常使用如下式定义的平方误差损失函数：</p><p><span class="math display">\[\mathcal{L} = \sum_{\mathcal{G} \in \mathcal{T}}\|MLP(z_{\mathcal{G}_i}) - y_{\mathcal{G}_i} \|_2^2\]</span></p><p>其中，MLP 是具有单一变量输出的密集连接的神经网络，<spanclass="math inline">\(y_{\mathcal{G}_i} \in \mathbb{R}\)</span> 是训练图<span class="math inline">\(\mathcal{G}_i\)</span> 的标签值。</p><p><strong>用于关系预测的 GNN</strong></p><p>深度图信息最大化（DGI）节点嵌入 <spanclass="math inline">\(z_u\)</span> 和图嵌入 <spanclass="math inline">\(z_{\mathcal{G}}\)</span>之间的互信息，损失函数如下式：</p><p><span class="math display">\[\mathcal{L} = -\sum_{u \in \mathcal{V}_{train}} \mathbb{E}_{\mathcal{G}}log(D(z_u,z_{\mathcal{G}})) +\gamma\mathbb{E}log(1-D(\tilde{z}_u,z_{\mathcal{G}}))\]</span></p><p>其中，<span class="math inline">\(z_u\)</span> 表示 GNN 根据图 <spanclass="math inline">\(\mathcal{G}\)</span> 生成的节点 <spanclass="math inline">\(u\)</span> 的嵌入，而 <spanclass="math inline">\(\tilde{z}_u\)</span> 表示根据 <spanclass="math inline">\(\mathcal{G}\)</span> 的损坏版本 <spanclass="math inline">\(\tilde{\mathcal{G}}\)</span> 生成的节点 <spanclass="math inline">\(u\)</span> 的嵌入。这里用 <spanclass="math inline">\(D\)</span>表示判别函数，它是一个被训练用以预测节点嵌入是基于真实的图 <spanclass="math inline">\(\mathcal{G}\)</span> 还是损坏版本的图 <spanclass="math inline">\(\tilde{\mathcal{G}}\)</span>生成的。在通常情况下，通过一种随机的方式（如打乱特征矩阵中的元素）修改节点特征或邻接矩阵，抑或是同时修改两者来破坏图。上述损失函数背后的思想是：GNN模型必须学会生成可以区分真是图和其损坏版本的节点嵌入。这一优化目标与最大化节点嵌入<span class="math inline">\(z_u\)</span> 和图嵌入 <spanclass="math inline">\(z_{\mathcal{G}}\)</span>之间的互信息密切相关。</p><h2 id="节点采样">节点采样</h2><p>基于节点级别消息传递的角度直接实现 GNN可能存在计算效率低的问题。例如，当多个节点共享邻域时，如果图中所有节点独立执行消息传递操作，最终可能会执行大量冗余计算。</p><p><strong>图级别的实现方法</strong></p><p><strong>子采样和小批量</strong></p><p>为了限制 GNN的内存占用并促进小批量训练方式，可以在消息传递过程中使用节点集的子集。从数学角度，可以认为这是在每个批次中为图中节点的子集运行节点级的GNN 公式。</p><p>挑战在于不能在不丢失信息的情况下简单地在图上的一部分节点中执行消息传递操作，每次删除节点时也会删除其关联的边，这无法保证选择的节点的随机子集会构成连接图，并且为每个小批次选择一个节点的随机自己会对模型性能产生严重的不利影响。</p><p>通过对节点邻域进行子采样的策略来克服此问题：首先为每个批次选择一组目标节点，然后递归采样这些节点的邻域以确保能够保持图的连通性。为了尽可能避免为一个批次采样过多节点的情况，建议使用固定容量对每个节点的邻域进行子采样以提高批量张量操作的效率。</p><h2 id="参数共享和正则化">参数共享和正则化</h2><p><strong>层间参数共享</strong></p><p>在 GNN 的所有聚合和更新函数中使用相同的函数。通常情况下，在6层以上的GNN 中最有效，并且通常与门控更新函数结合使用。</p><p><strong>边丢弃</strong></p><p>训练过程中随机删除（或屏蔽）邻接矩阵的边。直观来看，这将使 GNN不太容易过拟合，并且对邻接矩阵中的噪声更具鲁棒性。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python程序运行时间查看方法</title>
    <link href="/2023/05/30/python-cheng-xu-yun-xing-shi-jian-cha-kan-fang-fa/"/>
    <url>/2023/05/30/python-cheng-xu-yun-xing-shi-jian-cha-kan-fang-fa/</url>
    
    <content type="html"><![CDATA[<p>time包查看程序运行时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><br>start_time = time.time()  <span class="hljs-comment"># 记录程序开始运行时间</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000000</span>):<br>    <span class="hljs-keyword">pass</span><br>end_time = time.time()  <span class="hljs-comment"># 记录程序结束运行时间</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;cost %f s&#x27;</span> % (end_time - start_time))<br></code></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>cost <span class="hljs-number">0.179781</span> s<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（三）节点嵌入</title>
    <link href="/2023/05/29/tu-shen-jing-wang-luo-xue-xi-ri-ji-san-jie-dian-qian-ru/"/>
    <url>/2023/05/29/tu-shen-jing-wang-luo-xue-xi-ri-ji-san-jie-dian-qian-ru/</url>
    
    <content type="html"><![CDATA[<p>节点嵌入目的是将节点编码为包含图位置和局部邻域结构信息的低维向量，其几何关系与原始图或网络中的关系相对应。</p><h2 id="简单加权图节点嵌入">简单加权图节点嵌入</h2><h3 id="基于编码-解码框架">基于编码-解码框架</h3><p>编码器将图中每个节点映射为低维向量或低维嵌入，解码器将利用低维节点嵌入重构原始图中每一个节点的邻域信息。</p><p><strong>编码器</strong></p><p>编码器将节点 <span class="math inline">\(v \in \mathcal{V}\)</span>映射为向量嵌入 <span class="math inline">\(z_v \in \mathbb{R}^d\)</span>，其中，<span class="math inline">\(z_v\)</span> 为节点 <spanclass="math inline">\(v \in \mathcal{V}\)</span>对应的嵌入表示，可表示为下式： <span class="math display">\[ENC: \mathcal{V} \rightarrow \mathbb{R}^d\]</span> 编码器依赖 shallow embedding 方法，即根据节点的 ID进行简单地嵌入查找，如下式： <span class="math display">\[ENC(v) = Z[v]\]</span> 其中，<span class="math inline">\(Z \in\mathbb{R}^{|\mathcal{V}| \times d}\)</span>为包含所有节点嵌入向量的矩阵，<span class="math inline">\(Z[v]\)</span>表示节点 <span class="math inline">\(v\)</span> 对应 <spanclass="math inline">\(Z\)</span> 的某一行向量。</p><p><strong>解码器</strong></p><p>解码器的任务是根据解码器生成的节点嵌入重构某些确定的图形统计信息。</p><p>标准形式为成对的解码器，如下式： <span class="math display">\[DEC:\mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}^+\]</span> 成对的解码器可以预测成对出现的节点的关系和相似性。</p><p>在一对节点的嵌入表示 <span class="math inline">\((z_u,z_v)\)</span>中使用成对解码器，能重构节点 <span class="math inline">\(u\)</span> 和<span class="math inline">\(v\)</span>之间的关系。重构的目标在于通过最小化重构损失来优化编码器和解码器，如下式：<span class="math display">\[DEC(ENC(u),ENC(v)) = DEC(z_u,z_v) \approx S[u,v]\]</span> 其中，假定 <span class="math inline">\(S[u,v]\)</span>是基于图的节点间相似性的度量。</p><p><strong>损失函数</strong></p><p>实现重构目标函数的标准操作是在一组训练节点对 <spanclass="math inline">\(\mathcal{D}\)</span> 上最小化经验重构损失 <spanclass="math inline">\(\mathcal{L}\)</span> ，如下式： <spanclass="math display">\[\mathcal{L} = \sum_{(u,v)\in \mathcal{D}}\mathcal{l}(DEC(z_u,z_v),S[u,v])\]</span> 其中，<span class="math inline">\(\mathcal{l}: \mathbb{R}\times \mathbb{R}\)</span> 是衡量解码后的近似值 <spanclass="math inline">\(DEC(z_u,z_v)\)</span> 与真值 <spanclass="math inline">\(S[u,v]\)</span> 之间差异的函数。在训练集 <spanclass="math inline">\(\mathcal{D}\)</span>上，全过程的目标就是通过训练编码器和解码器，有效地重构成对节点的关系信息。</p><h3 id="基于因式分解的方法">基于因式分解的方法</h3><p>节点嵌入表示来解码局部邻居结构与在图的邻接矩阵中重构邻居密切相关，邻接矩阵中重构邻居是使用矩阵分解来学习节点-节点相似性矩阵<span class="math inline">\(S\)</span> 的低维表示，其中， <spanclass="math inline">\(S\)</span>概括了邻接矩阵并且能描述用户定义的节点-节点的相似性概念。</p><p><strong>拉普拉斯特征映射</strong></p><p>使用基于节点嵌入之间的 <span class="math inline">\(L_2\)</span>距离定义解码器，如下式： <span class="math display">\[DEC(z_u,z_v) = ||z_u - z_v||_2^2\]</span>根据节点在图中的相似性对节点对进行加权，以得到最终的损失函数，如下式：<span class="math display">\[\mathcal{L} = \sum_{(u,v) \in \mathcal{D}} DEC(z_u,z_v) \cdot S[u,v]\]</span> <strong>内积法</strong></p><p>基于内积的解码器，如下式： <span class="math display">\[DEC(z_u,z_v) = z_u^Tz_v\]</span>假设两个节点之间的相似度（如这两个节点局部邻域之间重叠的部分）与节点嵌入表示的点积成正比。</p><p>损失函数如下式： <span class="math display">\[\mathcal{L} = \sum_{(u,v) \in \mathcal{D}} ||DEC(Z_u,z_v)-S[u,v]||_2^2\]</span> 将节点嵌入表示 <span class="math inline">\(z_u \in\mathbb{R}^d\)</span> 堆叠为矩阵 <span class="math inline">\(Z \in\mathbb{R}^{|\mathcal{V}| \times d}\)</span>，则可以将目标函数写成下式： <span class="math display">\[\mathcal{L} = ||{ZZ^T}_2^2||\]</span></p><h3 id="随机游走嵌入表示">随机游走嵌入表示</h3><p>随机游走嵌入表示是内积法应用于邻域重构的随机度量计算的方法。</p><p><strong>Deepwalk 和 node2vec</strong></p><p>Deepwalk 和 node2vec 使用了 shallow embedding方法和内积解码器，巧妙的定义相似度和邻域重构的概念，通过优化嵌入表示对随机游走的统计信息进行编码。通过学习节点嵌入使得下式成立：<span class="math display">\[DEC(z_u,z_v) \overset{d}{=} \frac{e^{z_u^Tz_v}}{\sum_{v_k \in\mathcal{V}}e^{z_u^Tz_k}} \approx p_{\mathcal{g},T}(v|u)\]</span> 其中，<spanclass="math inline">\(p_{\mathcal{g},T}(v|u)\)</span> 是指从节点 <spanclass="math inline">\(u\)</span> 出发，随机游走 <spanclass="math inline">\(T\)</span> 步后访问节点 <spanclass="math inline">\(v\)</span> 的概率，<span class="math inline">\(T\in {2,\cdots}, 10\)</span>。</p><p>通过上式的解码器使用下式训练随机游走嵌入： <spanclass="math display">\[\mathcal{L} = \sum_{(u,v) \in \mathcal{D}} -log(DEC(z_u,z_v))\]</span>通过从每一个节点开始进行随机游走采样，可以生成随机游走的训练集，并用<span class="math inline">\(\mathcal{D}\)</span> 表示。</p><p>上式损失函数时间复杂度为 <spanclass="math inline">\(O(|\mathcal{D}||\mathcal{V}|)\)</span> 。Deepwalk使用层次的 softmax函数近似表示解码器，主要通过利用二叉树结构来加速计算，node2vec使用噪声对比方法近似表示损失函数，使用下式所示的负样本近似表示归一化引子：<span class="math display">\[\mathcal{L} = \sum_{(u,v)\in \mathcal{D}} -log(\sigma(z_u^T,z_v)) -\gamma \mathbb{E}_{v_n \simP_n(\mathcal{V})}[log(-\sigma(z_u^T,z_{v_n}))]\]</span> 其中，<span class="math inline">\(\sigma\)</span> 表示 log激活函数， <span class="math inline">\(P_n(\mathcal{V})\)</span>表示节点 <span class="math inline">\(\mathcal{V}\)</span> 的分布，<spanclass="math inline">\(\gamma &gt; 0\)</span> 为超参数。<spanclass="math inline">\(P_n(\mathcal{V})\)</span>往往服从均匀分布，且期望值可采用蒙特卡洛算法近似计算。</p><p>Deepwalk 只简单使用均匀的随机游走来定义分布 <spanclass="math inline">\(p_{\mathcal{g},T}(v|u)\)</span>，而 node2vec方法引入了超参数，这些超参数允许随机游走在于图的广度优先搜索或深度优先搜索更相似的游走的概率之间平滑地插值。</p><p><strong>随机游走与矩阵分解方法</strong></p><p>假设定义下式为节点-节点相似度矩阵的值： <span class="math display">\[S_{DW} = log(\frac{vol(\mathcal{V})}{T}(\sum_{t=1}^TP^t)D^{-1})-log(b)\]</span> 其中 <span class="math inline">\(b\)</span> 为常值，<spanclass="math inline">\(P=D^{-1}A\)</span> 。</p><p>还可以将上式内部的部分表示分解成下式所示形式： <spanclass="math display">\[(\sum_{t=1}^TP^T)D^{-1} =D^{-\frac{1}{2}}(U(\sum_{t=1}^T\Lambda^t)U^T)D^{-\frac{1}{2}}\]</span> 其中，<span class="math inline">\(U\LambdaU^T=L_{sym}\)</span> 是对称归一化拉普拉斯算子的本征分解。</p><p>裘捷中等提出通过 DeepWalk 学习到的嵌入 <spanclass="math inline">\(Z\)</span> 需满足下式要求： <spanclass="math display">\[Z^TZ \approx S_{DW}\]</span> <strong>shallow embedding 的局限性</strong></p><ul><li><p>shallow embedding方法不会在编码器的节点之间共享任何参数，因为编码器会直接为每个节点优化唯一的嵌入向量，不进行参数共享会导致算法在统计和计算过程中效率低下。</p></li><li><p>shallow embedding 方法没有利用编码器中的节点特征。</p></li><li><p>shallow embedding方法是转导性（Transductive）的，只能为训练过程中存在的节点生成嵌入，无法为训练之后出现的新节点生成嵌入。</p></li></ul><h2 id="多关系图节点嵌入">多关系图节点嵌入</h2><p>设多关系图为 <span class="math inline">\(\mathcal{G} = (\mathcal{V},\varepsilon)\)</span>，其中边被定义为元组 <spanclass="math inline">\(e=(u,\tau,v)\)</span> 中两个节点间存在的特定关系<span class="math inline">\(\tau \in \mathcal{T}\)</span>。</p><p>可以将嵌入多关系图视作重构任务，即给定两个节点的嵌入表示 <spanclass="math inline">\(z_u\)</span> 和 <spanclass="math inline">\(z_v\)</span>，重构这些节点之间的关系。将解码器定义为可同时输入一对节点嵌入及一个关系类型（<spanclass="math inline">\(DEC: \mathbb{R}^d \times \mathcal{R} \times\mathbb{R}^d \rightarrow \mathbb{R}+\)</span>），将解码器的输出（<spanclass="math inline">\(DEC(z_u,\tau,z_v)\)</span>）定义为边 <spanclass="math inline">\((u,\tau,v)\)</span> 存在于图谱的可能性。</p><p>本节所有方法都是假定从低维向量中直接重构（多关系）邻居。</p><h3 id="损失函数">损失函数</h3><p><strong>负采样的交叉熵函数</strong> <span class="math display">\[\mathcal{L} = \sum_{(u,\tau,v) \in \varepsilon}-log(\sigma(DEC(z_u,\tau,z_v))) - \gamma\mathbb{E}_{v_n \simP_{n,u}(\mathcal{V})}[log(\sigma(-DEC(z_u,\tau,z_{v_n})))])\]</span> 其中，<span class="math inline">\(\sigma\)</span>为对数函数，<span class="math inline">\(P_{n,u}(\mathcal{V})\)</span>为节点集 <span class="math inline">\(\mathcal{V}\)</span>的负采样分布，且超参数 <span class="math inline">\(\gamma &gt;0\)</span> 。</p><p>实际存在于图谱中的、预测为 true 的边的对数似然如下式： <spanclass="math display">\[log(\sigma(DEC(z_u,\tau,z_v)))\]</span> 图中不存在的、预测为 false 的边的期望对数似然如下式： <spanclass="math display">\[\mathbb{E}_{v_n \simP_{n,u}(\mathcal{V})}[log(\sigma(-DEC(z_u,\tau,z_{v_n})))]\]</span> 使用蒙特卡洛近似法可以对期望值进行评估，常见损失函数如下式：<span class="math display">\[\mathcal{L} = \sum_{(u,\tau,v)\in \varepsilon}-log(\sigma(DEC(z_u,\tau,z_v))) - \sum_{v_n \in \mathcal{P}_{n,u}}-[log(\sigma(-DEC(z_u,\tau,z_{v_n})))]\]</span> 其中，<span class="math inline">\(\mathcal{P}_{n,u}\)</span>为从 <span class="math inline">\(P_{n,u}(\mathcal{V})\)</span>采样得到的较小节点集。</p><p><strong>最大间距损失</strong> <span class="math display">\[\mathcal{L} = \sum_{(u,\tau,v)\in \varepsilon} \sum_{v_n \in\mathcal{P}_{n,u}} max(0, -DEC(z_u,\tau,z_v) + DEC(z_u,\tau,z_{v_n}) +\Delta)\]</span>该损失函数采用对比估计策略，将真实节点解码后的得分与负样本进行对比。其中，<spanclass="math inline">\(\Delta\)</span>项为间隔项，如果所有样本的得分差距都很大，那损失将为 0，该损失函数称为hinge loss。</p><h3 id="多关系解码器">多关系解码器</h3><p><strong>RESCAL 方法</strong></p><p>解码器定义为如下式： <span class="math display">\[DEC(u,\tau,v) = z_u^TR_{\tau}z_v\]</span> 其中，<span class="math inline">\(R_{\tau} \in \mathbb{E}^{d\times d}\)</span> 是基于特定关系 <span class="math inline">\(\tau \in\mathcal{R}\)</span> 的可学习矩阵。可借助基本的重构损失函数训练嵌入矩阵<span class="math inline">\(Z\)</span> 和关系矩阵 <spanclass="math inline">\(R_{\tau}, \forall \tau \in\mathcal{R}\)</span>，如下式： <span class="math display">\[\begin{aligned}  \mathcal{L} &amp;= \sum_{u\in \mathcal{V}} \sum_{v\in \mathcal{V}}\sum_{\tau\in \mathcal{R}} ||DEC(u,\tau,v)-\mathcal{A}[u,\tau,v||^2 \\  &amp;= \sum_{u\in \mathcal{V}} \sum_{v\in \mathcal{V}} \sum_{\tau\in\mathcal{R}} ||z_u^T\mathcal{R}_{\tau},z_v - \mathcal{A}[u,\tau,v]||\end{aligned}\]</span> 其中，<span class="math inline">\(\mathcal{A} \in\mathbb{R}^{|\mathcal{V}| \times |\mathcal{R}| \times|\mathcal{V}|}\)</span> 为多关系图的邻接张量。</p><p>该方法进行关系表示时需要较高的计算量和统计成本，对于每种类型的关系，RESCAL模型具有 <span class="math inline">\(O(d^2)\)</span>的参数量，与节点表示相比，关系表示要求具有更大数量级的参数。</p><p><strong>平移解码模型（Translational Decoders）</strong></p><p>TransE 模型将关系作为嵌入空间中的平移向量，定义的解码器如下图所示：<span class="math display">\[DEC(z_u,\tau,z_v) = -|| z_u + r_{\tau} +z_v ||\]</span> 其中，使用 <span class="math inline">\(d\)</span>维嵌入向量来表示每种关系。在嵌入空间中，根据关系嵌入对头节点进行平移，边存在的可能性与头节点和尾节点嵌入间的距离成比例。</p><p>改进模型 1 <span class="math display">\[DEC(z_u,\tau,z_v) = -|| g_{1,\tau}(z_u) + r_{\tau} - g_{2,\tau}(z_v)||\]</span> 其中，<span class="math inline">\(g_{i,\tau}\)</span>为一种基于关系 <span class="math inline">\(\tau\)</span>空间的可训练转换方式。</p><p>改进模型 2 TransH <span class="math display">\[DEC(z_u,\tau,z_v) = -|| (z_u - w_r^Tz_uw_r) + r_{\tau} - (z_u -w_r^Tz_vw_r) ||\]</span> TransH模型可将实体嵌入映射到一个可学习的特定关系超平面上（在执行转换之前，由法线向量<span class="math inline">\(w_r\)</span> 定义）。</p><p><strong>多段点积（Multi-Linear Dot Products）</strong></p><p>多段点积方法通过从简单图中扩展点积解码器来研究多关系解码模型，解码器如下式所示：<span class="math display">\[DEC(z_u, \tau, z_v) = &lt;z_u,r_{\tau},z_v&gt; = \sum_{i=1}^d z_u[i]\times r_{\tau}[i] \times z_v[i]\]</span>该方法直接计算解码器中三个向量表示的点积。该解码器只能编码对称的关系，满足下式：<span class="math display">\[\begin{aligned}DEC(z_u, \tau, z_v) &amp;= &lt;z_u,r_{\tau},z_v&gt; \\&amp;= \sum_{i=1}^d z_u[i] \times r_{\tau}[i] \times z_v[i] \\&amp;= &lt;z_v,r_{\tau},z_u&gt; \\&amp;= DEC(z_v, \tau, z_u)\end{aligned}\]</span> <strong>复解码器</strong></p><p>复解码器能够编码有向且不对称的关系。</p><p>ComplEx 通过引入复值嵌入表示来扩展 DistMult 解码器，定义为下式：<span class="math display">\[\begin{aligned}DEC(z_u,\tau,z_v) &amp;= Re&lt;z_u,r_{\tau},\bar{z}_v&gt; \\&amp;= Re(\sum_{i=1}^d z_u[i] \times r_{\tau}[i] \times z_v[i])\end{aligned}\]</span> 其中，<span class="math inline">\(z_u,z_v,r_{\tau} \in\mathbb{C}^d\)</span> 为复值嵌入，Re表示复值向量的实部。由于采用了尾部嵌入表示的复共轭 <spanclass="math inline">\(\bar{z}_v\)</span>，因此这种解码方法适用于非对称关系。</p><p>RotateE 方法主要将解码过程定义为嵌入表示在复平面上的旋转，如下式：<span class="math display">\[DEC(z_u,\tau,z_v) = || Z_u \circ r_{\tau} - z_v ||\]</span> 其中，<span class="math inline">\(\circ\)</span>为哈达玛乘积。假设上式中所有嵌入表示都是复值，且使 <spanclass="math inline">\(|r_{\tau}[i]|=1,\forall i \in{1,\cdots,d}\)</span>，则关系嵌入的每一维向量能表示成 <spanclass="math inline">\(r_{\tau}[i]=e^{i\theta_{r,i}}\)</span>，其对应关系向量在复平面内的旋转。</p><h2 id="解码器的性能表征">解码器的性能表征</h2><p><strong>对称性与非对称性</strong></p><p>满足下式的关系具有对称性： <span class="math display">\[(u,\tau,v) \in \varepsilon \leftrightarrow (v,\tau,u) \in \varepsilon\]</span> 满足下式的关系具有非对称性： <span class="math display">\[(u,\tau,v) \in \varepsilon \leftrightarrow (v,\tau,u) \notin \varepsilon\]</span> DistMult 仅能建模对称关系。</p><p>TransE 解码器仅能建模非对称关系，如下式所示： <spanclass="math display">\[\begin{aligned}DEC(Z_u, \tau, z_v) &amp;= DEC(z_v,\tau,z_u) \\-|| z_u + r_{\tau} -z_v || &amp;= -|| z_v + r_{\tau} -z_u || \\\Longrightarrow -r_{\tau} &amp;= r_{\tau} \\\Longrightarrow r_{\tau} &amp;= 0\end{aligned}\]</span> <strong>互逆性</strong></p><p>互逆性是指一种关系的存在暗含这另一种相反方向关系的存在，如下式：<span class="math display">\[(u,\tau_1,v) \leftrightarrow (v,\tau_2,u) \in \varepsilon\]</span> DisMult解码器无法对这种关心模式建模，但大多数其它类型解码器都能够表示逆关系。</p><p><strong>组合性</strong></p><p>满足下式的关系具有组合性： <span class="math display">\[(u,\tau_1,v) \in \varepsilon \wedge (v,\tau_2,u) \in \varepsilon\rightarrow (u,\tau_3,v) \in \varepsilon\]</span> 在 TransE 模型中，当 <span class="math inline">\(r_{\tau_3} =r_{\tau_1} + r_{\tau_2}\)</span>，满足上述组合性。在 RESCAL中可以通过定义 <span class="math inline">\(R_{\tau_3} =R_{\tau_1}R_{\tau_2}\)</span> 满足上述组合性。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（二）柳永篇</title>
    <link href="/2023/05/28/song-ci-er-liu-yong-pian/"/>
    <url>/2023/05/28/song-ci-er-liu-yong-pian/</url>
    
    <content type="html"><![CDATA[<center>曲玉管</center><center>陇首云飞，江边日晚，烟波满目凭栏久。一望关河萧索，千里清秋，忍凝眸。</center><center>杳杳神京，盈盈仙子，别来锦字终难偶。断雁无凭，冉冉飞下汀州，思悠悠。</center><center>暗想当初，有多少，幽欢佳会；岂知聚散难期，翻成雨恨云愁。</center><center>阻追游，每登山临水，惹起平生心事，一场消黯，永日无言，却下层楼。</center><p><br></p><center>雨霖铃</center><p>寒蝉凄切，对长亭晚，骤雨初歇。都门帐饮无绪，留恋处，兰舟催发。执手相看泪眼，竟无语凝噎。念去去，千里烟波，暮霭沉沉楚天阔。</p><p>多情自古伤离别，更那堪、冷落清秋节！今宵酒醒何处？杨柳岸、晓风残月。此去经年，应是良辰好景虚设。便纵有千种风情，更与何人说？</p><p><br></p><center>蝶恋花</center><center>伫倚危楼风细细，望极春愁，黯黯生天际。草色烟光残照里，无言谁会凭栏意？</center><center>拟把疏狂图一醉，对酒当歌，强乐还无味。衣带渐宽终不悔，为伊消得人憔悴。</center><p><br></p><center>采莲令</center><p>月华收，云淡霜天曙。西征客、此时情苦。翠娥执手，送临歧、轧轧开朱户。千娇面、盈盈伫立，无言有泪，断肠争忍回顾？</p><p>一叶兰舟，便恁急桨凌波去。贪行色、岂知离绪，万般方寸，但饮恨、脉脉同谁语？更回首、重城不见，寒江天外，隐隐两三烟树。</p><p><br></p><p>定风波</p><p>自春来，惨绿愁红，芳心是是可可。日上花梢，莺穿柳带，犹压香衾卧。暖酥消、腻云亸，终日厌厌倦梳裹。无那，恨薄情一去，音书无个。</p><p>早知恁个，悔当初、不把雕鞍锁。向鸡窗，只与蛮笺象管，拘束教吟课。镇相随、莫抛躲，针线闲拈伴伊坐。和我，免使年少光阴虚过。</p>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>宋词（一）苏轼篇</title>
    <link href="/2023/05/28/song-ci-yi-su-shi-pian/"/>
    <url>/2023/05/28/song-ci-yi-su-shi-pian/</url>
    
    <content type="html"><![CDATA[<center>水调歌头</center><p>丙辰中秋，欢饮达旦，大醉，作此篇兼怀子由。</p><p>明月几时有，把酒问青天。不知天上宫阙，今夕是何年。我欲乘风归去，惟恐琼楼玉宇，高处不胜寒。起舞弄清影，何似在人间。</p><p>转朱阁，低绮户，照无眠。不应有恨，何事长向别时圆？人有悲欢离合，月有阴晴圆缺。此时古难全。但愿人长久，千里共婵娟。</p><p><br></p><center>水龙吟 次韵章质夫《杨花词》</center><p>似花还似非花，也无人惜从教坠。抛家傍路，思量却是，无情有思。萦损柔肠，困酣娇眼，欲开还闭。梦随风万里，寻郎去处，又还被莺呼起。</p><p>不恨此花飞尽，恨西园、落红难缀。晓来雨过，遗踪何在，一池萍碎。春色三分，二分尘土，一分流水。细看来不是杨花，点点是离人泪。</p><p><br></p><center>念奴娇 赤壁怀古</center><p>大江东去，浪淘尽、千古风流人物。故垒西边，人道是、三国周郎赤壁。乱石穿空，惊涛拍岸，卷起千堆雪。江山如画，一时多少豪杰。</p><p>遥想公瑾当年，小乔初嫁了，雄姿英发。羽扇纶巾，谈笑间、樯橹灰飞烟灭。故国神游，多情应笑我，早生华发。人生如梦，一樽还酹江月。</p><p><br></p><center>永遇乐</center><p>彭城夜宿燕子楼，梦盼盼，因作此词。</p><p>明月如霜，好风如水，清景无限。曲港跳鱼，圆荷泻露，寂寞无人见。紞如三鼓，铿然一叶，黯黯梦云惊断。夜茫茫、重寻无处，觉来小园行遍。</p><p>天涯倦客，山中归路，望断故园心眼。燕子楼空，佳人何在？空锁楼中燕。古今如梦，何曾梦觉，但有旧欢新怨。异时对、黄楼夜景，为余浩叹。</p><p><br></p><center>洞仙歌</center><p>余七岁时，见眉州老尼，姓朱，忘其名，年九十岁。自言尝随其师入蜀主孟昶宫中，一日大热，蜀主与花蕊夫人夜起纳凉摩诃池上，作一词，朱具能记之。今四十年，朱已死久矣，人无知此词者，但记其首两句，暇日寻味，岂“洞仙歌”令乎？乃为足之云。</p><p>冰肌玉骨，自清凉无汗。水殿风来暗香满。秀帘开、一点明月窥人，人未寝、欹枕钗横鬓乱。</p><p>起来携素手，庭户无声，时见疏星度河汉。试问夜如何？夜已三更，金波淡、玉绳低转。但屈指、西风几时来，又不道流年、暗中偷换。</p><p><br></p><center>卜算子</center><center>黄州定慧院寓居作。</center><center>缺月挂疏桐，漏断人初静。谁见幽人独往来，飘渺孤鸿影。</center><center>惊起却回头，有恨无人省。拣尽寒枝不肯栖，寂寞沙洲冷。</center><p><br></p><center>青玉案</center><center>和贺方回韵，送伯固归吴中故居。</center><center>三年枕上吴中路，遣黄犬、随君去。若到松江呼小渡，莫惊鸳鹭，四桥尽是、老子经行处。</center><center>《辋川图》上看春暮，常记高人右丞句。作个归期天定许，春衫犹是，小蛮针线，曾湿西湖雨。</center><p><br></p><center>临江仙 夜归临皋</center><center>夜饮东坡醒复醉，归来仿佛三更。家童鼻息已雷鸣，敲门都不应，倚杖听江声。</center><center>长恨此身非我有，何时忘却营营。夜阑风静縠纹平，小舟从此逝，江海寄余生。</center><p><br></p><center><p>定风波</p><center>三月七日沙湖道中遇雨，雨具先去，同行皆狼狈，余独不觉。已而遂晴，故作此。</center><center>莫听穿林打叶声，何妨吟啸且徐行。竹杖芒鞋轻胜马。谁怕？一蓑烟雨任平生。</center><center>料峭春风吹酒醒，微冷，山头斜照却相迎。回首向来萧瑟处，归去，也无风雨也无晴。</center><p><br></p><center>江城子</center><center>乙卯正月二十日夜记梦</center><center>十年生死两茫茫，不思量，自难忘。千里孤坟，无处话凄凉。纵使相逢应不识，尘满面，鬓如霜。</center><center>夜来幽梦忽还乡，小轩窗，正梳妆。相顾无言，惟有泪千行。料得年年肠断处，明月夜，短松冈。</center><p><br></p>]]></content>
    
    
    <categories>
      
      <category>宋词</category>
      
    </categories>
    
    
    <tags>
      
      <tag>宋词</tag>
      
      <tag>文学</tag>
      
      <tag>诗词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（二）传统机器学习方法</title>
    <link href="/2023/05/28/tu-shen-jing-wang-luo-xue-xi-ri-ji-er-chuan-tong-ji-qi-xue-xi-fang-fa/"/>
    <url>/2023/05/28/tu-shen-jing-wang-luo-xue-xi-ri-ji-er-chuan-tong-ji-qi-xue-xi-fang-fa/</url>
    
    <content type="html"><![CDATA[<p><strong>标准机器学习范式</strong></p><p>首先，基于启发式函数或领域知识提取一些统计特征；然后将其作为标准机器学习分类器（如逻辑回归）的输入。</p><h2 id="图统计特征">图统计特征</h2><h3 id="节点层面的统计特征">节点层面的统计特征</h3><p><strong>节点的度</strong></p><p>节点<spanclass="math inline">\(u\)</span>的度反映与这个节点相连接的边的数目，可由下式表示<span class="math display">\[d_u = \sum_{v \in \mathcal{V}} A[u,v]\]</span> 对于有向图和加权图，度分为入度和出度。对矩阵<spanclass="math inline">\(A\)</span>中的节点<spanclass="math inline">\(u\)</span>的行求和可以得到出度，对节点<spanclass="math inline">\(u\)</span>的列求和可以得到入度。</p><p><strong>节点的中心性</strong></p><p>角度一：特征向量中心性度量不仅考虑邻居节点个数的度，还考虑了邻居节点的重要性。<span class="math display">\[e_u = \frac{1}{\lambda}\sum_{v\in \mathcal{V}}A[u,v]e_v, \forall u \in\mathcal{V}\]</span> 将向量表示<spanclass="math inline">\(e\)</span>代入上述等式取代节点中心性向量可得到邻接矩阵的标准特征向量方程：<span class="math display">\[\lambda e = Ae\]</span>角度二：特征向量中心性衡量了一个节点在路径无限长的情况下在随机游走时被访问的概率。这种理解方式连接了节点重要性、随机游走和谱三个重要概念。<spanclass="math inline">\(\lambda\)</span>是<spanclass="math inline">\(A\)</span>的主要特征向量，可通过幂次迭代法则计算<spanclass="math inline">\(e\)</span>如下所示： <span class="math display">\[e^{(t+1)} = Ae^{(t)}\]</span> 从向量<spanclass="math inline">\(e^{(0)}=(1,1,\cdots,1)^T\)</span>开始，依据幂次迭代法则，第一次迭代可以得到每个节点的度，在第<spanclass="math inline">\(t\)</span>次迭代<span class="math inline">\((t\geq 1)\)</span>时，<spanclass="math inline">\(e^{(t)}\)</span>包括了尝试到达每个节点且长度为<spanclass="math inline">\(t\)</span>的路线的长度的数量，无限重复下去可得到路径无限长时每个节点被访问的次数。</p><p><strong>聚类系数</strong></p><p>聚类系数通过一个节点的局部邻域中闭合三角形的比例度量节点的邻居节点聚类的紧密程度。聚类系数计算方法Local Variant 方法如下式所示： <span class="math display">\[c_u = \frac{|(v_1, v_2)\in \varepsilon: v_1, v_2 \in\mathcal{N}(u)|}{\binom{d_u}{2}}\]</span> 其中<span class="math inline">\(\mathcal{N}(u)=\{v\in\mathcal{V}: (u,v)\in \varepsilon\}\)</span>表示节点<spanclass="math inline">\(u\)</span>的邻居节点。</p><p><strong>闭合三角形、自我中心图、Motifs</strong></p><p><strong>闭合三角形</strong></p><h3 id="图层面的统计特征">图层面的统计特征</h3><p>节点袋</p><p>Weisfeiler-Lehman 核</p><p>Graphlets 和基于路径的方法</p><h2 id="邻域重叠检测">邻域重叠检测</h2><h2 id="图的拉普拉斯矩阵和图的谱方法">图的拉普拉斯矩阵和图的谱方法</h2>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络学习日记（一）图基础知识</title>
    <link href="/2023/05/24/tu-shen-jing-wang-luo-xue-xi-ri-ji-yi-tu-ji-chu-zhi-shi/"/>
    <url>/2023/05/24/tu-shen-jing-wang-luo-xue-xi-ri-ji-yi-tu-ji-chu-zhi-shi/</url>
    
    <content type="html"><![CDATA[<h2 id="图的定义">图的定义</h2><p>图由节点集合<spanclass="math inline">\(\mathcal{V}\)</span>和边集合<spanclass="math inline">\(\varepsilon\)</span>组成，记作<spanclass="math inline">\(\mathcal{G}=(\mathcal{V},\varepsilon)\)</span>，节点<spanclass="math inline">\(u\in \mathcal{V}\)</span>到节点<spanclass="math inline">\(v\in \mathcal{V}\)</span>的边表示为<spanclass="math inline">\((u,v)\in\varepsilon\)</span>.</p><h2 id="图的表示">图的表示</h2><p>图可以由邻接矩阵<span class="math inline">\(A\in\mathbb{R}^{|\mathcal{V}|\times|\mathcal{V}|}\)</span>表示，矩阵的行和列代表节点索引，矩阵元素<spanclass="math inline">\(A[u,v]\)</span>表示节点<spanclass="math inline">\(u\)</span>和节点<spanclass="math inline">\(v\)</span>的连接情况，如果<spanclass="math inline">\((u,v)\in \varepsilon\)</span>，则<spanclass="math inline">\(A[u,v]=1\)</span>，否则<spanclass="math inline">\(A[u,v]=0\)</span>。</p><h2 id="多关系图的分类">多关系图的分类</h2><p><strong>异构图</strong></p><p>异构图可以通过节点类型将节点划分为不相交的子集，即<spanclass="math inline">\(\mathcal{V} = \mathcal{V}_1 \bigcup \mathcal{V}_2\bigcup \cdots \bigcup \mathcal{V}_k\)</span>，其中<spanclass="math inline">\(\mathcal{V}_i \bigcap \mathcal{V}_j = \emptyset,\forall \neq j\)</span>。</p><p><strong>多重图</strong></p><p>多重图中的边只能连接不同类型的节点，即<spanclass="math inline">\((u,\tau_i,v)\in \varepsilon \rightarrow u \in\mathcal{V}_j,v \in \mathcal{V}_k \bigwedge j \neqk\)</span>。在多重图中，通常假设图可以被分解为<spanclass="math inline">\(k\)</span>个层级，每个节点可以属于一层或多层，每层代表唯一特定关系，表示本层内边的类型。</p><h2 id="图机器学习任务">图机器学习任务</h2><h3 id="节点预测">节点预测</h3><p>提供训练集真实的标签<span class="math inline">\(\mathcal{V}_{train}\subset \mathcal{V}\)</span>时，通过所有的节点<spanclass="math inline">\(u \in \mathcal{V}\)</span>预测标签<spanclass="math inline">\(y_u\)</span>应该属于哪种类型、类别或属性。</p><p>节点预测可通过显式利用节点之间的连接进行分类，如下几种节点之间的连接性质：</p><p><strong>同质性</strong></p><p>图中的节点与其邻居节点的属性相似，即节点有与邻居节点共享属性的趋势。</p><p><strong>异质性</strong></p><p>假定节点将优先连接到具有不同标签的节点。</p><p><strong>结构等价性</strong></p><p>具有相似局部结构的节点将具有相似的标签。</p><p><strong>节点预测是有监督还是半监督任务？</strong></p><p><strong>非标准的半监督任务。</strong></p><p>在半监督学习中，模型训练过程同时使用有标签数据和无标签数据，标准的半监督学习以独立同分布假设为前提，标准的监督学习在训练过程中不使用所有无标签的测试数据。节点分类任务中，图中节点全部都被使用，包括无标签节点，故节点分类任务是半监督学习，同时节点分类任务对一组相互连接的节点进行建模，打破了独立同分布假设，故节点分类是非标准的半监督任务。</p><h3 id="关系预测">关系预测</h3><p>给定一组节点<spanclass="math inline">\(\mathcal{V}\)</span>和部分边的集合<spanclass="math inline">\(\varepsilon_{train}\)</span>（<spanclass="math inline">\(\varepsilon_{train} \subset \varepsilon,\varepsilon\)</span>表示全体边的集合），利用这些给定信息推断缺失边的集合<spanclass="math inline">\((\varepsilon\)</span>  $ _{train})$。</p><h3 id="社区发现">社区发现</h3><p>通过输入一张图<spanclass="math inline">\(\mathcal{G}=(\mathcal{V},\varepsilon)\)</span>推断出潜在的社区结构。</p><p>社区发现常被类比为图领域的无监督学习中的聚类任务。</p><h3 id="图预测">图预测</h3><p>图预测包括对整张图进行分类、回归与聚类。</p><p>图分类或图回归任务中，数据集由多张不同图构成，图机器学习算法针对每张图进行独立预测，而不是预测图的组成部分。在图聚类任务中，目标是学习一个无监督的测量图与图之间相似性的策略。</p><p>图分类与图回归任务属于标准监督学习范畴。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
