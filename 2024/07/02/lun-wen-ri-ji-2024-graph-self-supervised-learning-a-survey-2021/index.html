

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="培根请加蛋">
  <meta name="keywords" content="">
  
    <meta name="description" content="Graph Self-Supervised Learning- A Survey">
<meta property="og:type" content="article">
<meta property="og:title" content="论文日记 2024 Graph Self-Supervised Learning- A Survey (2021)">
<meta property="og:url" content="https://blog.lfd.world/2024/07/02/lun-wen-ri-ji-2024-graph-self-supervised-learning-a-survey-2021/index.html">
<meta property="og:site_name" content="培根请加蛋">
<meta property="og:description" content="Graph Self-Supervised Learning- A Survey">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702161643736.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702204633170.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702201602188.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702201756321.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702201823164.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702201849595.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702203539859.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702203610823.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702203631682.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702205636812.png">
<meta property="og:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702205700737.png">
<meta property="article:published_time" content="2024-07-02T08:09:00.000Z">
<meta property="article:modified_time" content="2024-11-19T15:13:30.803Z">
<meta property="article:author" content="培根请加蛋">
<meta property="article:tag" content="Graph">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702161643736.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>论文日记 2024 Graph Self-Supervised Learning- A Survey (2021) - 培根请加蛋</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.lfd.world","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":false,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>培根请加蛋</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="论文日记 2024 Graph Self-Supervised Learning- A Survey (2021)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-07-02 16:09" pubdate>
          2024年7月2日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          32k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          266 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">论文日记 2024 Graph Self-Supervised Learning- A Survey (2021)</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="abstract">Abstract</h2>
<p>图的深度学习最近引起了人们的极大兴趣。然而，大多数工作都集中在（半）监督学习上，导致标签依赖重、泛化性差、鲁棒性弱等缺点。为了解决这些问题，自我监督学习（self-supervised
learning, SSL）通过精心设计的 pretext tasks
提取信息知识，而不依赖于手动标签，已成为图数据的一种有前景和趋势的学习范式。与计算机视觉和自然语言处理等其他领域的
SSL 不同，图上的 SSL
具有独特的背景、设计思想和分类法。在图自监督学习的框架下，我们对使用 SSL
技术处理图数据的现有方法进行了及时、全面的回顾。我们构建了一个统一的框架，以数学方式形式化图
SSL 的范式。根据 pretext tasks
的目标，我们将这些方法分为四类：基于生成的方法
(generation-based)、基于辅助属性的方法 (auxiliary
property-based)、基于对比的方法 (contrast-based) 和混合方法 (hybrid
approaches)。我们进一步描述了图 SSL 在各个研究领域的应用，并总结了图 SSL
的常用数据集、评估基准、性能比较和开源代码。最后，我们讨论了该研究领域剩余的挑战和潜在的未来方向。</p>
<p><strong>Index Terms</strong>: Self-supervised learning, graph
analytics, deep learning, graph representation learning, graph neural
networks.</p>
<h2 id="introductioin">Introductioin</h2>
<p>近年来，图的深度学习 [1]、[2]、[3]、[4]
在人工智能研究社区中变得越来越流行，因为图结构数据在包括电子商务在内的许多领域中无处不在
[5]、交通 [6]、化学 [7] 和知识库
[8]。大多数关于图的深度学习研究都集中在（半）监督学习场景，其中利用特定的下游任务（例如节点分类）来训练具有注释良好的手动标签的模型。尽管这些研究取得了成功，但对标签的严重依赖带来了一些缺点。首先，手动标签的收集和注释成本高昂，特别是对于拥有大规模数据集（例如引文和社交网络
[9]）或对领域知识有需求（例如化学和医学 [10]
的研究领域）。其次，由于过度拟合问题，纯粹的监督学习场景通常泛化性较差，特别是在训练数据稀缺的情况下
[11]。第三，有监督图深度学习模型容易受到与标签相关的对抗性攻击，导致图监督学习的鲁棒性较弱
[12]。</p>
<p>为了解决（半）监督学习的缺点，自监督学习（SSL）提供了一种有前途的学习范式，可以减少对手动标签的依赖。在
SSL 中，模型是通过解决一系列手工辅助任务（所谓的 pretext
tasks）来学习的，其中监督信号是从数据本身自动获取的，而不需要手动注释。借助精心设计的
pretext tasks，SSL
使模型能够从未标记的数据中学习更多信息表示，以实现更好的性能
[13]、[14]、泛化 [9]、[15]、[16] 和鲁棒性 [17] ，[18]
关于各种下游任务。</p>
<p>SSL 被图灵奖获得者 Yoshua Bengio 和 Yann LeCun
描述为“人类水平智能的关键”，最近在计算机视觉 (CV) 和自然语言处理 (NLP)
领域取得了巨大成功。 CV 领域的早期SSL方法为视觉表示学习 [19]
设计了各种语义相关的 pretext tasks，例如图像修复 [20]、图像着色 [21]
和拼图游戏 [22] 等。最近，自监督对比学习框架（例如，MoCo [23]、SimCLR
[24] 和 BYOL [25]）利用图像变换下语义的不变性来学习视觉特征。在 NLP
领域，早期的词嵌入方法 [26]、[27] 与 SSL
具有相同的想法，即从数据本身学习。通过语言 pretext tasks
进行预训练，最近的大规模语言模型（例如 BERT [28] 和 XLNet [29]）在多个
NLP 任务上实现了最先进的性能。</p>
<p>继 SSL 在 CV 和 NLP 上取得巨大成功之后，最近，人们对将 SSL
应用于图结构数据越来越感兴趣。然而，将 CV/NLP 设计的 pretext tasks
转移到图数据分析并非易事。主要挑战是图处于不规则的非欧几里得数据空间中。与图像/语言数据所在的
2D/1D
规则网格欧几里德空间相比，非欧几里德空间更通用，但也更复杂。因此，一些针对网格结构数据的
pretext tasks
无法直接映射到图数据。此外，图数据中的数据示例（节点）与拓扑结构自然相关，而
CV（图像）和 NLP（文本）中的示例通常是独立的。因此，如何处理图 SSL
中的这种依赖关系成为 pretext tasks 设计的一个挑战。图 1
通过一些玩具示例说明了这种差异。考虑到图分析中的 SSL
与其他研究领域的显着差异，图 SSL 需要专有的定义和分类法。</p>
<p><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702161643736.png" alt="Fig. 1" style="zoom:67%;" /></p>
<p>Fig. 1: Toy examples of different SSL pretext tasks in CV, NLP and
graph analytics. In generative tasks, graph SSL should consider the
topological structure in an irregular grid as well as node features,
while SSL in CV/NLP just needs to recover the information in 2D/1D grid
space. In contrastive tasks, the dependency between nodes is
non-negligible in graph SSL, while the samples in CV/NLP are
independent.</p>
<p>图 SSL 的历史至少可以追溯到无监督图嵌入的早期研究
[30]、[31]。这些方法通过最大化截断随机游走中上下文节点之间的一致性来学习节点表示。经典的无监督学习模型图自动编码器（GAE）[32]也可以被视为学习重建图结构的图SSL方法。自2019年以来，最近的图SSL浪潮带来了各种
pretext tasks
的设计，从对比学习[13]、[33]到图属性挖掘[10]、[17]。考虑到图 SSL
研究的不断增长趋势以及相关 pretext tasks
的多样性，迫切需要构建统一的框架和系统分类来总结图 SSL
的方法和应用。</p>
<p>为了填补这一空白，本文对图 SSL
快速发展的领域进行了全面且最新的概述，并提供了相关应用的丰富资源和讨论。本文的目标受众是想要了解图数据自监督学习的一般机器学习研究人员、想要跟踪图神经网络
(GNN) 最新进展的图学习研究人员以及需要了解图数据的领域专家。希望将图 SSL
方法推广到新应用程序或其他领域。本次调查的核心贡献总结如下：</p>
<ul>
<li><strong>统一的框架和系统的分类。</strong>我们提出了一个统一的框架，以数学形式化图
SSL
方法。基于我们的框架，我们将现有的工作系统地分为四组：基于生成的方法、基于辅助属性的方法、基于对比的方法和混合方法。我们还构建了下游任务的分类法和
SSL 学习方案。</li>
<li><strong>全面且最新的审查。</strong>我们对经典和最新的图 SSL
方法进行了全面、及时的审查。对于每种类型的图 SSL
方法，我们提供细粒度的分类、数学描述、详细的比较和高级摘要。</li>
<li><strong>丰富的资源和应用。</strong>我们收集了丰富的图 SSL
资源，包括数据集、评估基准、性能比较和开源代码。我们还总结了图 SSL
在各个研究领域的实际应用。</li>
<li><strong>展望未来方向</strong>。我们指出当前研究的技术局限性。我们进一步从不同角度提出了未来工作的六个有前景的方向。</li>
</ul>
<p><em>与相关调查文章的比较。</em> 一些现有的调查主要从一般 SSL [34]、CV
SSL [19] 或自监督对比学习 [35]
的角度进行综述，而本文纯粹关注图结构数据的
SSL。与最近关于图自监督学习的调查 [36]、[37]
相比，我们的调查对该主题有了更全面的概述，并提供了以下差异：（1）统一的编码器-解码器框架来定义图SSL；
（2）从数学角度进行系统化、更细粒度的分类； （3）更及时的审查； (4)
更详细的资源总结，包括性能比较、数据集、实现和实际应用；
(5)对挑战和未来方向进行更具前瞻性的讨论。</p>
<p>本文的其余部分组织如下。第 2
节定义了相关概念并提供了其余各节中使用的符号。第3节描述了图 SSL
的框架，并从多个角度进行了分类。第 4-7 节分别回顾了四类图 SSL 方法。第 8
节总结了图 SSL 实证研究的有用资源，包括性能比较、数据集和开源实现。第 9
节调查了各个领域的实际应用。第 10 节分析了剩余的挑战和未来可能的方向。第
11 节最后总结了本文。</p>
<h2 id="definition-and-notation">Definition and Notation</h2>
<p>在本节中，我们概述了图 SSL
的相关术语定义，列出了常用的符号，并定义了与图相关的概念。</p>
<h3 id="term-definitions">Term Definitions</h3>
<p>在图 SSL 中，我们提供了以下相关基本概念的定义。</p>
<p><strong>手动标签与伪标签(Manual Labels Versus Pseudo Labels)</strong>
手动标签，在一些论文 [19]
中也称为人工注释标签，是指由人类专家或工作人员手动注释的标签。相反，伪标签表示机器可以在没有任何人类知识的情况下从数据中自动获取的标签。一般来说，伪标签比手动标签需要更低的获取成本，因此在手动标签难以获取或数据量巨大时，伪标签具有优势。在自监督学习环境中，可以设计特定的方法来生成伪标签，从而增强表示学习。</p>
<p><strong>下游任务与前置任务 (Downstream Tasks Versus Pretext
Tasks)</strong>
下游任务是用于评估不同模型学习的特征表示的质量或性能的图分析任务。典型应用包括节点分类和图分类。pretext
tasks
是指模型要解决的预先设计的任务（例如图重建），它有助于模型从未标记的数据中学习更通用的表示，从而通过提供更好的初始化或更有效的正则化来使下游任务受益。一般来说，解决下游任务需要手动标签，而
pretext tasks 通常通过伪标签来学习。</p>
<p><strong>监督学习、无监督学习和自我监督学习 (Supervised Learning,
Unsupervised Learning and Self-Supervised Learning)</strong>
监督学习是指利用明确定义的手动标签来训练机器学习模型的学习范式。相反，无监督学习是指不使用任何手动标签的学习范式。作为无监督学习的一个子集，自监督学习表示监督信号从数据本身生成的学习范式。在自监督学习方法中，模型通过
pretext tasks 进行训练，以获得更好的性能和下游任务的泛化能力。</p>
<h3 id="notations">Notations</h3>
<p>我们在本小节中提供了本文中使用的重要符号（在附录 B.1
中进行了总结）以及不同类型图和 GNN 的定义。</p>
<p><strong><em>Definition 1 (Plain Graph)</em></strong> 一个 plain graph
被表示为 <span
class="math inline">\(\mathcal{G}=(\mathcal{V},\mathcal{E})\)</span>，其中
<span class="math inline">\(\mathcal{V}=\{v_1,\dots,v_n\}\
(|\mathcal{V}|=n)\)</span> 是点集，<span
class="math inline">\(\mathcal{E}(|\mathcal{E}|=m)\)</span>
是边集，我们自然有 <span class="math inline">\(\mathcal{E} \subseteq
\mathcal{V} \times \mathcal{V}\)</span>。点 <span
class="math inline">\(v_i\)</span> 的邻居表示为 <span
class="math inline">\(\mathcal{N}(v_i) = \{v_j \in \mathcal{V}|e_{i,j}
\in \mathcal{E}\}\)</span>。图的拓扑表示为邻接矩阵 <span
class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span>，其中 <span
class="math inline">\(A_{i,j}=1\)</span> 表示 <span
class="math inline">\(e_{i.j}\in \mathcal{E}\)</span>，<span
class="math inline">\(A_{i,j}=0\)</span> 表示 <span
class="math inline">\(e_{i,j} \notin \mathcal{E}\)</span>。</p>
<p><strong><em>Definition 2 (Attributed Graph)</em></strong>
属性图是指节点和/或边与其自身特征（也称为属性）相关联的图。节点和边的特征矩阵分别表示为
<span class="math inline">\(X_{node} \in \mathbb{R}^{n \times
d_{node}}\)</span> 和 <span class="math inline">\(X_{edge} \in
\mathbb{R}^{m \times
d_{edge}}\)</span>。在更常见的只有节点具有特征的场景中，我们用 X ∈ Rn×d
简称为节点特征矩阵，并将属性图表示为 <span
class="math inline">\(\mathcal{G} = (\mathcal{V}, \mathcal{E},
\bf{X})\)</span>。</p>
<p>还有一些动态图和异构图，其定义在附录B.2中给出。</p>
<p>大多数审查的方法利用 GNN
作为骨干编码器，通过利用丰富的底层节点连接（即邻接矩阵 <span
class="math inline">\(\bf{A}\)</span> 和可学习参数），将输入原始节点特征
<span class="math inline">\(\bf{X}\)</span> 转换为紧凑节点表示 <span
class="math inline">\(\bf{H}\)</span>。此外，读出函数 <span
class="math inline">\(R(\cdot)\)</span> 通常用于从节点级表示 <span
class="math inline">\(\bf{H}\)</span> 生成图级表示 <span
class="math inline">\(h_{\mathcal{G}}\)</span>。GNN 和 readout
函数的公式在附录 B.3 中介绍。此外，在附录 B.4
中，我们制定了本次调查中常用的损失函数。</p>
<h2 id="framework-and-categorization">Framework and Categorization</h2>
<p>在本节中，我们提供了图 SSL
的统一框架，并从不同角度对其进行了进一步分类，包括 pretext
tasks、下游任务以及两者的组合（即自监督训练方案）。</p>
<h3
id="unified-framework-and-mathematical-formulation-of-graph-self-supervised-learning">Unified
Framework and Mathematical Formulation of Graph Self-Supervised
Learning</h3>
<p>我们构建了一个编码器-解码器框架来形式化图 SSL。编码器 <span
class="math inline">\(f_{\theta}\)</span>（由 <span
class="math inline">\(\theta\)</span> 参数化）旨在为图 <span
class="math inline">\(\mathcal{G}\)</span> 中的每个节点 <span
class="math inline">\(v_i\)</span> 学习低维表示（又名嵌入）<span
class="math inline">\(h_i \in \bf{H}\)</span>。一般来说，编码器 <span
class="math inline">\(f_{\theta}\)</span> 可以是 GNN [13]、[33]、[38]
或其他类型的用于图学习的神经网络 [30]、[31]、[39]。pretext 解码器 <span
class="math inline">\(p_{\phi}\)</span>（由 <span
class="math inline">\(\phi\)</span> 参数化）将 <span
class="math inline">\(\bf{H}\)</span> 作为 pretext tasks 的输入。 <span
class="math inline">\(p_{\phi}\)</span> 的架构取决于特定的下游任务。</p>
<p>在此框架下，图 SSL 可以表述为： <span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{ssl}\left(f_\theta,p_\phi,\mathcal{D}\right),
\tag{1}
\]</span> 其中 <span class="math inline">\(\mathcal{D}\)</span>
表示在未标记的图 <span class="math inline">\(\mathcal{G}\)</span> 中满足
<span
class="math inline">\((\mathcal{V},\mathcal{E})\sim\mathcal{D}\)</span>
的图数据分布，<span class="math inline">\(\mathcal{L}_{ssl}\)</span> 是
SSL 损失函数，它根据特定的精心设计的 pretext tasks 对 pretext
解码器的输出进行正则化。</p>
<p>通过利用经过训练的图编码器 <span
class="math inline">\(f_{\theta^*}\)</span>，生成的表示可以用于各种下游任务。这里我们引入下游解码器
<span class="math inline">\(q_{\psi}\)</span>（由 <span
class="math inline">\(\psi\)</span>
参数化），并将下游任务表示为图监督学习任务： <span
class="math display">\[
\theta^{**},\psi^*=\arg\min_{\theta^*,\psi}\mathcal{L}_{sup}\left(f_{\theta^*},q_\psi,\mathcal{G},y\right),
\tag{2}
\]</span> 其中 <span class="math inline">\(y\)</span>
表示下游任务标签，<span class="math inline">\(L_{sup}\)</span>
是训练下游任务模型的监督损失。</p>
<p>在下面的小节中，我们根据 3.2 节中的等式（1）指定了四种图 SSL
变体，通过不同地组合等式（1）和（2）在 3.3
节中指定了三种图自监督训练方案，以及基于三种类型的下游任务3.4
节中的等式（2）。</p>
<p><img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702204633170.png" alt="Fig. 2" style="zoom:67%;" /></p>
<h3 id="taxonomy-of-graph-self-supervised-learning">Taxonomy of Graph
Self-supervised Learning</h3>
<p>通过利用 pretext 解码器和目标函数的不同设计，图 SSL
从概念上可以分为四种类型，包括基于生成的方法、基于辅助属性的方法、基于对比的方法和混合方法。下面简要讨论这些方法的分类，如图
2 所示，每种方法的概念图如图 3 所示。</p>
<table>
<tr>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702201602188.png">
</center>
</td>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702201756321.png">Fig.
3
</center>
</td>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702201823164.png">
</center>
</td>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702201849595.png">
</center>
</td>
</tr>
</table>
<p><strong>基于生成的方法</strong> 从特征和结构两个角度形成图数据重构的
pretext
tasks。具体来说，他们专注于节点/边缘特征或/和图邻接重建。在这种情况下，式(1)可以进一步推导为：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{ssl}\Big(p_\phi(f_\theta(\tilde{\mathcal{G}})),\mathcal{G}\Big),
\tag{3}
\]</span> 其中 <span class="math inline">\(f_{\theta}\)</span> 和 <span
class="math inline">\(p_{\phi}\)</span> 是图编码器和 pretext
解码器。<span class="math inline">\(\tilde{\mathcal{G}}\)</span>
表示具有扰动的节点/边特征或/和邻接矩阵的图数据。对于大多数基于生成的方法，通常定义自监督目标函数
<span class="math inline">\(L_{ssl}\)</span>
来测量重建图数据与原始图数据之间的差异。代表性方法之一是 GAE
[32]，它通过重建图邻接矩阵来学习嵌入。</p>
<p><strong>基于属性的辅助方法</strong>
通过利用更大的属性集和拓扑图属性来丰富监督信号。特别是，对于不同的精心设计的辅助属性，我们进一步将这些方法分为两种类型：基于回归和基于分类。形式上，它们可以表述为：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{ssl}\Big(p_\phi\big(f_\theta(\mathcal{G})\big),c\Big),
\tag{4}
\]</span> 其中 <span class="math inline">\(c\)</span>
表示特定的精心设计的辅助属性。对于基于回归的方法，<span
class="math inline">\(c\)</span> 可以是局部或全局图属性，例如 <span
class="math inline">\(\mathcal{G}\)</span>
内的节点度或到聚类的距离。另一方面，对于基于分类的方法，辅助属性通常被构造为伪标签，例如图分区或聚类索引。关于目标函数，对于基于回归的方法，<span
class="math inline">\(L_{ssl}\)</span> 可以是均方误差
(MSE)；对于基于分类的方法，<span class="math inline">\(L_{ssl}\)</span>
可以是交叉熵 (CE) 损失。作为一项开创性工作，M3S
[40]使用节点聚类来构造提供监督信号的伪标签。</p>
<p><strong>基于对比的方法</strong>
通常是基于互信息（MI）最大化的概念开发的，其中同一对象（例如节点、子图和图）的增强实例之间的估计
MI 被最大化。对于基于对比的图 SSL，等式 (1) 重新表述为： <span
class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{ssl}\bigg(p_\phi\bigg(f_\theta(\tilde{\mathcal{G}}^{(1)}),f_\theta(\tilde{\mathcal{G}}^{(2)})\bigg)\bigg),
\tag{5}
\]</span> 其中 <span
class="math inline">\(\tilde{\mathcal{G}}^{(1)}\)</span> 和 <span
class="math inline">\(\tilde{\mathcal{G}}^{(2)}\)</span> 是 <span
class="math inline">\(\mathcal{G}\)</span>
的两个不同的增强实例。在这些方法中，pretext 解码器 <span
class="math inline">\(p_{\phi}\)</span>
表示估计两个实例之间一致性的鉴别器（例如，双线性函数或点积）， <span
class="math inline">\(L_{ssl}\)</span>
表示对比损失。通过将它们结合起来并优化 <span
class="math inline">\(L_{ssl}\)</span>，pretext
任务旨在估计和最大化正样本对（例如，同一对象的增强实例）之间的 MI
并最小化负样本（例如，从不同对象派生的实例）之间的 MI，这隐式地包含在
<span class="math inline">\(L_{ssl}\)</span>
中。代表性工作包括跨尺度方法（例如，DGI
[13]）和同尺度方法（例如，GraphCL [38] 和 GCC [15]）。</p>
<p><strong>混合方法</strong> 利用以前的类别，并由多个 pretext
解码器和/或训练目标组成。我们根据等式（3）至（5）的公式将这一方法分支表述为两个或多个图
SSL 方案的加权或未加权组合。 GMI [41]
联合考虑边缘级重建和节点级对比度，是一种典型的混合方法。</p>
<p><strong>讨论。</strong>不同的图 SSL
方法具有不同的属性。基于生成的方法很容易实现，因为重建任务很容易构建，但有时对于大规模图来说，恢复输入数据非常消耗内存。基于属性的辅助方法具有简单的解码器和损失函数设计；然而，选择有用的辅助属性通常需要领域知识。与其他类别相比，基于对比的方法具有更灵活的设计和更广泛的应用。然而，对比框架、增强策略和损失函数的设计通常依赖于耗时的实证实验。混合方法受益于多个
pretext tasks，但主要挑战是如何设计联合学习框架来平衡每个组件。</p>
<h3 id="taxonomy-of-self-supervised-training-schemes">Taxonomy of
Self-Supervised Training Schemes</h3>
<table>
<tr>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702203539859.png">
</center>
</td>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702203610823.png">Fig.
4
</center>
</td>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702203631682.png">
</center>
</td>
</tr>
</table>
<p>根据图编码器、自监督 pretext tasks
和下游任务之间的关系，我们研究了三种类型的图自监督训练方案：预训练和微调（Pre-training
and Fine-tuning, PF）、联合学习（Joint Learning,
JL）和无监督表示学习（Unsupervised Representation Learning,
URL）。它们的简要流程如图 4 所示。</p>
<p><strong>预训练和微调（PF）。</strong> 在 PF 方案中，编码器 <span
class="math inline">\(f_{\theta}\)</span> 首先在预训练数据集上使用
pretext tasks
进行预训练，这可以看作是编码器参数的初始化。之后，预训练的编码器 <span
class="math inline">\(f_{\theta_{init}}\)</span>
在微调数据集（带有标签）上与下游解码器 <span
class="math inline">\(q_{\psi}\)</span>
在特定下游任务的监督下一起进行微调。请注意，预训练和微调的数据集可以相同或不同。
PF 方案的制定定义如下： <span class="math display">\[
\begin{aligned}
\theta^{*},\phi^{*}&amp;
=\arg\min_{\theta,\phi}\mathcal{L}_{ssl}\left(f_\theta,p_\phi,\mathcal{D}\right),
\\
&amp;&amp;\text{(6)} \\
\theta^{**},\phi^{*}&amp;
=\arg\min_{\theta^*,\psi}\mathcal{L}_{sup}\left(p_{\theta^*},q_\psi,\mathcal{G},y\right).
\end{aligned}
\]</span> <strong>联合学习（JL）。</strong> 在 JL 方案中，编码器与
pretext
和下游任务联合训练。损失函数由自监督损失函数和下游任务损失函数组成，其中权衡超参数
<span class="math inline">\(\alpha\)</span>
控制自监督项的贡献。这可以被视为一种多任务学习，其中 pretext tasks
充当下游任务的正则化： <span class="math display">\[
\theta^{*},\phi^{*},\psi^{*}=\arg\min_{\theta,\phi,\psi}\left[\alpha\mathcal{L}_{ssl}\left(f_{\theta},p_{\phi},\mathcal{D}\right)+\mathcal{L}_{sup}\left(f_{\theta},q_{\psi},\mathcal{G},y\right)\right].
\tag{7}
\]</span> <strong>无监督表示学习（URL）。</strong> URL 方案的第一阶段与
PF
相似。区别在于：（1）在第二阶段，当使用下游任务训练模型时，编码器的参数被冻结（即<span
class="math inline">\(\theta^*\)</span>）；
(2)两个阶段的训练在同一数据集上进行。 URL的表述定义为： <span
class="math display">\[
\begin{aligned}
\theta^{*},\phi^{*}&amp;
=\arg\min_{\theta,\phi}\mathcal{L}_{ssl}\left(f_\theta,p_\phi,\mathcal{D}\right),
\\
&amp;&amp;\text{(8)} \\
\psi^{*}&amp;
=\arg\min_\psi\mathcal{L}_{sup}\left(f_{\theta^*},q_\psi,\mathcal{G},y\right).
\end{aligned}
\]</span> 与其他方案相比，URL
更具挑战性，因为编码器训练期间没有监督。</p>
<h3 id="taxonomy-of-downstream-tasks">Taxonomy of Downstream Tasks</h3>
<p>根据预测目标的规模，我们将下游任务分为节点级、链路级和图级任务。具体来说，节点级任务旨在根据节点表示来预测图中节点的属性。链路级任务推断边或节点对的属性，其中下游解码器将两个节点的嵌入映射到链路级预测中。此外，图级任务从具有多个图的数据集中学习并预测每个图的属性。基于式（2），我们给出了三类任务的下游解码器
<span class="math inline">\(q_{\psi}\)</span>、下游目标 <span
class="math inline">\(L_{sup}\)</span> 和下游任务标签 <span
class="math inline">\(y\)</span> 的具体定义，详见附录 C。</p>
<table>
<tr>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702205636812.png">
</center>
</td>
<td>
<center>
<img src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240702205700737.png">
</center>
</td>
</tr>
</table>
<p>Fig. 5: Examples of two categories of generation-based methods: Graph
Completion and Denoising Link Reconstruction.</p>
<h2 id="generation-based-methods">Generation-based Methods</h2>
<p>基于生成的方法旨在重建输入数据并将输入数据用作监督信号。这类方法的起源可以追溯到自动编码器
[42]，它学习使用编码器网络将数据向量压缩为低维表示，然后尝试使用解码器网络重建输入向量。与以矢量格式表示的通用输入数据不同，图数据是互连的。因此，基于生成的图
SSL
方法通常将完整图或子图作为模型输入，并单独重建其中一个组件，即特征或结构。根据重建的对象，我们将这些工作分为两个子类：（1）学习重建图的特征信息的特征生成，以及（2）学习重建图的拓扑结构信息的结构生成。图
5 给出了两种示例方法的流程，表 1 说明了基于生成的工作的摘要。</p>
<p>TABLE 1: Main characteristics of generation-based graph SSL
approaches. “FG” and “SG” mean “Feature Generation” and “Structure
Generation”, respectively. Missing values (“-”) in Input Data
Perturbation indicate that the method takes the original graph data as
input.</p>
<figure>
<img
src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240703210113081.png"
alt="image-20240703210113081" />
<figcaption aria-hidden="true">image-20240703210113081</figcaption>
</figure>
<h3 id="feature-generation">Feature Generation</h3>
<p>特征生成方法通过从扰动的或原始的图中恢复特征信息来学习。基于等式（3），特征生成方法可以进一步形式化为：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{mse}\left(p_\phi\left(f_\theta\left(\tilde{\mathcal{G}}\right)\right),\hat{\mathbf{X}}\right),
\tag{9}
\]</span> 其中 <span class="math inline">\(p_{\phi}(\cdot)\)</span>
是特征回归的解码器（例如，将表示映射到重构特征的全连接网络），<span
class="math inline">\(L_{mse}\)</span> 是均方误差（MSE）损失函数，<span
class="math inline">\(\hat{\bf{X}}\)</span>
是各种类型的通用表达式特征矩阵，例如节点特征矩阵、边特征矩阵或低维特征矩阵。</p>
<p>为了利用节点之间的依赖关系，特征生成方法的一个代表性分支遵循
<em>masked 特征回归策略</em>，该策略的动机是 CV 域中的图像修复
[20]。具体来说，在预处理阶段，某些节点/边的特征被 0
或特定标记屏蔽。然后，模型尝试根据未屏蔽的信息恢复屏蔽的特征。图补全
[17]
是一种代表性方法。它首先通过删除输入图的某些节点的特征来掩盖它们。然后，学习目标是使用
GCN [1]
编码器根据相邻节点的特征来预测屏蔽节点特征。我们可以将图补全视为等式（9）的实现，其中
<span class="math inline">\(\hat{\bf{X}}=\bf{X}\)</span> 且 <span
class="math inline">\(\tilde{\mathcal{G}}=(\bf{A},\tilde{\bf{X}})\)</span>。类似地，由于重建高维和稀疏特征的困难，AttributeMask
[43] 旨在重建经过主成分分析（PCA）[52]（<span
class="math inline">\(\hat{\bf{X}}=PCA(\bf{X})\)</span>）处理的密集特征矩阵，而不是原始特征。
AttrMasking [16] 不仅重建节点属性，还重建边属性，可以写成 <span
class="math inline">\(\hat{\bf{X}} = [\bf{X},
\bf{X}_{edge}]\)</span>。</p>
<p>方法的另一个分支旨在从噪声特征生成特征。受去噪自动编码器 [53]
的启发，MGAE [44] 从每个 GNN
层的噪声输入特征中恢复原始特征。这里我们也表示 <span
class="math inline">\(\tilde{\mathcal{G}}=(\bf{A},\tilde{\bf{X}})\)</span>
但这里 <span class="math inline">\(\tilde{\bf{X}}\)</span>
被随机噪声破坏了。 [45]
中提出，损坏的特征重建和损坏的嵌入重建旨在从损坏的特征重建原始特征和隐藏嵌入。</p>
<p>此外，直接从干净的数据中重建特征也是一种可用的解决方案。 GALA [46]
训练拉普拉斯平滑锐化图自动编码器模型，其目标是根据干净的输入图重建原始特征矩阵。类似地，自动编码
[45] 从干净的输入中重建原始特征。对于这两种方法，我们可以将 <span
class="math inline">\(\tilde{\mathcal{G}}=(\bf{A},\bf{X})\)</span> 和
<span class="math inline">\(\hat{\bf{X}}=\bf{X}\)</span> 形式化。</p>
<h3 id="structure-generation">Structure Generation</h3>
<p>与重建特征信息的特征生成方法不同，结构生成方法通过恢复结构信息来学习。在大多数情况下，目标是重建邻接矩阵，因为邻接矩阵可以简单地表示图的拓扑结构。基于等式（3），结构生成方法可以形式化如下：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{ssl}\Big(p_\phi\left(f_\theta\left(\tilde{\mathcal{G}}\right)\right),\mathbf{A}\Big),
\tag{10}
\]</span> 其中 <span class="math inline">\(p_{\phi}(\cdot)\)</span>
是用于结构重建的解码器，<span class="math inline">\(\bf{A}\)</span>
是（完整或部分）邻接矩阵。</p>
<p>GAE [32] 是结构生成方法最简单的实例。在 GAE 中，基于 GCN
的编码器首先从原始图 (<span
class="math inline">\(\tilde{\mathcal{G}}=\mathcal{G}\)</span>)
生成节点嵌入 <span class="math inline">\(\bf{H}\)</span>。然后，具有
sigmoid 激活的内积函数作为其解码器，从 <span
class="math inline">\(\bf{H}\)</span> 中恢复邻接矩阵。由于邻接矩阵 A
通常是二元且稀疏的，因此采用 BCE
损失函数来最大化恢复的邻接矩阵与原始邻接矩阵之间的相似性，其中正样本和负样本分别是存在的边
(<span class="math inline">\(\mathbf{A}_{i,j}=1\)</span>)
和未连接的节点对 (<span
class="math inline">\(\mathbf{A}_{i,j}=0\)</span>)。为了避免由于邻接极度稀疏而导致的训练样本不平衡问题，可以使用两种策略来防止平凡解决：（1）用
<span class="math inline">\(\mathbf{A}_{i,j} = 1\)</span> 重新加权项；或
(2) 子采样项，其中 <span class="math inline">\(\mathbf{A}_{i,j} =
0\)</span>。</p>
<p>作为经典的学习范式，GAE 有一系列的衍生作品。 VGAE [32]
进一步将变分自动编码器 [54] 的思想集成到 GAE
中。它采用基于推理模型的编码器，通过两个并行输出层估计平均值和偏差，并使用先验分布和估计分布之间的
Kullback-Leibler 散度。继 VGAE 之后，SIG-VAE [47]
考虑分层变分推理来学习图数据的更多生成表示。 ARGA/ARVGA [48]
使用生成对抗网络（GAN）[55] 规范 GAE/VGAE
模型。具体来说，训练鉴别器来区分假数据和真实数据，这迫使潜在嵌入的分布更接近高斯先验。
SuperGAT [49]
进一步将这个想法扩展到编码器中的每一层。具体来说，它根据编码器中每一层的潜在表示重建邻接矩阵。</p>
<p>另一种解决方案是重建被屏蔽的边，而不是重建完整的图。去噪链接重建 [50]
随机丢弃现有边缘以获得扰动图 <span
class="math inline">\(\tilde{\mathcal{G}}\)</span>。然后，该模型旨在使用经过
BCE 损失训练的基于成对相似性的解码器来恢复丢弃的连接。 EdgeMask [43]
也有类似的扰动策略，其中非参数 MAE
函数最小化两个连接节点的嵌入之间的差异。Zhu 等人 [51] 对输入图 (<span
class="math inline">\(\tilde{\mathcal{G}}=(\tilde{\bf{A}},\tilde{\bf{X}})\)</span>)
应用两种扰动策略，即随机删除链接和随机覆盖特征，其目标是通过解码器恢复屏蔽链接。</p>
<p><strong>讨论。</strong>由于学习目标不同，基于生成的方法的两个分支具有不同的解码器和损失函数设计。由于结构生成侧重于边缘重建，因此通过结构生成学习到的表示通常包含更多的节点对级信息；相反，特征生成方法通常捕获节点级知识。</p>
<h2 id="auxiliary-propert-based-methods">Auxiliary Propert-based
Methods</h2>
<p>基于辅助属性的方法从节点、链路和图级属性获取监督信号，这些属性可以从图数据中自由获得。这些方法与监督学习具有相似的训练范式，因为它们都通过“样本-标签”对进行学习。它们的区别在于标签的获取方式：在监督学习中，人工标注的标签往往需要昂贵的成本；在基于辅助属性的
SSL 中，伪标签是自动生成的，无需任何成本。</p>
<p>按照监督学习的一般分类法，我们将基于辅助属性的方法分为两个子类别：（1）辅助属性分类，利用基于分类的
pretext tasks 来训练编码器；（2）辅助属性回归，通过基于回归的方法执行
SSL pretext tasks。图 6 提供了它们的流程，表 2
总结了辅助的基于属性的方法。</p>
<figure>
<img
src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240703213157158.png"
alt="image-20240703213157158" />
<figcaption aria-hidden="true">image-20240703213157158</figcaption>
</figure>
<figure>
<img
src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240703213218557.png"
alt="image-20240703213218557" />
<figcaption aria-hidden="true">image-20240703213218557</figcaption>
</figure>
<h3 id="auxiliary-property-classification">Auxiliary Property
Classification</h3>
<p>辅助属性分类方法借鉴监督分类任务的训练范式，自动创建离散伪标签，构建分类器作为
pretext 解码器，并使用交叉熵（CE）损失 <span
class="math inline">\(L_{ce}\)</span>
来训练模型。源自等式（4），我们将该方法分支的形式化为： <span
class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{ce}\Big(p_\phi(f_\theta(\mathcal{G})),c\Big),
\tag{11}
\]</span> 其中 <span class="math inline">\(p_{\phi}\)</span>
是基于神经网络分类器的解码器，输出 <span
class="math inline">\(k\)</span> 维概率向量（<span
class="math inline">\(k\)</span> 是类的数量），<span
class="math inline">\(c \in \mathcal{C} = {c_1, \cdots, c_k}\)</span>
是属于离散和有限标签集 <span class="math inline">\(\mathcal{C}\)</span>
相应伪标签，根据伪标签集 <span
class="math inline">\(\mathcal{C}\)</span>
的定义，我们进一步构造了辅助属性分类下的两个子类别，即基于聚类的方法和基于配对关系的方法。</p>
<h4 id="clustering-based-methods">Clustering-based Methods</h4>
<p>构造伪标签的一种有前途的方法是根据节点的属性或结构特征将节点划分为不同的簇。为了实现这一点，引入了映射函数
<span class="math inline">\(\Omega : \mathcal{V} \rightarrow
\mathcal{C}\)</span>
来获取每个节点的伪标签，该函数建立在特定的无监督聚类/分区算法
[59]、[60]、[61]、[62]
的基础上。然后，学习目标是将每个节点分类到其相应的簇中。根据式（11），学习目标细化为：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\frac{1}{|\mathcal{V}|}\sum_{v_i\in\mathcal{V}}\mathcal{L}_{ce}\Big(p_\phi\big([f_\theta(\mathcal{G})]_{v_i}\big),\Omega(v_i)\Big),
\tag{12}
\]</span> 其中 <span class="math inline">\([\cdot]_{v_i}\)</span> 是提取
<span class="math inline">\(v_i\)</span> 表示的选取函数。</p>
<p>节点聚类 [17]
是一种利用属性信息生成伪标签的代表性方法。具体来说，它利用基于特征的聚类算法（<span
class="math inline">\(\Omega\)</span> 的实例）以 <span
class="math inline">\(\bf{X}\)</span> 为输入将节点集划分为 <span
class="math inline">\(k\)</span>
个簇，每个簇指示一个用于分类的伪标签。节点聚类背后的直觉是具有相似特征的节点往往具有一致的语义属性。
M3S [40] 使用 DeepCluster [61] 算法引入了 SSL
的多阶段自训练机制。在每个阶段，它首先在嵌入 <span
class="math inline">\(\bf{H}\)</span> 的节点上运行 K
均值聚类。之后，执行对齐以将每个聚类映射到类标签。最后，具有高置信度的未标记节点被赋予相应的（伪）标签并用于训练模型。在
M3S 中，<span class="math inline">\(\mathcal{C}\)</span>
借用自手动标签集 <span class="math inline">\(\mathcal{Y}\)</span>，<span
class="math inline">\(\Omega\)</span> 由 Kmeans 和对齐算法组成。</p>
<p>除了基于特征的聚类之外，图划分 [17]
还根据节点的结构特征来划分节点。具体来说，它通过最小化子集之间的连接将节点分组为多个子集
[59]，并将 <span class="math inline">\(\Omega\)</span>
定义为图划分算法。聚类保留 [50] 首先利用图聚类算法 [63]
来获取不重叠的聚类，然后通过基于注意力的聚合器计算每个聚类的表示。之后，表示每个节点与聚类表示之间相似性的向量被分配为每个节点的软伪标签。此外，CAGNN
[56]
首先运行基于特征的聚类来生成伪标签，然后通过最小化聚类间边缘来细化聚类，这吸收了属性聚类算法和结构聚类算法的优点。</p>
<h4 id="pair-relation-based-methods">Pair Relation-based Methods</h4>
<p>除了聚类和图属性之外，另一种监督信号是图中每对节点之间的关系。在这些方法中，解码器的输入不是单个节点或图，而是一对节点。映射函数
<span class="math inline">\(\Omega: \mathcal{V} \times \mathcal{V}
\rightarrow \mathcal{C}\)</span>
用于根据成对上下文关系定义伪标签。我们将目标函数写为： <span
class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\frac1{|\mathcal{P}|}\sum_{v_i,v_j\in\mathcal{P}}\mathcal{L}_{ce}\Big(p_\phi\big([f_\theta(\mathcal{G})]_{v_i,v_j}\big),\Omega(v_i,v_j)\Big),
\tag{13}
\]</span> 其中 <span class="math inline">\(\mathcal{P} \subseteq
\mathcal{V} \times \mathcal{V}\)</span> 是特定 pretext tasks
定义的节点对集合，<span class="math inline">\([\cdot]_{v_i,v_j}\)</span>
是提取并连接 <span class="math inline">\(v_i\)</span> 和 <span
class="math inline">\(v_j\)</span> 的节点表示的选取函数。</p>
<p>一些方法将两个节点之间的距离作为辅助属性。例如，S2GRL [57]
通过预测两个节点之间的最短路径来学习。具体来说，一对节点的标签被定义为它们之间的最短距离。形式上，我们可以将映射函数写为
<span
class="math inline">\(\Omega(v_i,v_j)=dist(v_i,v_j)\)</span>。解码器的构建是为了测量节点对之间的交互，其定义为两个嵌入向量之间的元素距离。节点对集合
<span class="math inline">\(\mathcal{P}\)</span>
收集所有可能的节点对，包括所有节点与其 1 到 <span
class="math inline">\(K\)</span> 跳邻域的组合。 PairwiseDistance [43]
具有与 <span class="math inline">\(\text{S}^2\)</span>GRL
非常相似的学习目标和解码器，但引入了距离上限，可以表示为 <span
class="math inline">\(\Omega(v_i,v_j)=\text{max}(dist(v_i,v_j),4)\)</span>。</p>
<p>中心性得分排名 [50] 提出了一个 pretext
tasks，预测一对节点之间中心性得分的相对顺序。对于每个节点对（<span
class="math inline">\(v_i，v_j\)</span>），它首先计算四种类型的中心性分数<span
class="math inline">\(s_i，s_j\)</span>（特征中心性，中介性，接近性和子图中心性），然后通过比较
<span class="math inline">\(s_i\)</span> 和 <span
class="math inline">\(s_j\)</span>
的值来创建其伪标签。我们将映射函数形式化为：<span
class="math inline">\(\Omega(v_i,v_j)=\mathbb{I}(s_i&gt;s_j)\)</span>，其中
<span class="math inline">\(\mathbb{I}(\cdot)\)</span> 是恒等函数。</p>
<h3 id="auxiliary-property-regression">Auxiliary Property
Regression</h3>
<p>辅助属性回归方法构建了预测图的广泛数值属性的 pretext
tasks。与辅助属性分类相比，最显着的区别是辅助属性是一定范围内的连续值，而不是有限集合中的离散伪标签。我们将等式（4）细化为回归版本：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{mse}\left(p_\phi(f_\theta(\mathcal{G})),c\right),
\tag{14}
\]</span> 其中 <span class="math inline">\(L_{mse}\)</span> 是回归的 MSE
损失函数，<span class="math inline">\(c \in \mathbb{R}\)</span>
是连续属性值。</p>
<p>NodeProperty [43] 是一个节点级 pretext
task，用于预测每个节点的属性。节点属性的可用选择包括其度数、局部节点重要性和局部聚类系数。以节点度为例，目标函数如下：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\frac{1}{|\mathcal{V}|}\sum_{v_i\in\mathcal{V}}\mathcal{L}_{mse}\left(p_\phi\big([f_\theta(\mathcal{G})\big)]_{v_i},\Omega(v_i)\big),\right.
\tag{15}
\]</span> 其中 <span class="math inline">\(\Omega(v_i)=\sum_{j=1}^n
\bf{A}_{ij}\)</span> 是计算节点 <span class="math inline">\(v_i\)</span>
度数的映射函数。 Distance2Cluster [43]
旨在回归每个节点到预定义图簇的距离。具体来说，它首先使用METIS算法
[64]将图划分为多个簇，并将每个簇内度数最高的节点定义为簇中心。然后，目标是预测每个节点到所有聚类中心之间的距离。</p>
<p>另一种方法将成对属性作为回归目标。例如，PairwiseAttrSim [43]
的目标是根据两个节点的嵌入来预测两个节点的特征相似度。我们将其目标函数形式化如下：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\frac{1}{|\mathcal{P}|}\sum_{v_i,v_j\in\mathcal{P}}\mathcal{L}_{mse}\left(p_\phi\big([f_\theta(\mathcal{G})]_{v_i,v_j}\big),\Omega(v_i,v_j)\big),\right.
\tag{16}
\]</span> 其中映射函数 <span
class="math inline">\(\Omega(v_i,v_j)=\text{cosine}(x_i,x_j)\)</span>
是原始特征的余弦相似度。在 PairwiseAttrSim
中，选择相似度和相异度最高的节点对构成节点对集合 <span
class="math inline">\(\mathcal{P}{}\)</span>。与PairwiseAttrSim
类似，SimP-GCN [58]
也考虑将预测原始特征的余弦相似度作为下游任务的自监督正则化。</p>
<p><strong>讨论。</strong>正如我们所观察到的，辅助属性分类方法比回归方法更加多样化，因为离散伪标签可以通过各种算法获取。在未来的工作中，回归方法预计将利用更多的连续属性。</p>
<h2 id="contrast-based-methods">Contrast-based Methods</h2>
<p>基于对比的方法建立在互信息（MI）最大化的思想之上
[65]，它通过预测两个增强实例之间的一致性来学习。具体来说，具有相似语义信息的图实例（即正样本）之间的
MI 被最大化，而具有不相关信息的图实例（即负样本）之间的 MI
被最小化。与视觉领域 [24]、[66] 类似，存在多种多粒度的图形增强和对比
pretext tasks 来丰富监督信号。</p>
<p>遵循第 3.2.3 节中定义的基于对比的图 SSL
的分类法，我们从三个角度研究该方法分支：（1）生成各种图实例的图增强；
（2）图对比学习，在非欧空间上形成各种对比 pretext tasks；
(3)互信息估计，测量实例之间的 MI，并与特定 pretext tasks
一起形成对比学习目标。</p>
<h3 id="graph-augmentations">Graph Augmentations</h3>
<p>最近视觉领域对比学习的成功在很大程度上依赖于精心设计的图像增强，这表明数据增强使
pretext tasks
更具挑战性，从而有利于模型探索更丰富的底层语义信息[67]。然而，由于图结构数据的性质，很难将欧几里德空间的增强直接应用到非欧几里德空间。受图像增强（例如图像剪切和裁剪
[24]）的推动，现有的图增强可以分为三种类型：基于属性的、基于拓扑的以及两者的组合（即混合增强）。图
7 展示了五种代表性增强策略的示例。正式地，给定图 G，我们将第 <span
class="math inline">\(i\)</span> 个增强图实例定义为 <span
class="math inline">\(\tilde{\mathcal{G}}^{(i)}=t_i(\mathcal{G})\)</span>，其中
<span class="math inline">\(t_i \sim \tau\)</span> 是选定的图增强和
<span class="math inline">\(\tau\)</span> 是一组可用的增强。</p>
<h4 id="attributive-augmentations">Attributive augmentations</h4>
<p>此类增强通常放置在节点属性上。给定 <span
class="math inline">\(\mathcal{G} = (\bf{A},
\bf{X})\)</span>，增强图表示为： <span class="math display">\[
\tilde{\mathcal{G}}^{(i)}=(\mathbf{A},\tilde{\mathbf{X}}^{(i)})=(\mathbf{A},t_i(\mathbf{X})),
\tag{17}
\]</span> 其中 <span class="math inline">\(t_i(\cdot)\)</span>
仅放置在节点特征矩阵上 <span
class="math inline">\(\tilde{\bf{X}}^{(i)}\)</span>
表示增强的节点特征。具体来说，属性增强有两种变体。第一种类型是节点特征屏蔽（NFM）[16]、[33]、[38]、[43]、[68]，它随机屏蔽给定图中部分节点的特征。特别是，我们可以完全（即按行）用零
[16]、[43] 屏蔽选定的特征向量，或部分（即按列）用零 [33]、[68]
屏蔽多个选定的特征通道。我们将节点特征屏蔽操作表述为： <span
class="math display">\[
t_i(\mathbf{X})=\mathrm{M}\circ\mathrm{X}, \tag{18}
\]</span> 其中，<span class="math inline">\(\bf{M}\)</span> 是与 <span
class="math inline">\(\bf{X}\)</span> 形状相同的掩蔽矩阵，<span
class="math inline">\(\circ\)</span> 表示 Hadamard
乘积。对于给定的掩码矩阵，其元素已初始化为
1，掩码条目被分配为零。除了随机采样掩蔽矩阵 <span
class="math inline">\(\bf{M}\)</span> 之外，我们还可以自适应地计算它
[69]、[70]。例如，GCA [69]
保持重要节点特征未被屏蔽，同时为那些不重要节点分配更高的屏蔽概率，其中重要性通过节点中心性来衡量。</p>
<p>另一方面，节点特征 shuffle（NFS）[13]、[71]、[72]
不是屏蔽特征矩阵的一部分，而是部分地、按行地扰动节点特征矩阵。换句话说，与输入图相比，增强图中的几个节点被放置到其他位置，如下所示：
<span class="math display">\[
t_i(\mathbf{X})=[\mathbf{X}]_{\widetilde{\mathcal{V}}}, \tag{19}
\]</span> 其中 <span class="math inline">\([\cdot]_{v_i}\)</span>
是一个选取函数，它从节点特征矩阵中索引 <span
class="math inline">\(v_i\)</span> 的特征向量 <span
class="math inline">\(\tilde{\mathcal{V}}\)</span>
表示部分打乱的节点集。</p>
<h4 id="topological-augmentations">Topological augmentations</h4>
<p>从结构角度来看图增强主要作用于图邻接矩阵，其公式如下： <span
class="math display">\[
\tilde{\mathcal{G}}^{(i)}=(\tilde{\mathcal{A}}^{(i)},\mathbf{X})=(t_i(\mathbf{A}),\mathbf{X}),
\tag{20}
\]</span> 其中 <span class="math inline">\(t_i(\cdot)\)</span>
通常放置在图邻接矩阵上。对于这个方法分支，边缘修改（EM）[9]、[38]、[51]、[68]、[73]、[74]
是最常见的方法之一，它通过以下方式部分扰乱给定的图邻接关系：随机删除和插入一部分边。我们将这个过程定义如下：
<span class="math display">\[
t_i(\mathbf{A})=\mathbf{M_1}\circ\mathbf{A}+\mathbf{M_2}\circ(1-\mathbf{A}),
\tag{21}
\]</span> 其中 <span class="math inline">\(\bf{M}_1\)</span> 和 <span
class="math inline">\(\bf{M}_2\)</span>
是边丢弃和插入矩阵。具体地，<span
class="math inline">\(\bf{M}_1\)</span> 和 <span
class="math inline">\(\bf{M}_2\)</span> 是通过随机屏蔽 <span
class="math inline">\(\bf{A}\)</span> 和 <span
class="math inline">\((1-\bf{A})\)</span> 中值为 1
的部分元素生成的。与节点特征掩蔽类似，<span
class="math inline">\(\bf{M}_1\)</span> 和 <span
class="math inline">\(\bf{M}_2\)</span> 也可以自适应计算
[69]。此外，可以基于对抗性学习生成边缘修改矩阵
[18]、[75]，这增加了学习表示的鲁棒性。</p>
<p>与边修改不同，图扩散（GD）[76] 是另一种类型的结构增强
[14]、[68]，它通过计算权重将节点与其间接连接的邻居连接起来，将全局拓扑信息注入给定的图邻接中:
<span class="math display">\[
t_i(\mathbf{A})=\sum_{k=0}^\infty\Theta_k\mathbf{T}^k, \tag{22}
\]</span> 其中 <span class="math inline">\(\Theta\)</span> 和 <span
class="math inline">\(\bf{T}\)</span>
分别是加权系数和转移矩阵。具体来说，上述扩散公式有两个实例化 [14]。令
<span class="math inline">\(\Theta_k = \frac{e^{-\iota}t^k}{k!}\)</span>
且 <span class="math inline">\(\bf{T} =
\bf{AD}−1\)</span>，我们有基于热核的图扩散： <span
class="math display">\[
t_i(\mathbf{A})=\exp{(\iota\mathbf{AD}^{-1}-\iota)}, \tag{23}
\]</span> 其中 <span class="math inline">\(\iota\)</span>
表示扩散时间。类似地，基于个性化 PageRank 的图扩散定义如下：<span
class="math inline">\(\Theta_k=\beta(1-\beta)^k\)</span> 且 <span
class="math inline">\(\bf{T}=\bf{D}^{-\frac{1}{2}}\bf{A}\bf{D}^{-\frac{1}{2}}\)</span>：
<span class="math display">\[
t_i(\mathbf{A})=\beta\big(\mathbf{I}-(1-\beta)\mathbf{D}^{-\frac12}\mathbf{A}\mathbf{D}^{-\frac12}\big),
\tag{24}
\]</span> 其中 <span class="math inline">\(\beta\)</span>
表示可调传送概率。</p>
<h4 id="hybrid-augmentations">Hybrid augmentations</h4>
<p>值得注意的是，给定的图增强可能不仅涉及属性增强，还同时涉及拓扑增强，我们将其定义为混合增强并表示为：
<span class="math display">\[
\tilde{\mathcal{G}}^{(i)}=(\tilde{\mathbf{A}}^{(i)},\tilde{\mathbf{X}}^{(i)})=(t_i(\mathbf{A},\mathbf{X})).
\tag{25}
\]</span> 在这种情况下，增强 <span
class="math inline">\(t_i(\cdot)\)</span>
被放置在节点特征和图邻接矩阵上。子图采样（SS）[14]、[16]、[74]、[77]
是一种典型的混合图增强，类似于图像裁剪。具体来说，它将一部分节点及其底层链接采样为增强图实例：
<span class="math display">\[
t_i(\mathbf{A},\mathbf{X})=[(\mathbf{A},\mathbf{X})]_{\mathcal{V}^{\prime}\in\mathcal{V}},
\tag{26}
\]</span> 其中 <span class="math inline">\(\mathcal{V}&#39;\)</span>
表示 <span class="math inline">\(\mathcal{V}\)</span> 的子集，<span
class="math inline">\([\cdot]_{\mathcal{V}&#39;}\)</span>
是一个选取函数，用于索引具有节点集V'的子图的节点特征和邻接矩阵。关于
<span class="math inline">\(\mathcal{V}&#39;\)</span>
的生成，已经提出了几种方法，例如均匀采样 [74]、基于随机游走的采样 [15]
和基于 top-k 重要性的采样 [77]。</p>
<p>除了子图采样之外，大多数图对比方法都严重依赖于结合上述策略的混合增强。例如，GRACE
[33] 应用边缘丢弃和节点特征掩蔽，而 MVGRL [14]
采用图扩散和子图采样来生成不同的对比视图。</p>
<h3 id="graph-contrastive-learning">Graph Contrastive Learning</h3>
<p>由于对比学习的目的是最大化具有相似语义信息的实例之间的
MI，因此可以构造各种 pretext tasks
来丰富这些信息的监督信号。关于等式（5）中 pretext 解码器 <span
class="math inline">\(p_{\phi}(\cdot)\)</span>
的表述，我们将现有工作分为两个主流：同尺度对比学习和跨尺度对比学习。方法的前一个分支以相同的尺度（例如，节点与节点）区分图实例，而第二种类型的方法则跨多个粒度（例如，节点与图）进行对比。图
8 和表 3 分别提供了基于对比度的方法的流程和摘要。</p>
<figure>
<img
src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240705213824252.png"
alt="Fig. 8" />
<figcaption aria-hidden="true">Fig. 8</figcaption>
</figure>
<figure>
<img
src="https://blog-img-1304093596.cos.ap-shanghai.myqcloud.com/undefinedimage-20240705213919882.png"
alt="Table 3" />
<figcaption aria-hidden="true">Table 3</figcaption>
</figure>
<h4 id="same-scale-contrast">Same-Scale Contrast</h4>
<p>根据对比的尺度，我们进一步将相同尺度的对比学习方法分为两个子类型：节点级和图级。</p>
<h5 id="node-level-same-scale-contrast">Node-Level Same-Scale
Contrast</h5>
<p>该类别下的早期方法 [30]、[31]、[78]、[79]
主要是学习节点级表示，并建立在具有相似上下文信息的节点应该共享节点级表示的思想之上。类似的表述。换句话说，这些方法试图将节点的表示拉近其上下文邻域，而不依赖于复杂的图增强。我们将它们表述如下：
<span class="math display">\[
\theta^*=\arg\min_\theta\frac1{|\mathcal{V}|}\sum_{v_i\in\mathcal{V}}\mathcal{L}_{con}\big(p\big([f_\theta(\mathbf{A},\mathbf{X})]_{v_i},[f_\theta(\mathbf{A},\mathbf{X})]_{v_c}\big)\big),
\tag{27}
\]</span> 其中 <span class="math inline">\(v_c\)</span> 表示 <span
class="math inline">\(v_i\)</span> 的上下文节点，例如，从 <span
class="math inline">\(v_i\)</span>
开始的随机游走中的相邻节点。在这些方法中，pretext
鉴别器（即解码器）通常是点积，因此我们在等式中省略其参数。具体来说，DeepWalk
[30] 引入了一种基于随机游走 (RW)
的方法来提取无属性图中选定节点周围的上下文信息。它最大化了 Skip-Gram
模型 [26]、[27] 中同一路径内节点的共现（即，由二元分类器测量的
MI）。类似地，node2vec [31] 采用偏置 RW
来探索更丰富的节点上下文信息并产生更好的性能。另一方面，GraphSAGE [78]
将上述两种方法扩展到属性图，并提出了一种新颖的 GNN
以归纳方式计算节点嵌入，它也应用 RW
作为其内部采样策略。在异构图上，SELAR [80]
对元路径进行采样以捕获上下文信息。它由一个主链接预测任务和几个元路径预测辅助任务组成，以强制同一元路径内的节点共享更紧密的语义信息。</p>
<p>与上述方法不同，现代节点级同尺度对比方法正在通过各种图增强来探索更丰富的底层语义信息，而不是限制在子图采样：
<span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{con}\Big(p_\phi(f_\theta(\tilde{\mathbf{A}}^{(1)},\tilde{\mathbf{X}}^{(1)}),f_\theta(\tilde{\mathbf{A}}^{(2)},\tilde{\mathbf{X}}^{(2)}))\Big),
\tag{28}
\]</span> 其中 <span class="math inline">\(\tilde{\bf{A}}^{(1)}\)</span>
和 <span class="math inline">\(\tilde{\bf{A}}^{(2)}\)</span>
是两个增强图邻接矩阵。类似地 <span
class="math inline">\(\tilde{\bf{X}}^{(1)}\)</span> 和 <span
class="math inline">\(\tilde{\bf{X}}^{(2)}\)</span>
是不同增强下的两个节点特征矩阵。上式中的判别器 <span
class="math inline">\(p_{\phi}(\cdot)\)</span> 可以是 <span
class="math inline">\(\phi\)</span>
的参数（例如，双线性变换），也可以不是（例如，余弦相似度，其中 <span
class="math inline">\(\Phi
=\emptyset\)</span>）。在这些方法中，大多数都处理属性图：GRACE [33]
采用两种图增强策略，即节点特征掩蔽和边缘丢弃，来生成两个对比视图，然后将两个图视图之间相同节点的表示拉得更近同时将其余节点推开（即视图内和视图间负例）。基于该框架，GCA
[69]
进一步引入了基于底层图属性的图结构数据的自适应增强，从而获得更具竞争力的性能。不同的是，GROC
[18] 提出了图链接的对抗性增强，以提高学习节点表示的鲁棒性。由于 SimCLR
[24] 在视觉领域的成功，GraphCL(N) [81]
进一步将这一思想扩展到图结构数据，它依靠节点特征掩蔽和边缘修改来生成两个对比视图，然后不同视图内的两个目标节点之间的
MI 最大化。 CGPN [82]
将泊松学习引入节点级对比学习，这有利于极其有限的标记数据下的节点分类任务。在普通图上，GCC
[15] 利用 RW 作为增强来提取节点的上下文信息，然后利用 MoCo [23]
的对比框架将节点的表示与其对应的表示进行对比。另一方面，HeCo [83]
在异构图上进行对比，其中从网络模式和元路径两个角度生成两个对比视图，而编码器通过最大化同一节点的嵌入之间的
MI 来训练两种观点。</p>
<p>除了那些依赖精心设计的负样本的方法之外，像 BGRL [84]
这样的方法建议对图实例本身进行对比，从而减轻对故意设计的负采样策略的依赖。
BGRL 利用了 BYOL [25] 中知识蒸馏的优势，其中引入了动量驱动的 Siamese
架构来指导监督信号的提取。具体来说，它使用节点特征屏蔽和边缘修改作为增强，并且
BGRL 的目标与 BYOL 相同，其中来自在线和目标网络的节点表示之间的 MI
最大化。 SelfGNN [85] 采用相同的技术，不同之处在于 SelfGNN
使用其他图增强，例如图扩散 [76]、节点特征分割、标准化和粘贴。除了 BYOL
之外，Barlow Twins [86]
是另一种类似但强大的方法，不使用负样本来防止模型崩溃。 G-BT [87]
扩展了图数据分析的冗余减少原则，其中优化目标是最小化通过两个增强图视图的节点嵌入生成的身份和互相关度量之间的差异。另一方面，MERIT
[68] 提出结合 Siamese 知识蒸馏和传统图对比学习的优点。它利用 SimSiam
[88]
中的自蒸馏框架，同时引入额外的节点级负数，以进一步利用底层语义信息并丰富监督信号。</p>
<h5 id="graph-level-same-scale-contrast">Graph-Level Same-Scale
Contrast</h5>
<p>对于同尺度对比下的图级表示学习，区分通常放在图表示上： <span
class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\mathcal{L}_{con}\Big(p_\phi(\tilde{\mathbf{g}}^{(1)},\tilde{\mathbf{g}}^{(2)})\Big),
\tag{29}
\]</span> 其中 <span
class="math inline">\(\tilde{g}^{(i)}=\mathcal{R}(f_{\theta}(\tilde{\bf{A}}^{(i)},\tilde{\bf{X}}^{(i)}))\)</span>
表示增强图 <span
class="math inline">\(\tilde{\mathcal{G}}^{(i)}\)</span> 的表示，<span
class="math inline">\(\mathcal{R}(\cdot)\)</span>
是基于节点表示的生成图级别嵌入的 readout
函数。等式（29）下的方法可以与上述节点级方法共享类似的增强和骨干对比框架。例如，GraphCL(G)
[38] 采用 SimCLR [24]
来形成其对比管道，使两个视图的图级表示更加接近。类似地，DACL [89]
也是基于 SimCLR
构建的，但它设计了一种通用而有效的增强策略，即基于混合的数据插值。
AD-GCL
[75]提出了一种对抗性边缘丢弃机制作为增强，以减少编码器获取的冗余信息量。
JOAO[70]提出了联合增强优化的概念，其中通过联合优化增强选择和对比目标来制定双层优化问题。与
GCC [15] 类似，CSSL [74] 是基于 MoCo [23]
构建的，但它与图级嵌入形成对比。 LCGNN [90]
中也可以找到类似的设计。另一方面，关于知识蒸馏，IGSD [73]正在利用BYOL
[25]的概念，类似于 MERIT [68]。</p>
<h4 id="cross-scale-contrast">Cross-Scale Contrast</h4>
<p>与同等规模的对比图实例不同，该方法分支将区分置于各种图拓扑（例如，节点与图）之间。我们在此类别下进一步构建两个子类，即
c 对比和 context-global 对比。</p>
<h5 id="patch-global-cross-scale-contrast">Patch-Global Cross-Scale
Contrast</h5>
<p>对于节点级表示学习，我们将这种对比定义如下： <span
class="math display">\[
\begin{aligned}\theta^*,\phi^*=\arg\min_{\theta,\phi}\frac1{|\mathcal{V}|}\sum_{v_i\in\mathcal{V}}\mathcal{L}_{con}\Big(p_\phi\Big([f_\theta(\mathbf{A},\mathbf{X})]_{v_i},\mathcal{R}\big(f_\theta(\mathbf{A},\mathbf{X})\big)\Big)\Big),\end{aligned}
\tag{30}
\]</span> 其中 <span class="math inline">\(\mathcal{R}(\cdot)\)</span>
表示我们在上一小节中提到的 readout 函数。在这一类别下，DGI [13]
是第一个提出将节点级嵌入与图级表示进行对比的方法，其目的是最大化不同尺度的这两种表示之间的
MI，以帮助图编码器学习本地和全局语义信息。基于这个想法，GIC [91]
首先根据嵌入对图中的节点进行聚类，然后将节点拉近其相应的聚类摘要，同时使用
DGI
目标进行优化。除了属性图之外，异构图上的一些工作也基于类似的模式：HDGI
[92] 可以被视为异构图上的 DGI
的一个版本，其中的区别在于图的最终节点嵌入是通过聚合节点来计算的不同元路径下的表示。类似地，ConCH
[93] 与 DGI
具有相同的目标，并聚合基于元路径的节点表示来计算异构图的节点嵌入。不同的是，DMGI
[94]
将多重图视为多个属性图的组合。对于它们中的每一个，给定选定的目标节点及其关联的关系类型，首先计算特定于关系的节点嵌入。图级表示和此类节点嵌入之间的
MI 被最大化，如 DGI 中一样。 EGI [95]通过强制节点特征
structure-respecting，然后最大化节点嵌入与其周围自我图之间的 MI
来提取高级可转移图知识。在时空图上，STDGI [71] 最大化时间步 <span
class="math inline">\(t\)</span> 处的节点表示与 <span
class="math inline">\(t+1\)</span>
处的原始节点特征之间的一致性，以指导图编码器捕获丰富的语义信息以预测未来的节点特征。</p>
<p>请注意，上述方法没有明确使用任何图增强。对于基于增强的 patch-global
对比方法，我们将等式（30）重新表述如下： <span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\frac{1}{|\mathcal{V}|}\sum_{v_i\in\mathcal{V}}\mathcal{L}_{con}\Big(p_\phi(\tilde{\mathbf{h}}_i^{(1)},\tilde{\mathbf{g}}^{(2)})\Big),
\tag{31}
\]</span> 其中 <span
class="math inline">\(\tilde{h}_i^{(1)}=[f_{\theta}(\tilde{\bf{A}}^{(1)},\tilde{\bf{X}}^{(1)})]\)</span>
是增强视图 1 中节点 <span class="math inline">\(v_i\)</span>
的表示，并且 <span
class="math inline">\(\tilde{g}_i^{(2)}=\mathcal{R}(f_{\theta}(\tilde{\bf{A}}^{(2)},\tilde{\bf{X}}^{(2)}))\)</span>
表示不同增强视图 2 的表示。在这个定义的框架下，MVGRL [14] 首先通过图扩散
[76]
和子图采样生成两个图视图。然后，它通过最大化视图中的节点嵌入与另一个视图的图级表示之间的
MI 来丰富局部和全局监督信号。另一方面，SUBG-CON [77] 继承了 MVGRL
的目标，同时采用了不同的图增强。具体来说，它首先从大规模输入图中提取中心节点的前
<span class="math inline">\(k\)</span>
个信息最丰富的邻居。然后，编码的节点表示进一步打乱以增加 pretext
的难度。在异构图上，SLiCE [96]
将节点拉近最接近的上下文图，而不是明确地将节点与整个图进行对比。此外，SLiCE
通过上下文翻译机制丰富了节点嵌入的本地化信息。</p>
<p>对于基于 patch-global
对比的图级表示学习，我们可以使用等式（30）来表示。 InfoGraph [97] 与 DGI
[13]
共享相似的模式。它将图表示直接与节点嵌入进行对比，以区分节点是否属于给定图。为了进一步推动
InfoGraph、Robinson 等人的对比方法。
[98]提出了一种通用而有效的硬负采样策略，使底层 pretext tasks
的解决更具挑战性。</p>
<h5 id="context-global-cross-scale-contrast">Context-Global Cross-Scale
Contrast</h5>
<p>跨尺度图对比学习类别下的另一种流行设计是 context-global
全局对比，其定义如下： <span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\frac{1}{|\mathcal{S}|}\sum_{s\in\mathcal{S}}\mathcal{L}_{con}\Big(p_\phi(\tilde{\mathbf{h}}_s,\tilde{\mathbf{g}})\Big),
\tag{32}
\]</span> 其中 <span class="math inline">\(S\)</span> 表示增强输入图
<span class="math inline">\(\tilde{\mathcal{G}}\)</span>
中的一组上下文子图，其中增强通常基于此类别下的图采样。在上式中，<span
class="math inline">\(\tilde{h}_s\)</span> 是增强上下文子图 <span
class="math inline">\(s\)</span> 的表示，<span
class="math inline">\(\tilde{g}\)</span> 表示 <span
class="math inline">\(S\)</span> 中所有子图的图级表示。具体来说，我们令
<span
class="math inline">\(\tilde{h}_s=\mathcal{R}([f_{\theta}(\tilde{\bf{A}},\tilde{\bf{X}})]_{v_i
\in s})\)</span>，且 <span
class="math inline">\(\tilde{g}=\mathcal{R}(f_{\theta}(\tilde{\bf{A}},\tilde{\bf{X}}))\)</span>。然而，对于某些方法，例如
[99] 和 [100]，图级表示是在原始输入图上计算的，其中 <span
class="math inline">\(\tilde{g}=\mathcal{R}(f_{\theta}(\bf{A},\bf{X}))\)</span>。其中，BiGI
[99] 是二分图上的节点级表示学习方法，继承了DGI [13]
的对比模式。具体来说，它首先通过聚合两种类型的节点嵌入来计算输入图的图级表示。然后，它对原始图进行采样，然后计算两个节点之间的目标边的局部上下文表示。
BiGI 的优化目标是最大化这种局部上下文和全局表示之间的
MI，然后训练好的图编码器可以用于各种边缘级下游任务。为了学习图级嵌入，HTC
[100] 最大化全图表示和上下文嵌入（采样子图的聚合）之间的 MI。与 HTC
类似但不同的是，MICRO-Graph [101]
提出了一种不同但新颖的基于主题学习的采样作为隐式增强，以生成几个语义信息子图，其中每个子图的嵌入更接近整个图的表示。考虑到图级表示基于增强输入图的场景，作为等式（32）所示的默认设置，SUGAR
[102] 首先从给定图中采样 <span class="math inline">\(n\)</span>
个子图，然后提出基于强化学习的 top-k 采样策略，在大小为 <span
class="math inline">\(n\)</span> 的候选集中选择 <span
class="math inline">\(n&#39;\)</span>
个信息丰富的子图。最后，在子图嵌入和草图表示之间建立 SUGAR
的对比，即通过组合这 <span class="math inline">\(n&#39;\)</span>
个子图生成的图。</p>
<h3 id="mutual-information-estimation">Mutual Information
Estimation</h3>
<p>大多数基于对比的方法依赖于两个或多个实例之间的 MI
估计。具体来说，从正池中采样的一对实例的表示被拉近，而来自负集的对应实例被推开。给定一对实例
(<span class="math inline">\(x_i, x_j\)</span>)，我们让 (<span
class="math inline">\(h_i, h_j\)</span>) 表示它们的表示。因此，(<span
class="math inline">\(i, j\)</span>) 之间的 MI 由 [103] 给出： <span
class="math display">\[
\begin{aligned}
\mathcal{MI}(\mathbf{h}_i,\mathbf{h}_j)=KL\big(P(\mathbf{h}_i,\mathbf{h}_j)||P(\mathbf{h}_i)P(\mathbf{h}_j)\big)=\mathbb{E}_{P(\mathbf{h}_i,\mathbf{h}_j)}\Big[\log\frac{P(\mathbf{h}_i,\mathbf{h}_j)}{P(\mathbf{h}_i)P(\mathbf{h}_j)}\Big],
\end{aligned} \tag{33}
\]</span> 其中 <span class="math inline">\(KL(\cdot)\)</span> 表示
Kullback-Leibler 散度，最终目标是训练编码器区分来自联合密度 <span
class="math inline">\(P(h_i, h_j)\)</span> 的一对实例和来自边缘密度
<span class="math inline">\(P(h_i)\)</span> 和 <span
class="math inline">\(P(h_j)\)</span>。在本小节中，我们定义了从等式（33）导出的两种常见形式的下界和三种特定形式的无界
MI 估计量。</p>
<h4 id="jensen-shannon-estimator">Jensen-Shannon Estimator</h4>
<p>尽管 Donsker-Varadhan 表示提供了 KL 散度的严格下界 [36]，但
Jensen-Shannon 散度 (JSD) 在图对比学习中更常见，它提供了 MI
的下界和更有效的估计。我们基于它定义对比损失如下： <span
class="math display">\[
\begin{aligned}
\mathcal{L}_{con}\big(p_{\phi}(\mathbf{h}_{i},\mathbf{h}_{j})\big)=-\mathcal{MI}_{JSD}(\mathbf{h}_{i},\mathbf{h}_{j})
=\mathbb{E}_{\mathcal{P}\times\widetilde{\mathcal{P}}}\bigg[\log\left(1-p_\phi(\mathbf{h}_i,\mathbf{h}^{\prime}_j)\right)\bigg]-\mathbb{E}_{\mathcal{P}}\bigg[\log\left(p_\phi(\mathbf{h}_i,\mathbf{h}_j)\right)\bigg].
\end{aligned}\tag{34}
\]</span> 在上式中，<span class="math inline">\(h_i\)</span> 和 <span
class="math inline">\(h_j\)</span> 从相同的分布 <span
class="math inline">\(\mathcal{P}\)</span> 中采样，<span
class="math inline">\(h&#39;_j\)</span> 从不同的分布 <span
class="math inline">\(\mathcal{P}\)</span> 中采样。对于判别器 <span
class="math inline">\(p_{\phi}(\cdot)\)</span>，可以取多种形式，其中通常采用双线性变换
[104]，即 <span class="math inline">\(p_{\phi}(h_i,h_j)=h_i^T\Phi
h_j\)</span>，如 [13]、[14]、[102] 中。具体来说，通过令 <span
class="math inline">\(p_{\phi}(h_i,h_j)=\text{sigmoid}(p&#39;_{\phi}(h_i,h_j))\)</span>，等式（34）可以以
InfoGraph [97] 中的另一种形式表示。</p>
<h4 id="noise-contrastive-estimator">Noise-Contrastive Estimator</h4>
<p>与 JSD 类似，噪声对比估计器（又名 InfoNCE）提供了一个下限 MI
估计，它自然由一个正例对和 <span class="math inline">\(N\)</span>
个负例对组成 [36]。基于 InfoNCE 的对比损失定义如下： <span
class="math display">\[
\begin{aligned}\mathcal{L}_{con}\Big(p_{\phi}(\mathbf{h}_{i},\mathbf{h}_{j})\Big)=-\mathcal{MI}_{NCE}(\mathbf{h}_{i},\mathbf{h}_{j})=-\mathbb{E}_{\mathcal{P}\times\widetilde{\mathcal{P}}^N}\Big[\log\frac{e^{p_{\phi}(\mathbf{h}_{i},\mathbf{h}_{j})}}{e^{p_{\phi}(\mathbf{h}_{i},\mathbf{h}_{j})}+\sum_{n\in
N}e^{p_{\phi}(\mathbf{h}_{i},\mathbf{h}_{n}^{\prime})}}\Big],\end{aligned}
\tag{35}
\]</span> 其中判别器 <span
class="math inline">\(p_{\phi}(\cdot)\)</span> 可以是与温度参数 <span
class="math inline">\(\tau\)</span> 的点积，即 <span
class="math inline">\(p(h_i,h_j)=h_i^Th_j/\tau\)</span> ，如 GRACE [33]
和 GCC [15] 中。</p>
<h4 id="triplet-loss">Triplet Loss</h4>
<p>除了上述两个下界 MI
估计器之外，还可以采用三元组边际损失来估计数据实例之间的
MI。然而，最小化该损失并不能保证 MI 最大化，因为它不能代表 MI
的下限。正式地，Jiao 等人 [77] 将该损失函数定义如下： <span
class="math display">\[
\mathcal{L}_{con}\Big(p(\mathbf{h}_i,\mathbf{h}_j)\Big)=\mathbb{E}_{\mathcal{P}\times\widetilde{\mathcal{P}}}\Big[\max\Big[p_\phi(\mathbf{h}_i,\mathbf{h}_j)-p_\phi(\mathbf{h}_i,\mathbf{h&#39;}_j)+\epsilon,0\Big]\Big],
\tag{36}
\]</span></p>
<p>其中 <span class="math inline">\(\epsilon\)</span> 是 margin
值，判别器 <span class="math inline">\(p(h_i,h_j)=1/1 +
e^{(-h_i^Th_j)}\)</span>。</p>
<h4 id="byol-loss">BYOL Loss</h4>
<p>对于受 BYOL [25] 启发且不依赖负样本的方法，例如 BGRL
[25]，它们的目标函数也可以解释为 non-bound MI 估计器。</p>
<p>给定 <span class="math inline">\(h_i, h_j \sim
\mathcal{P}\)</span>，我们将损失定义如下： <span class="math display">\[
\mathcal{L}_{con}\Big(p(\mathbf{h}_i,\mathbf{h}_j)\Big)=\mathbb{E}_{\mathcal{P}\times\mathcal{P}}\Big[2-2\cdot\frac{\left[p_\psi(\mathbf{h}_i)\right]^T\mathbf{h}_j}{\left\|p_\psi(\mathbf{h}_i)\right\|\left\|\mathbf{h}_j\right\|}\Big],
\tag{37}
\]</span> 其中 <span class="math inline">\(p_{\psi}\)</span>
表示连体网络中由 <span class="math inline">\(\psi\)</span>
参数化的在线预测器，它可以防止模型因动量编码器、停止梯度等其他机制而崩溃。特别是，这种情况下的
pretext 解码器表示两个实例之间的均方误差，它已在上面的等式中展开。</p>
<h4 id="barlow-twins-loss">Barlow Twins Loss</h4>
<p>与 BYOL
类似，该目标减轻了对负样本的依赖，但实现起来更简单，这是由冗余减少原则驱动的。具体来说，给定从分布
<span class="math inline">\(\mathcal{P}\)</span>
采样的一批数据实例的两个视图 <span
class="math inline">\(H^{(1)}\)</span> 和 <span
class="math inline">\(H^{(2)}\)</span>
的表示，我们将这个损失函数定义如下 [86]： <span class="math display">\[
\begin{aligned}
\mathcal{L}_{con}(\mathbf{H^{(1)}},\mathbf{H^{(2)}})=
\mathbb{E}_{\mathcal{B}\sim\mathcal{P}|\mathcal{B}|}\bigg[\sum_{a}(1-\frac{\sum_{i\in\mathcal{B}}\mathbf{H}_{ia}^{(1)}\mathbf{H}_{ia}^{(2)}}{\left\|\mathbf{H}_{ia}^{(1)}\right\|\left\|\mathbf{H}_{ia}^{(2)}\right\|})^{2}
+\lambda\sum_a\sum_{b\neq
a}\Big(\frac{\sum_{i\in\mathcal{B}}\mathrm{H}_{ia}^{(1)}\mathrm{H}_{ib}^{(2)}}{\left\|\mathrm{H}_{ia}^{(1)}\right\|\left\|\mathrm{H}_{ib}^{(2)}\right\|}\Big)^2\bigg],
\end{aligned} \tag{38}
\]</span> 其中 <span class="math inline">\(a\)</span> 和 <span
class="math inline">\(b\)</span> 索引表示向量的维度，<span
class="math inline">\(i\)</span> 索引批次 <span
class="math inline">\(\mathcal{B}\)</span> 内的样本。</p>
<h2 id="hybrid-methods">Hybrid Methods</h2>
<p>与上述仅利用单个 pretext tasks 来训练模型的方法相比，混合方法采用多个
pretext tasks
来更好地利用各种类型监督信号的优势。混合方法以多任务学习方式将各种
pretext tasks
集成在一起，其中目标函数是两个或多个自监督目标的加权和。混合图 SSL
方法的公式为： <span class="math display">\[
\theta^*,\phi^*=\arg\min_{\theta,\phi}\sum_{i=1}^N\alpha_i\mathcal{L}_{ssl_i}\left(f_\theta,p_{\phi_i},\mathcal{D}_i\right),
\tag{39}
\]</span> 其中 <span class="math inline">\(N\)</span> 是 pretext tasks
的数量，<span class="math inline">\(\alpha_i, \mathcal{L}_{ssl},
p_{\phi_1}\)</span> 和 <span
class="math inline">\(\mathcal{D}_i\)</span> 分别是第 <span
class="math inline">\(i\)</span> 个 pretext tasks
的权衡权重、损失函数、pretext 解码器和数据分布。</p>
<p>混合图 SSL 的一个常见想法是将不同的基于生成的任务组合在一起。 GPT-GNN
[9] 将特征和结构生成集成到 GNN
的预训练框架中。具体来说，对于每个采样的输入图，它首先随机屏蔽一定数量的边和节点。然后，使用两个生成任务同时训练编码器：属性生成（使用
MSE 损失重建屏蔽特征）和边缘生成（使用对比损失预测屏蔽边缘）。
Graph-Bert [39] 结合了归因任务和结构 pretext tasks
来预训练图转换器模型。具体来说，节点原始属性重建从节点的嵌入中重建原始特征，而图结构恢复旨在通过余弦相似度解码器恢复两个节点之间的图扩散值。
PT-DGNN [105] 将属性生成和结构生成相结合的思想扩展到动态图的预训练
GNN。此外，Manessi 等人 [45] 建议用三种类型的特征生成任务来训练
GNN。</p>
<p>另一个想法是将生成性 pretext tasks 和对比性 pretext tasks
整合在一起。 GMI [41]采用联合学习目标进行图表示学习。在 GMI
中，对比学习目标（即特征 MI）是通过 JSD
估计器最大化节点嵌入和邻居特征之间的一致性，生成目标（即边
MI）是最小化邻接矩阵的重建误差有 BCE 损失。 CG3 [106] 联合考虑对比和生成
SSL 来解决半监督节点分类问题。在 CG3 中，建立了两个并行编码器（GCN 和
HGCN）来提供图的局部和全局视图。在对比学习中，InfoNCE
对比损失用于最大化两个视图的节点嵌入之间的
MI。在生成学习中，生成解码器用于根据两个视图嵌入的串联来重建拓扑结构。
MVMI-FT [107]
提出了一种跨尺度对比学习框架，该框架从不同视图学习节点表示，并且还使用图重建模块来学习跨视图共享信息。</p>
<p>由于不同类型的对比可以从不同的角度提供监督信号，因此一些方法将多个基于对比的任务集成在一起。
GraphLoG [108]由三个对比目标组成：子图与子图、图与图以及图与上下文。
InfoNCE 损失充当三种类型对比的 MI 估计器。 HDMI [109]
混合了同尺度和跨尺度对比学习，将给定的多路网络分解为多个属性图。对于它们中的每一个，HDMI
提出了三个不同的目标，以最大化原始节点特征、节点嵌入和图级表示之间的
MI。 G-Zoom
[110]使用三个尺度的同尺度对比来学习表示，从多个角度提取有价值的线索。
LnL-GNN [111] 利用双层 MI
最大化来分别通过社区检测和基于特征的聚类获得的局部和非局部邻域进行学习。</p>
<p>不同的基于辅助属性的任务也可以集成到混合方法中。Hu 等人 [50]
提出同时预训练多个任务的
GNN，以捕获可转移的通用图结构，包括去噪链接重建、中心性得分排名和聚类保留。在
GROVER [10]
中，作者同时在节点级别（上下文属性预测）和图级别（主题预测）中使用辅助属性分类任务预训练
GNN Transformer 模型。Kou 等人
[112]将结构生成、特征生成和辅助属性分类任务混合到聚类模型中。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Deep-Learning/" class="category-chain-item">Deep Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Graph/" class="print-no-link">#Graph</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>论文日记 2024 Graph Self-Supervised Learning- A Survey (2021)</div>
      <div>https://blog.lfd.world/2024/07/02/lun-wen-ri-ji-2024-graph-self-supervised-learning-a-survey-2021/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>培根请加蛋</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年7月2日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/07/06/lun-wen-ri-ji-2024-foundation-models-for-time-series-analysis-a-tutorial-and-survey-2024/" title="论文日记 2024 Foundation Models for Time Series Analysis- A Tutorial and Survey (2024)">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">论文日记 2024 Foundation Models for Time Series Analysis- A Tutorial and Survey (2024)</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/06/28/lun-wen-ri-ji-2024-a-continual-learning-survey-defying-forgetting-in-classification-tasks-2021/" title="论文日记 2024 A continual learning survey-Defying forgetting in classification tasks (2021)">
                        <span class="hidden-mobile">论文日记 2024 A continual learning survey-Defying forgetting in classification tasks (2021)</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>







  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
