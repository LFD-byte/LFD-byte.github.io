

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="培根请加蛋">
  <meta name="keywords" content="">
  
    <meta name="description" content="Foundation Models for Time Series Analysis@A Tutorial and Survey">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Diary-2024-Foundation Models for Time Series Analysis@A Tutorial and Survey (2024)">
<meta property="og:url" content="https://blog.lfd.world/2024/07/06/paper-diary-2024-foundation-models-for-time-series-analysis-a-tutorial-and-survey-2024/index.html">
<meta property="og:site_name" content="培根请加蛋">
<meta property="og:description" content="Foundation Models for Time Series Analysis@A Tutorial and Survey">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-07-06T14:12:00.000Z">
<meta property="article:modified_time" content="2024-11-21T07:48:48.878Z">
<meta property="article:author" content="培根请加蛋">
<meta property="article:tag" content="Time-Series">
<meta name="twitter:card" content="summary_large_image">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Paper Diary-2024-Foundation Models for Time Series Analysis@A Tutorial and Survey (2024) - 培根请加蛋</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.lfd.world","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":false,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>培根请加蛋</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Paper Diary-2024-Foundation Models for Time Series Analysis@A Tutorial and Survey (2024)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-07-06 22:12" pubdate>
          2024年7月6日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          14k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          115 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Paper Diary-2024-Foundation Models for Time Series Analysis@A Tutorial and Survey (2024)</h1>
            
            
              <div class="markdown-body">
                
                <h1
id="foundation-models-for-time-series-analysis-a-tutorial-and-survey">Foundation
Models for Time Series Analysis: A Tutorial and Survey</h1>
<h2 id="abstract">Abstract</h2>
<p>时间序列分析是数据挖掘社区中的焦点，是提取对无数实际应用程序至关重要的有价值见解的基石。基础模型（FM）的最新进展从根本上重塑了时间序列分析的模型设计范式，在实践中推动了各种下游任务。这些创新方法通常利用预先训练或微调的
FM
来利用为时间序列分析量身定制的通用知识。本次调查旨在为时间序列分析提供
FM 的全面且最新的概述。虽然之前的调查主要关注 FM
在时间序列分析中的应用或流程方面，但它们往往缺乏对阐明 FM
为何以及如何有益于时间序列分析的基本机制的深入了解。为了解决这一差距，我们的调查采用了以方法论为中心的分类，描述了时间序列
FM
的各种关键要素，包括模型架构、预训练技术、适应方法和数据模式。总体而言，这项调查旨在巩固与时间序列分析相关的
FM 的最新进展，强调其理论基础、最近的发展进展以及未来探索的途径。</p>
<p><strong>Keywords:</strong> Time series, foundation model, deep
learning</p>
<h2 id="introduction">Introduction</h2>
<p>时间序列数据的特点是顺序和时间依赖性，封装了有关不同系统和过程动态的有价值的信息
[45,86,113]。各种时间序列数据（例如股票价格、交通流量、电力）为计算分析带来了独特的挑战和机遇，每种数据都需要量身定制的方法来有效捕获其固有属性。时间序列数据的分析和理解是数据挖掘的重要组成部分，促进许多领域的关键见解和决策
[46, 94]，包括金融 [19, 108]、医疗保健 [52, 62]、云计算 [116] 、环境
[14, 70]、能源 [123] 和城市计算 [79, 88]。</p>
<p>近年来，深度学习的进步，特别是基于 transformer 的模型
[85]，彻底改变了时间序列分析领域 [95]。继开创性工作 [54] 之后，探索
transformer 在时间序列分析中的应用的研究兴趣激增
[101,118,119]。深度学习和 transformer
背后的动机在于它们能够从原始数据中自动学习综合表示，从而捕获复杂的非线性关系和时间依赖性，而无需手动特征工程。与众多时间序列应用程序中的传统统计方法相比，这种功能可以显着提高性能。基础模型（foundation
models, FM），例如自然语言处理（NLP）中的大语言模型（LLM）[117]
和计算机视觉（CV）中的高级模型
[2]，已经成为能够实现在各自领域最先进技术的强大范式。这些 FM
的成功可归因于它们能够利用大量数据来训练通用表示，随后对其进行微调，甚至以
zero-shot
的方式直接部署它们，以在各种下游任务中表现出色。这种方法不仅节省了特定任务模型开发的需要，而且概括了对世界的广泛理解，赋予这些模型卓越的多功能性和效率[6,
49]。</p>
<p>受 FM 在 CV 和 NLP
等广泛领域取得的卓越成就的启发，时间序列基础模型（TSFM）的概念作为时间序列分析的一个有前途的方向而受到关注。
TSFM
旨在利用基础模型范式的力量来开发能够熟练理解和预测跨不同领域的时间序列数据的通用模型。通过利用大规模时间序列数据集，TSFM
有望在一系列时间序列任务上获得卓越的性能，并提供一个统一的框架来加速该领域的研究和应用开发。</p>
<p>尽管 TSFM 前景广阔且发展迅速，但之前的文献中明显缺乏从方法论角度对
TSFM 进行系统分析。如表 1 所示，现有研究主要集中在 TSFM 的数据视角 [48]
或 pipeline 视角
[45]。为了弥补这一差距，本调查旨在为学习各种时间序列的基础模型提供全面的方法分析。这次检查将集中在审查他们的模型架构、预训练技术、适应方法和数据模式。通过这一努力，我们试图阐明
TSFM
核心要素的整体情况，从而增强对其功效背后的基本原理以及驱动其在时间序列分析中的巨大潜力的机制的理解。</p>
<p>与之前的调查相比，本手稿包含了最广泛的时间序列数据类型（见表
1）、空间时间序列以及其他类型，例如轨迹和事件。我们在图 1
中进一步总结了当前 TSFM 的发展路线图，以促进对 TSFM
动态和不断发展的格局的进一步创新和理解。简而言之，我们的主要贡献在于三个方面：</p>
<ul>
<li><strong>全面且最新的调查</strong>
我们针对各种时间序列的基础模型提供全面且最新的调查，包括标准时间序列、空间时间序列和其他类型（即轨迹和事件）。</li>
<li><strong>以方法论为中心的新颖分类法</strong>
我们引入了一种新颖的分类法，它从方法论的角度对 TSFM
进行了第一次全面的分析，使人们能够充分理解 FM
为何以及如何在时间序列数据中取得令人钦佩的性能的机制。</li>
<li><strong>未来的研究机会</strong>
我们讨论并强调了使用基础模型增强时间序列分析的未来途径，新兴研究人员可以更深入地研究该领域。</li>
</ul>
<h2 id="background">Background</h2>
<p><strong>Foundation Models</strong>
基础模型（FM），也称为大型预训练模型，是一类在大量数据上进行预训练的深度模型，因此配备了广泛的常识和模式。为此，这些模型可以作为跨不同领域的各种任务的通用起点。具体来说，FM
可以使用相对少量的特定任务数据进行微调或适应特定任务，展现出显着的灵活性和效率。在
CV 中，文本提示模型 CLIP [73] 和视觉提示模型 SAM [51] 等 FM
推动了图像识别、对象检测等领域的进步。在 NLP 中，BERT [25] 和 GPT-3 [8]
等 FM 彻底改变了文本理解和生成任务。受到 FM
在上述领域取得的巨大成功的启发，本次调查深入研究了这些模型在时间序列分析领域的使用。</p>
<p>具体来说，我们从方法论的角度研究
TSFM：基础模型的组成部分包括数据模态、架构、预训练和适应技术：1）数据模态是指用于模型训练的数据类型，从单一模态，例如时间序列、文本、图像和音频到多模态；
2）架构是指采用深度神经网络作为 FM 的骨干，Transformers [85, 95]
因其能够有效处理顺序数据而成为流行的选择；
3）预训练涉及如何使用监督或自监督学习在大型、多样化的数据集上训练模型以获得对数据的广泛理解；
4）采用微调或小样本学习等适应方法，使预先训练的 FM
适应特定任务。这种全面的 FM
框架，涵盖从数据模态到适应的范围，有助于理解在时间序列分析中使用它们。</p>
<p><strong>Categories of Time
Series</strong>。时间序列通常被描述为数据点的有序序列。图 2
说明了本次调查中讨论的各种类型的时间序列，包括标准时间序列、空间时间序列、轨迹和事件。请注意，轨迹和事件可以被视为时间序列，因为每个数据点都与特定的时间戳（和位置）相关联，从而允许使用异常检测等时间序列技术进行分析。这些时间序列的公式如下。</p>
<p><em>Definition 1 (Standard Time Series)</em>
标准时间序列定义为按时间排序的 <span class="math inline">\(T\)</span>
个数据点的序列。可以表示为 <span class="math inline">\(\text{X}=\{
x_1,x_2.\cdots,x_T \} \in \mathbb{R}^{T \times D}\)</span>，其中 <span
class="math inline">\(x_t \in \mathbb{R}^D\)</span> 是时间步 <span
class="math inline">\(t\)</span> 的数据点，<span
class="math inline">\(D\)</span> 是每个数据点的维度。当 <span
class="math inline">\(D=1\)</span> 时，<span
class="math inline">\(\text{X}\)</span> 称为单变量时间序列，当 <span
class="math inline">\(D&gt;1\)</span> 时，<span
class="math inline">\(\text{X}\)</span> 称为多元时间序列。</p>
<p><em>Definition 2 (Spatial Time Series)</em>
它是指同时具有时间和空间维度的数据点序列，可以表示为 <span
class="math inline">\(\mathcal{X}=\{ \text{X}_1,
\text{X}_2,\cdots,\text{X}_T \} \in \mathbb{R}^{N \times T \times
D}\)</span>，其中 <span class="math inline">\(\text{X}_t \in
\mathbb{R}^{N \times D}\)</span> 表示生成的信号由 <span
class="math inline">\(N\)</span> 个传感器组成，每个传感器都有 <span
class="math inline">\(D\)</span> 个特征。此外，<span
class="math inline">\(N\)</span>
个传感器通常与空间相关性相关联，根据空间相关性，空间时间序列可以进一步分为两个子类型：i）时空图，当这些传感器的空间相关性由具有相邻矩阵的图
<span class="math inline">\(G\)</span> 描述时 <span
class="math inline">\(\text{A} \in \mathbb{R}^{N \times N}\)</span>; ii)
时空栅格，当传感器在地理空间中均匀分布为网格时。</p>
<p><em>Definition 3 (Trajectory)</em>
轨迹是一系列带有时间戳的位置，描述了对象在地理空间中的运动。可以表示为
<span class="math inline">\(\mathcal{T}=\{ (l_1,l_2,\cdots,l_T) \} \in
\mathbb{R}^{T \times 2}\)</span>，其中 <span
class="math inline">\(l_t\)</span> 表示物体在时间 <span
class="math inline">\(t\)</span>
的位置，用二维坐标即纬度和经度表示。</p>
<p><em>Definition 4 (Event Sequence)</em>
事件序列是一组按时间顺序排列的事件，描述特定上下文中动作或事件的进展。它可以形式化为
<span class="math inline">\(\mathcal{E}=\{ (e_1,t_1),(e_2,t_2), \dots,
(e_n,t_n) \}\)</span>，其中 <span class="math inline">\(e_i\)</span>
是由谓词参数结构描述的事件，该结构捕获发生的性质，<span
class="math inline">\(t_i\)</span> 表示 <span
class="math inline">\(e_i\)</span> 发生时的时间戳。</p>
<h2 id="taxonomy">Taxonomy</h2>
<p>所提出的分类法如图 3 所示，相关工作见表
2。所提出的分类法展开了结构化且全面的分类，以增强对时间序列分析基础模型的理解。它分为四个层次结构，从数据类别开始，然后是模型架构、预训练技术，最后是应用领域。与以前的分类法不同，我们的分类法通过从方法论角度更深入地研究基础模型来区分自己，并重点关注其架构设计、预训练和适应技术。这种以方法为中心的观点对于研究人员来说至关重要，它为基础模型为何以及如何在时间序列分析中表现出巨大潜力的机制提供了宝贵的见解。</p>
<p>深入研究数据类别的细节，我们将时间序列数据分为三种不同的类型：标准时间序列、空间时间序列和其他类型，其中包括轨迹数据和事件数据。标准时间序列数据以其顺序和时间依赖性为特征，构成了传统时间序列分析的支柱。另一方面，空间时间序列数据通过合并地理或空间信息引入了额外的复杂性，这使得它们对于城市计算和环境监测的应用至关重要。最后，“其他”类别，包括轨迹和事件数据，代表时间起着关键作用的不同数据集，例如物体随时间的移动或特定事件的发生，为时间序列分析提供了更广阔的视角。</p>
<p>从方法论的角度来看：i）关于模型架构，拟议的分类法强调了三个主要类别：基于
Transformer 的模型、非基于 Transformer 的模型和基于扩散的模型。基于
Transformer
的模型利用自注意力机制来捕获时间序列内的远程依赖关系，在处理顺序数据方面提供显着的优势。基于非
Transformer
的模型具有多样化的架构，可以通过有效处理时间模式来满足各种时间序列任务。基于扩散的模型是一种新颖的补充，它采用随机过程对数据生成过程进行建模，为时间序列分析提供了创新的解决方案。
ii）在预训练技术方面，所提出的分类法将其分为完全监督方法和自监督方法，后者包括对比方法、生成方法和混合方法。此分类显示了如何使用或不使用标签来训练不同的
FM。 iii）适应策略，例如零样本学习、prompt 工程、tokenization
和微调，进一步例证了 FM 在定制特定时间序列应用方面的多功能性。</p>
<h2 id="data-perspective">Data Perspective</h2>
<p>在本节中，我们从各种数据角度探讨 TSFM
的进展：标准时间序列、空间时间序列等。我们进一步将每个小节中的讨论分为面向任务的或通用的基础模型。</p>
<h3 id="standard-time-series">Standard Time Series</h3>
<p>标准时间序列具有多种属性，包括不同的采样率和时间模式，这对开发相关 FM
提出了重大挑战。这些模型旨在识别来自不同来源的广泛时间序列数据中的通用模式，以增强特定任务或进行广泛分析。</p>
<p>现有的大多数尝试都属于面向任务的标准时间序列基础模型的范畴。他们利用单个或多个数据模式来针对特定时间序列任务（通常是预测或分类）构建强大的模型。对于仅涉及单一（即时间序列）模态的模型，它们可以从头开始开发，也可以在来自其他领域（如大语言或视觉模型）的现有预训练模型上开发
[117]。</p>
<p>在第一组中，Lag-Llama [75] 和 TimeGPT-1 [35]
代表了预测基础模型的开创性努力。这两个模型都对跨多个领域的大量时间序列数据进行了预训练。
Lag-Llama 采用仅解码器的 transformer 架构，利用 lags 作为协变量，而
TimeGPT1 具有具有多个 transformer
层的编码器-解码器结构，有助于高效的零样本预测。另一个值得注意的贡献是
TTMs [31]，这是最近在 TSMixer [30]
上创建与领域无关的预测模型的努力，该模型本身是在来自不同领域的不同时间序列数据集上进行预训练的。与
Lag-Llama 的方法相呼应，TimesFM [24]
作为一个仅解码器的模型出现，表现出强大的零样本预测能力。同时，Moirai
[97]
引入了一种方法，该方法采用基于掩码编码器的通用预测转换器，并结合新的预训练数据集
(LOTSA)，其中包含来自 9 个不同领域的 270
亿个观测值。此外，探索还扩展到了 TimeGrad [76] 和 TransFusion [82]
等扩散模型，这些模型主要侧重于优化数据似然的变分界限，将白噪声转换为目标分布的有意义的样本。</p>
<p>从头开始进行预训练可能成本高昂，这刺激了替代方法的开发，这些方法利用其他领域的预训练模型，例如大语言、视觉和声学模型。例如，LLM4TS
[11] 和 TEMPO [10] 通过微调 GPT-2 [74]
backbones，成功地在各种数据集上执行时间序列预测，其基础是 LLM
可以通过激活其数据集来适应处理非语言数据集。固有的能力。类似地，Voice2Series
[105]
致力于时间序列和声学数据的同步，以利用时间序列数据声学模型的分类能力。
Wimmer 等人提出了另一种方法
[96]，他们利用视觉语言模型（VLM）来预测市场变化。除了微调现有模型之外，一种独特的方法涉及
LLMs
的直接推理以进行时间序列预测，展示了值得称赞的零样本性能。一个值得注意的例子是
LLMTime
[36]，它引入了各种策略来有效地标记时间序列数据并将离散标记分布转换为灵活的连续值密度。</p>
<p>除了仅关注时间序列的单一数据模态的方法之外，还提出了开发多模态、面向任务的基础模型的举措。一个著名的例子是
Time-LLM
[47]，它引入了一个重新编程框架来集成文本和时间序列信息，将现有的 LLM
重新用于时间序列预测器，而无需额外的计算成本。同样，METS [52]
采用可训练的心电图编码器和冻结语言模型来处理配对的心电图和临床报告。此外，还有新兴的研究直接
prompting LLMs 完成特定的时间序列任务。例如，PromptCast [102]
将数字输入和输出转换为提示，将预测任务构建为句子到句子的转换，以直接利用语言模型进行预测。其他研究，例如一项涉及
LLMs
的研究，涉及历史股票价格数据、公司元数据和过去的经济/金融新闻，旨在增强股票回报预测[108]。另一个例子将
GNN 与 ChatGPT 结合起来预测股票走势
[19]，说明了这些方法的不同应用。其他值得注意的工作包括 [100] 和
[62]。</p>
<p>值得注意的是，最近的努力已转向创建通用、单一模式的标准 TSFM。 TS2Vec
[111]
是一项开创性的工作，它引入了一个通过对比学习来表示时间序列的通用框架。
SimMTM [27]
探索跨领域应用，其中通过屏蔽时间序列建模的预训练模型在预测和分类任务中表现出卓越的微调性能。最近的工作，例如
Timer [64] 和 UnitS
[34]，通过单一的大规模预训练模型促进一般时间序列分析，进一步推进了该领域的发展。此外，人们越来越有兴趣采用预训练模型（例如
LLMs）进行广泛的时间序列分析。 OFA [120] 和 TEST [83]
例证了这一趋势，尽管这两种方法都需要针对特定任务进行端到端微调。</p>
<h3 id="spatial-time-series">Spatial Time Series</h3>
<p>在复杂的现实世界系统中，时间序列数据通常显示复杂的空间依赖性和时间动态，以时空图和栅格等形式表现出来。与第
4.1
节中的讨论类似，空间时间序列的研究通常涵盖预测和分类等领域。与标准时间序列的基础模型不同，大多数现有的空间时间序列研究仍处于早期阶段，通常具有特定领域、单一模态和面向任务的特点。接下来，我们将相关工作分为两种特定的数据模式，并在不同的小节中进行讨论。</p>
<h4 id="spatio-temporal-graph">Spatio-Temporal Graph</h4>
<p>大多数时空图的基础模型都是面向任务的，并且只关注图数据。在交通领域，TFM
[88]
利用图结构和算法来分析交通系统内的行为和交互，在城市交通预测中显示出有希望的结果。
ST-LLM [58] 将时空信息与部分冻结的 LLM 相结合以改进流量预测，而 DiffSTG
[93]
将去噪扩散模型应用于时空图以进行概率流量预测。对领域不可知模型的努力包括
STEP [79]，它将时空 GNN 与预先训练的 transformer
联系起来，通过学习大量的历史数据来增强预测。同样，STGCL [61] 和 SPGCL
[53]
探索了将对比学习整合到时空图预测中，表明了其潜在的好处。时空图通用模型的研究有限。一个著名的例子，USTD
[41]，为预测和 kriging
任务引入了一个统一的模型，采用不确定性感知扩散方法来有效应对不同的挑战。</p>
<h4 id="spatio-temporal-raster">Spatio-Temporal Raster</h4>
<p>时空栅格是指以网格格式捕获和组织不同时间点的空间信息的数据模态。这种模式主要用于气候基础模型。例如，FourCastNet
[70]
是一个全球性的、数据驱动的天气预报模型，可在全球范围内提供准确的短期到中期预测。类似的模型，如
FengWu [13] 和 W-MAE [65]，也纷纷效仿。值得注意的是，Pangu-Weather [4]
经过 39
年的全球数据训练，与领先的数值天气预报系统相比，在所有评估变量上实现了卓越的确定性预报结果。另一方面，ClimaX
[67]
旨在通用气候基础模型，使用涵盖各种变量、时空范围和物理背景的不同数据集进行预训练。它旨在对各种气候和天气相关任务进行微调，例如预报、投影和降尺度，甚至针对预训练阶段未遇到的大气变量和时空尺度。然而，时空栅格数据的领域无关模型数量很少。例如，DYffusion
[9]
利用栅格数据固有的时间动态，将这些动态直接与模型的扩散步骤集成，以创建随机的、时间调节的插值器和预测网络。</p>
<h3 id="others">Others</h3>
<p>除了标准和空间时间序列之外，各种其他类型的数据也包含时间维度，包括轨迹、事件和临床记录。这一类别的大多数研究都集中在轨迹数据上。对于基于
Transformer 的模型，AuxMobLCast [103] 通过移动提示和辅助 POI
类别分类对预训练的 LLM
进行微调，以预测人类移动模式，有效地弥合了自然语言处理和时间序列预测之间的差距。
LLM-Mob [87] 将人员流动数据编码为结构化提示，指示 LLM
考虑长期和短期行为模式以及特定时间的背景，以生成对未来位置的准确且可解释的预测。对于非基于
Transformer 的模型，Trembr [33]
利用自动编码技术来有效提取嵌入轨迹中的道路网络和时间信息。而 START [43]
引入了一种混合方法来进行轨迹嵌入学习，通过结合掩码语言模型 [25] 和
SimCLR [17] 来增强其学习能力。最近，GTM [56]
将轨迹特征分为三个域，可以独立屏蔽和生成这些域，以满足给定任务的特定输入和输出要求。然后，在给定重新采样的稀疏轨迹的情况下，通过以自回归方式重建密集采样的轨迹来对
GTM 进行预训练。对于基于扩散的模型，DiffTraj [122]
通过条件反向轨迹去噪过程从白噪声中重建和合成地理轨迹。</p>
<h2 id="methodology-perspective">Methodology Perspective</h2>
<p>在本节中，我们从方法论角度剖析 TSFM，重点关注架构和
pipeline（包括预训练和适应）的复杂性。本讨论旨在阐明驱动这些模型的功效和适应性的复杂机制。</p>
<h3 id="architecture">Architecture</h3>
<p>如图 4 所示，我们首先深入研究 TSFM 的架构，包括基于 Transformer
的模型、非基于 Transformer
的节点和基于扩散的模型，重点关注塑造其能力的底层机制，以及如何构建它们。应用于各种时间序列。</p>
<h4 id="transformer-based-models">Transformer-based Models</h4>
<p>FM 的架构已经向 Transformer [85] 显着趋同，Transformer 是首次为 NLP
任务引入的模型架构。 Transformer
的核心创新在于它利用了注意力机制，使得模型能够动态地关注输入数据的不同部分。注意力函数可以简洁地描述为
<span class="math inline">\(\text{Attention}(Q, K, V) =
\text{Softmax}(QK^T /\sqrt{d_k} )V\)</span> ，其中 <span
class="math inline">\(Q\)</span>、<span class="math inline">\(K\)</span>
和 <span class="math inline">\(V\)</span>
分别表示查询、键和值矩阵，每个矩阵的维度为 <span class="math inline">\(T
\times d_k\)</span>，并且 <span class="math inline">\(d_k\)</span>
用作调节点积大小的缩放因子。从公式可以明显看出，注意力机制具有学习数据中全局、远程依赖关系的能力。这与以前的体系结构不同，以前的体系结构通常受到局部感受野或依赖窗口的限制。此外，Transformer
的设计本质上对并行化友好，可实现显着的可扩展性，能够处理大型数据集并构建具有数十亿参数的模型。这种捕获复杂数据模式的可扩展性和效率导致
Transformer
架构得到广泛采用，超越其最初在自然语言处理（NLP）[25]中的应用，扩展到计算机视觉（CV）、语音、视频、时间序列等领域（表2)
及以上。</p>
<p>基础模型框架的选择在时间序列分析领域仍然存在争议，这与自然语言处理中仅解码器模型的趋势形成鲜明对比。该领域值得注意的工作包括仅编码器[34,68,97]、编码器-解码器
[1,26,35] 和仅解码器 [24,64,75] 模型。Ansari 等人 [1]
分析编码器-解码器框架对仅解码器模型的适用性。Liu 等人 [64]
讨论虽然仅编码器模型因其在小数据集上的有效性而在时间序列预测中受到青睐，但仅解码器架构凭借其强大的泛化性和能力，可能更适合大规模时间序列模型。架构选择的多样性强调了该领域进一步探索的潜力和必要性。</p>
<p>在标准时间序列分析方面，Transformer
架构利用其序列建模功能来捕获时间动态。这包括重新利用时间序列的预训练 LLM
以利用其预先存在的序列建模优势 [102]，或直接使用 Transformer 作为 TSFM
的基础，从头开始训练以获得最适合时间序列数据细节的模型
[35]。此外，各种技术的创新也全面增强了Transformer
模型在时间序列分析方面的功能。 TSFM 的常见做法是将时间序列分割成
patches，这可以有效地将局部动态封装在输入 tokens 中
[10,11,20,24,47,68,75,97,120]。另一个关键设计是标准化层，其中可逆实例标准化
[50]
技术通过特定于实例的均值和方差标准化数据，然后在输出层将其恢复，已在上述模型中得到广泛应用。此外，诸如多分辨率分析之类的专门方法（以
Moirai [97] 为例，通过采用不同的斑块大小）和分解策略（如 TEMPO [10]
通过将复杂的相互作用分离为趋势、季节和残差来实现）成分，已被证明可以大大提高模型功效。</p>
<p>对于空间时间序列，利用注意力机制来建模空间和时间依赖性。例如，STLLM
[58]
采用一种新颖的部分冻结注意力策略进行交通预测，利用时空嵌入来捕获跨空间和时间的交通数据的复杂动态。相反，其他研究选择对空间和时间关系进行独立建模。
TFM [88]
就是一个很好的例子，它在动态图编码器中采用注意力机制进行空间建模，集成时间方面的时间编码，体现了变压器在解决交通系统时空依赖性方面的原理。除了同时建模空间和时间关系之外，还存在另一种方法，即使用额外的空间模型或外部空间信息来增强
Transformer 模型，以增强其对时间序列进行时间建模的能力。 STEP [79]
就是一个例子，它使用无监督的预训练 TransFormer
块来对长期历史时间序列的时间关系进行建模，同时应用基于 TransFormer
块表示的图结构学习器和时空 GNN。此外，Transformer
模型的应用扩展到时空即时学习领域，MetePFL [14] 和 FedWing [15]
等举措证明了这一点。</p>
<p>除了传统的时间序列数据之外，Transformer
架构还展示了在各种时态数据集（例如轨迹和医疗记录）中的有效性，如图 3
所示。这种扩展凸显了 Transformer 时态数据分析的多功能能力。</p>
<h4 id="non-transformer-based-models">Non-Transformer-based Models</h4>
<p>除了广泛采用的 Transformer
之外，各种传统的预训练方法都利用了多层感知器 (MLP) [31]、循环神经网络
(RNN) [33] 和卷积神经网络 (CNN) [98]
等模型作为预训练的骨干。这些模型各有其独特的优势，因其在传统数据和空间时间序列数据中的有效性而引人注目。</p>
<p>MLP 和 CNN 都因其有效建模空间和时间数据的能力而受到赞誉。特别是基于
CNN 的架构在一般时间序列表示的自监督学习中引起了极大的关注，特别强调使用
ResNet [27, 115] 和扩张卷积层 [69, 111] 作为基础
backbone。这些方法主要采用一维卷积运算。相比之下，TimesNet [98]
通过将一维时间序列数据转换为二维张量，引入了一种新颖的视角，通过使用参数高效的初始块，促进多周期性的自适应识别和复杂时间变化的提取。另一方面，基于
MLP
的模型因其轻量级设计而受到赞誉，在减少计算时间和成本方面具有优势。例如，TSMixer
[30] 和 TTMs [31]
都声称在内存使用和处理速度方面具有卓越的效率，同时仍然提供有竞争力的性能。</p>
<p>RNN 因其在时态数据建模方面的熟练程度而得到认可 [33, 39]。最近，人们对
RNN 架构的兴趣重新兴起，这对流行的基于 Transformer
的模型提出了巨大的挑战。这一趋势是由对模型的追求所推动的，这些模型不仅更节省资源，而且还善于通过其固有的线性复杂性处理更长的序列。一个值得注意的实施例是
RWKV-TS [40]，它利用了 RWKV [72]（一种 RNN
型基础模型架构），展示了一般时间序列分析的巨大潜力。这种新兴趋势为时间序列研究和应用提供了宝贵的机会。</p>
<h4 id="diffusion-based-models">Diffusion-based Models</h4>
<p>基于扩散的基础模型由于能够熟练地学习复杂的数据分布，在 CV [71, 78]
和视频 [7]
中获得了突出的地位，但它们在时间序列分析方面的探索仍处于起步阶段。这些模型的作用是逐步将噪声引入数据，然后将其反转，通过反向扩散过程有效地学习原始数据的生成过程。这种独特的机制使扩散模型具有巨大的潜力，可以作为多功能基础模型，能够处理时间序列中的预测、插补和异常检测。</p>
<p>在标准时间序列和其他时间数据中，扩散模型通过捕获时间动态来预测未来状态，生成从当前状态到潜在未来状态的平滑过渡
[76,
92]。应用于空间时间序列时，它们扩展了这种能力，可以对空间相关性和时间相关性进行建模，从而提供对空间和时间之间相互作用的洞察，这在交通预测等领域特别有益[93]。</p>
<h3 id="pipeline">Pipeline</h3>
<p>在这一部分中，我们从 pipeline 的角度回顾
TSFM，包括各种模型获取和适应机制。</p>
<h4 id="pre-training">Pre-training</h4>
<p>预训练是构建 TSFM
的初始且关键的步骤，因为在此阶段学习的知识使模型能够在不同的上下文中进行泛化，并以最小的调整快速适应各种下游任务。另一方面，预训练数据的多样性（例如，标准时间序列、空间时间序列和轨迹）以及数据的使用方式，导致在构建时存在广泛的预训练机制并部署基础模型。在本次调查中，我们提出了一个主要基于预训练阶段的学习目标的新视角，对
TSFM
的现有方法进行分类。这些机制包括完全监督、自我监督（生成、对比、生成和对比的混合）等。</p>
<p>全监督预训练是指基础模型最初在一个或多个带有标签的大型时间序列数据集上进行训练，以捕获复杂的时间动态并学习可概括的表示的策略。
TTMs [31]
提出了一种通用时间序列基础模型监督框架，能够处理多个时间序列数据集的异质性，并通过多分辨率增强的设计（例如，自适应修补、通过下采样等进行数据增强）TSFM
的完全监督预训练特别适合有足够标记历史数据的场景。此外，这种预训练技术更常用于一些特定领域的应用，例如交通
[29, 88] 和气候 [3,
70]，其中模型可以直接针对下游预测任务进行定制，并且易于进行最小化的调整。</p>
<p>我们将生成预训练策略归类为时间序列表示的一般建模，包括时间序列输入的重建和概率建模。在基于重建的预训练中，有效的学习目标是通过屏蔽自动编码策略恢复原始输入空间
[14,
79]。在概率建模方法中，由时间或时空编码器形成的潜在表示空间通过最大化对数似然来针对估计密度进行优化，基于此可以对预测进行采样
[76,
93]。此外，利用对比学习来增强预训练时间序列基础模型的鲁棒性也有好处。关键是通过生成信息丰富的正对以及在执行增强时过滤掉不合适的负对来构建和利用自监督信号
[61]。除了上述两种自监督策略外，混合变体的有效性也得到了验证，其中在较少时间序列数据上预训练的模型优于监督模型
[43]。</p>
<p>一般来说，自监督预训练使基础模型能够利用大量未标记的时间序列数据，提供通用的时间知识，可以针对特定的下游任务进一步微调。与全监督预训练相比，它为时间序列基础模型的获取提供了更通用、更现实的解决方案。</p>
<p>请注意，上述预训练方法通常从头开始构建模型，并从具有相同模态（即时间序列）的数据中获取通用知识。尽管如此，时间序列研究的最新进展提高了
LLM 的使用 [10, 11, 19, 36, 38, 47, 58, 62, 63, 81, 87, 100, 102, 103,
108, 120]，VLM [
96]，以及从其他数据模态（文本序列、图像文本序列、声学信号）预训练的 AM
[105]。</p>
<h4 id="adaptation">Adaptation</h4>
<p>适应阶段根据特定任务或数据集定制
TSFM，通过利用学习到的通用时态知识来增强其在这些任务上的性能。我们将先前的方法分为四个分支，包括直接使用、微调、prompt
engineering 和时间序列 tokenization（见图 5）。</p>
<p>直接使用（也称为零样本）意味着无需对目标数据集进行进一步微调，这表明预训练模型对于下游任务具有足够的能力。它还可以表明预训练数据集和目标数据集之间的同质性，特别是对于一些构建基础模型来完成特定领域任务的实际应用程序
[4, 13]。</p>
<p>微调是使基础模型适应目标任务的常见策略。基于基础模型在目标数据集上的使用方式，存在三种主流工作：微调整个模型
[65,67,70] 或特定组件（例如，训练位置嵌入和层归一化，同时保持前馈和微调
LLM 时注意力层会被冻结）[11,
120]，以直接推断结果，或将基础模型集成为整个模型的一部分 [19, 52, 83,
103]。</p>
<p>Prompt Engineering 更专注于基于 LLM 的 TSFM。prompt
可以使用特定于任务的文本输入手工制作，并直接用于查询输出以进行下游预测
[87, 102] 或中间嵌入作为特征增强
[103]。此外，在目标数据集上优化模型时，prompt
也可以是参数化向量和端到端可学习的 [10, 83]。与静态提示相比，可训练
prompt 的使用增强了 LLMs
理解和匹配给定时间序列输入上下文的能力。例如，TEMPO
[10]构建了一个具有不同键值对的可训练提示池，并检索具有最高相似度分数的最具代表性的候选提示。</p>
<p>时间序列 tokenization 旨在有效地将时间序列表示为嵌入，这在基于
transformer 的架构中也更频繁地采用 [10,47,68]。常见的 tokenization
技术包括可逆实例归一化[50]（可减轻分布偏移）、使用通道独立策略进行修补（有效且高效地提取时间序列上下文[68]）以及联合使用时间序列分解来显式表示可解释的组件[10]
] 以便于后续的时间建模。</p>
<p>除了适应 TSFM
的主要分支之外，还值得注意的是一些微调策略考虑了现实世界的约束。例如，微调是以保护隐私的方式进行的[14,
16]。</p>
<h3 id="modality">Modality</h3>
<p>在 TSFM
的预训练/适应过程中，先前的方法涉及单个或多个数据模态，其中标准时间序列、轨迹、栅格和文本可以被视为具有独特领域视角的不同形式。在这一部分中，我们回顾了跨不同领域的现有
TSFM 中使用的数据模式。</p>
<h4 id="single-modality">Single-modality</h4>
<p>目前大多数 TSFM
都是基于单模态数据构建和定制的。与多模态方法相比，单模态时间序列建模策略具有固有的简单性优势，并绕过了处理模态间隙的挑战，并且经常在广泛的实际应用中展示出出色的经验结果，例如交通[
58, 79] 和气候预测 [13, 67]。</p>
<h4 id="multi-modality">Multi-modality</h4>
<p>然而，单模态方法可能无法概括金融 [19, 108] 和医疗保健领域 [52, 62]
中几个具有挑战性的下游任务的全貌。为了解决这个问题，人们开始致力于开发多模式、面向任务的
FM，其中附加信息为增强模型能力提供了有用的信息。在 Chen
等人的研究中，基于对特定时间步长的新闻标题的分析，查询外部 ChatGPT
以构建代表公司的不断发展的图结构。因此，推断图和股票价格被输入时间序列模型（使用
GNN 和 LSTM
进行信息传播）以生成股票价格走势预测。医疗保健领域的另一个例子也证明了多模态医疗情境建模的有效性，该模型在自监督对比学习框架下对齐心电图（心电图）和相应医疗文本报告的嵌入，并执行心电图分类。在一般的多模态时间序列分析中，采用类似的跨模态对齐策略（例如对比学习
[83]、重新编程 [47]、token-wise 提示 [63]），其中多模态输入通常是文本
LLM 的数据集和预训练词嵌入的描述。作为一个值得注意的例子，Time-LLM [47]
引入了一个重新编程框架，该框架通过线性投影和多头注意力将来自预先训练的词嵌入和时间序列信息的语言知识对齐，其中手工制作的数据集描述也用于所需的文本标记嵌入作为提示，这进一步增强了嵌入空间并通知
LLM 理解任务上下文。因此，利用多模态数据有助于将现有的 LLM
重新用于时间序列预测器，而无需额外的计算成本。</p>
<h2 id="conclusion">Conclusion</h2>
<p>FM
的快速发展彻底改变了不同领域的研究领域。在本次调查中，我们对专门为时间序列分析而设计的
FM
进行了全面且最新的回顾。从以方法论为中心的角度出发，通过基于模型架构、预训练技术、适应技术和数据模态等关键组件对
FM 进行分类，提出了一种新颖的分类法。我们的调查有助于理解将 FM
应用到时间序列的基本机制。此外，我们相信，最新进展的整合以及未来潜在的方向（见附录）可以激发时间序列分析领域更多的创新工作。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Deep-Learning/" class="category-chain-item">Deep Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Time-Series/" class="print-no-link">#Time-Series</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Paper Diary-2024-Foundation Models for Time Series Analysis@A Tutorial and Survey (2024)</div>
      <div>https://blog.lfd.world/2024/07/06/paper-diary-2024-foundation-models-for-time-series-analysis-a-tutorial-and-survey-2024/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>培根请加蛋</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年7月6日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/07/13/2024-top-time-series-review/" title="2024 Top Time Series Review">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2024 Top Time Series Review</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/07/02/paper-diary-2024-graph-self-supervised-learning-a-survey-2021/" title="Paper Diary-2024-Graph Self-Supervised Learning@A Survey (2021)">
                        <span class="hidden-mobile">Paper Diary-2024-Graph Self-Supervised Learning@A Survey (2021)</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>







  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>
</html>
